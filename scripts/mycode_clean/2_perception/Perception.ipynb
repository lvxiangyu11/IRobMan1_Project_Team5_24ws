{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95482eee-ad8f-423f-a497-8990e8ac372a",
   "metadata": {},
   "source": [
    "# 1. record point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0fd4c1-97e3-4984-a270-edddc5a05230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory (used to replace __file__)\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "target_path1 = os.path.abspath(os.path.join(current_dir, '../3_move'))\n",
    "target_path2 = os.path.abspath(os.path.join(current_dir, '../1_getPointCloud'))\n",
    "\n",
    "# Add the paths to sys.path\n",
    "if target_path1 not in sys.path:\n",
    "    sys.path.append(target_path1)\n",
    "    \n",
    "if target_path2 not in sys.path:\n",
    "    sys.path.append(target_path2)\n",
    "# Check if the paths were added successfully\n",
    "print(\"Current Python Path:\")\n",
    "print(\"\\n\".join(sys.path))\n",
    "\n",
    "from ImageRecognizer import ImageRecognizer\n",
    "image_recognizer = ImageRecognizer(top_dir=\"/opt/ros_ws/src/franka_zed_gazebo/scripts/mycode_clean/2_perception/cubes/\")\n",
    "\n",
    "from utils import matrix_to_rpy_and_translation\n",
    "\n",
    "from PickAndPlace import PickAndPlace\n",
    "\n",
    "pick_place = PickAndPlace(approach_distance=0.3, restart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c575264-32a4-4fc0-9469-5ce80c9a98fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from save_point_cloud import PointCloudSaver\n",
    "import open3d as o3d\n",
    "import rospy\n",
    "point_cloud_saver = PointCloudSaver()\n",
    "\n",
    "# Wait for data to be ready\n",
    "rospy.loginfo(\"Waiting for data...\")\n",
    "rospy.sleep(1)  # Wait for topic data to be published\n",
    "\n",
    "# Save the point cloud\n",
    "world_file = \"/opt/ros_ws/src/franka_zed_gazebo/scripts/mycode_clean/2_perception/mesh/zed_point_cloud_world3.ply\"\n",
    "point_cloud_saver.save_point_clouds(world_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ebdb0eb-9e99-4745-88a8-52bfcc7225d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from utils import filter_point_cloud_by_depth_and_range, filter_point_cloud_by_depth\n",
    "\n",
    "# zed_ply_path = \"mesh/zed_point_cloud_world3.ply\"\n",
    "zed_ply_path = \"/opt/ros_ws/src/franka_zed_gazebo/scripts/mycode_clean/2_perception/mesh/zed_point_cloud_world3.ply\"\n",
    "\n",
    "coordinate_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(\n",
    "    size=0.1,  # Size of the coordinate axes, can be adjusted as needed\n",
    "    origin=[0, 0, 0]  # Origin of the coordinate axes\n",
    ")\n",
    "\n",
    "# Read the point cloud file\n",
    "point_cloud = o3d.io.read_point_cloud(zed_ply_path)\n",
    "if not point_cloud.has_points():\n",
    "    raise ValueError(f\"Failed to read point cloud from {zed_ply_path}\")\n",
    "filtered_point_cloud = filter_point_cloud_by_depth_and_range(point_cloud, depth_threshold=0.02, range=[0.001, -0.8, 1, 1.6])\n",
    "o3d.visualization.draw_geometries([filtered_point_cloud, coordinate_frame], window_name=\"Filtered Point Cloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9883e9d-4bce-412e-ad0a-224f0121b514",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from utils import calculate_max_layer\n",
    "\n",
    "# # Example usage\n",
    "# max_layer = calculate_max_layer(filtered_point_cloud, layer_height=0.04)\n",
    "# print(f\"MaxLayer: {max_layer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d5b48e-3edb-4683-a774-6ef27ecad23b",
   "metadata": {},
   "source": [
    "# 2. Perception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f7d2ab-d25d-4387-ac97-81de3ec741dd",
   "metadata": {},
   "source": [
    "## 2.1 Coarse and Fine registration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88fe104e-b675-4313-93a1-417cdc00c8cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "def register_and_filter(pointcloud, mesh, voxel_size=0.01):\n",
    "    # Convert mesh to point cloud\n",
    "    if isinstance(mesh, o3d.geometry.TriangleMesh):\n",
    "        mesh_pointcloud = mesh.sample_points_uniformly(number_of_points=1000)\n",
    "    elif isinstance(mesh, o3d.geometry.PointCloud):\n",
    "        mesh_pointcloud = copy.deepcopy(mesh)\n",
    "    \n",
    "    # Downsample the point cloud and compute features\n",
    "    def preprocess_point_cloud(pcd, voxel_size):\n",
    "        pcd_down = pcd.voxel_down_sample(voxel_size)\n",
    "        pcd_down.estimate_normals(\n",
    "            search_param=o3d.geometry.KDTreeSearchParamHybrid(\n",
    "                radius=voxel_size*2, \n",
    "                max_nn=30\n",
    "            )\n",
    "        )\n",
    "        pcd_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "            pcd_down,\n",
    "            search_param=o3d.geometry.KDTreeSearchParamHybrid(\n",
    "                radius=voxel_size*5, \n",
    "                max_nn=100\n",
    "            )\n",
    "        )\n",
    "        return pcd_down, pcd_fpfh\n",
    "\n",
    "    # Coarse registration\n",
    "    def execute_global_registration(source_down, target_down, source_fpfh, target_fpfh, voxel_size):\n",
    "        distance_threshold = voxel_size * 1.5\n",
    "        result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
    "            source_down, target_down, source_fpfh, target_fpfh, True,\n",
    "            distance_threshold,\n",
    "            o3d.pipelines.registration.TransformationEstimationPointToPoint(False),\n",
    "            4,\n",
    "            [\n",
    "                o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(0.9),\n",
    "                o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(distance_threshold)\n",
    "            ],\n",
    "            o3d.pipelines.registration.RANSACConvergenceCriteria(4000000, 500)\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    # Fine registration\n",
    "    def refine_registration(source, target, initial_transformation, voxel_size):\n",
    "        distance_threshold = voxel_size * 1  # Reduce the threshold for higher accuracy\n",
    "        result = o3d.pipelines.registration.registration_icp(\n",
    "            source, target, distance_threshold, initial_transformation,\n",
    "            o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "            o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=1000000)\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    # Execute point cloud preprocessing\n",
    "    source_down, source_fpfh = preprocess_point_cloud(mesh_pointcloud, voxel_size)\n",
    "    target_down, target_fpfh = preprocess_point_cloud(pointcloud, voxel_size)\n",
    "\n",
    "    # Execute registration\n",
    "    coarse_result = execute_global_registration(source_down, target_down, source_fpfh, target_fpfh, voxel_size)\n",
    "    refined_result = refine_registration(mesh_pointcloud, pointcloud, coarse_result.transformation, voxel_size)\n",
    "\n",
    "    # Transform the mesh point cloud\n",
    "    transform = refined_result.transformation\n",
    "    transformed_mesh_pointcloud = mesh_pointcloud.transform(transform)\n",
    "\n",
    "    # Create bounding box\n",
    "    oriented_bounding_box = transformed_mesh_pointcloud.get_oriented_bounding_box()\n",
    "    center = oriented_bounding_box.center\n",
    "    extent = oriented_bounding_box.extent\n",
    "    rotation_matrix = oriented_bounding_box.R\n",
    "\n",
    "    # Expand the bounding box\n",
    "    margin = voxel_size\n",
    "    expanded_extent = extent + 0.5 * margin\n",
    "    expanded_bounding_box = o3d.geometry.OrientedBoundingBox(\n",
    "        center=center,\n",
    "        extent=expanded_extent,\n",
    "        R=rotation_matrix\n",
    "    )\n",
    "\n",
    "    # Filter the point cloud\n",
    "    indices_inside_box = expanded_bounding_box.get_point_indices_within_bounding_box(pointcloud.points)\n",
    "    indices_outside_box = list(set(range(len(pointcloud.points))) - set(indices_inside_box))\n",
    "\n",
    "    # Separate the point cloud\n",
    "    remaining_pointcloud = pointcloud.select_by_index(indices_outside_box)\n",
    "    deleted_pointcloud = pointcloud.select_by_index(indices_inside_box)\n",
    "\n",
    "    return transform, remaining_pointcloud, deleted_pointcloud, refined_result.fitness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97050aea-fb60-4b73-b5ee-14d1b5891787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the cube\n",
    "# Define file paths\n",
    "cube_obj_path = \"mesh/cube_0.obj\"\n",
    "zed_ply_path = \"mesh/zed_point_cloud_world3.ply\"\n",
    "\n",
    "# Read the cube_0.obj mesh\n",
    "cube_mesh = o3d.io.read_triangle_mesh(cube_obj_path)\n",
    "cube_mesh.compute_vertex_normals()  # Compute normals for better visualization\n",
    "# cube_point_cloud = cube_mesh.sample_points_uniformly(number_of_points=50000)  # Convert to point cloud\n",
    "# cube_point_cloud = cube_mesh.sample_points_poisson_disk(number_of_points=1000)\n",
    "\n",
    "# Remove the lower part of the cube to prevent flipping along the z-axis\n",
    "# cube_point_cloud = filter_point_cloud_by_depth(cube_point_cloud, depth_threshold=-0.015)\n",
    "# o3d.visualization.draw_geometries([cube_point_cloud])\n",
    "\n",
    "\n",
    "coordinate_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(\n",
    "    size=0.1,  # Size of the coordinate axes, can be adjusted as needed\n",
    "    origin=[0, 0, 0]  # Origin of the coordinate axes\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54f3fe34-f397-4e76-8452-b24a6cf4e9f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_transform_z_axis_alignment(transform, tolerance=0.1):\n",
    "    \"\"\"\n",
    "    Check if the Z-axis of the transform is parallel to the Z-axis of the world coordinate system.\n",
    "    Allows a certain tolerance range to check if it is parallel or anti-parallel.\n",
    "    \n",
    "    Args:\n",
    "    - transform: 4x4 transformation matrix\n",
    "    - tolerance: Tolerance range for checking, default is 0.1\n",
    "    \n",
    "    Returns:\n",
    "    - True: If the Z-axis is parallel or anti-parallel\n",
    "    - False: If the Z-axis is not parallel\n",
    "    - The corrected transform\n",
    "    \"\"\"\n",
    "    z_axis = np.array([0, 0, 1])  # Z-axis of the world coordinate system\n",
    "    transform_z_axis = transform[:3, 2]  # Get the Z-axis of the transform (i.e., the third column of the rotation matrix)\n",
    "\n",
    "    # Compute the angle between the transform's Z-axis and the world coordinate system's Z-axis\n",
    "    dot_product = np.dot(transform_z_axis, z_axis)\n",
    "    # Compute the cosine of the angle, if close to 1 or -1, it means parallel or anti-parallel\n",
    "    if np.abs(dot_product) > (1 - tolerance):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def align_transform_z_axis(transform):\n",
    "    \"\"\"\n",
    "    If the Z-axis of the transform is anti-parallel to the Z-axis of the world coordinate system,\n",
    "    rotate by 180 degrees around the X-axis (np.pi) to flip the direction of the Z-axis.\n",
    "    \n",
    "    Args:\n",
    "    - transform: 4x4 transformation matrix\n",
    "    \n",
    "    Returns:\n",
    "    - The corrected transform matrix\n",
    "    \"\"\"\n",
    "    z_axis_world = np.array([0, 0, 1])  # Z-axis of the world coordinate system\n",
    "    transform_z_axis = transform[:3, 2]  # Get the Z-axis of the transform (the third column of the rotation matrix)\n",
    "\n",
    "    # Check if the Z-axis is anti-parallel to the world coordinate system's Z-axis\n",
    "    if np.dot(transform_z_axis, z_axis_world) < 0:  # Z-axis is anti-parallel\n",
    "        print(\"Correcting Z axis\")\n",
    "        # Create a rotation matrix to rotate 180 degrees around the X-axis\n",
    "        rotation_matrix = np.eye(4)\n",
    "        rotation_matrix[1, 1] = -1  # Rotate the matrix by 180 degrees around the X-axis\n",
    "        rotation_matrix[2, 2] = -1  # Rotate the matrix by 180 degrees around the X-axis\n",
    "        \n",
    "        # Perform matrix multiplication, applying the rotation matrix to the original transform\n",
    "        transform = np.dot(rotation_matrix, transform)\n",
    "\n",
    "    return transform\n",
    "\n",
    "def align_transform_z_axis_any_orientation(transform):\n",
    "    \"\"\"\n",
    "    Aligns the Z-axis of the transform with the negative Z-axis of the world coordinate system,\n",
    "    no matter what the original orientation is.\n",
    "    \n",
    "    Args:\n",
    "    - transform: 4x4 transformation matrix\n",
    "    \n",
    "    Returns:\n",
    "    - The corrected transform matrix with Z-axis pointing downward\n",
    "    \"\"\"\n",
    "    # Extract the original rotation matrix and translation vector\n",
    "    rotation = transform[:3, :3]\n",
    "    translation = transform[:3, 3]\n",
    "    \n",
    "    # Extract the current z-axis direction (third column of rotation matrix)\n",
    "    current_z = rotation[:, 2]\n",
    "    \n",
    "    # Target z-axis direction (downward in world coordinates)\n",
    "    target_z = np.array([0, 0, -1])\n",
    "    \n",
    "    # Check if already aligned within a small tolerance\n",
    "    if np.allclose(current_z, target_z, atol=1e-6):\n",
    "        return transform\n",
    "    \n",
    "    # For completely anti-aligned case (pointing directly up), simple 180° rotation works\n",
    "    if np.allclose(current_z, -target_z, atol=1e-6):\n",
    "        # Rotate 180 degrees around x-axis\n",
    "        r = np.eye(3)\n",
    "        r[1, 1] = -1\n",
    "        r[2, 2] = -1\n",
    "        new_rotation = rotation @ r\n",
    "    else:\n",
    "        # For any other orientation, we need to find the rotation that aligns vectors\n",
    "        # Compute the rotation axis (cross product of current and target z-axes)\n",
    "        rotation_axis = np.cross(current_z, target_z)\n",
    "        \n",
    "        # If current_z and target_z are parallel or anti-parallel, rotation_axis might be zero\n",
    "        # In that case, choose any perpendicular axis\n",
    "        if np.allclose(rotation_axis, 0, atol=1e-10):\n",
    "            # Find a non-zero component in current_z\n",
    "            if abs(current_z[0]) > 1e-10:\n",
    "                rotation_axis = np.array([current_z[1], -current_z[0], 0])\n",
    "            else:\n",
    "                rotation_axis = np.array([0, current_z[2], -current_z[1]])\n",
    "        \n",
    "        # Normalize the rotation axis\n",
    "        rotation_axis = rotation_axis / np.linalg.norm(rotation_axis)\n",
    "        \n",
    "        # Compute the rotation angle (dot product gives cosine of angle)\n",
    "        # We want the shorter arc between the vectors\n",
    "        cos_angle = np.dot(current_z, target_z)\n",
    "        angle = np.arccos(np.clip(cos_angle, -1.0, 1.0))\n",
    "        \n",
    "        # Rodrigues' rotation formula to get rotation matrix\n",
    "        K = np.array([\n",
    "            [0, -rotation_axis[2], rotation_axis[1]],\n",
    "            [rotation_axis[2], 0, -rotation_axis[0]],\n",
    "            [-rotation_axis[1], rotation_axis[0], 0]\n",
    "        ])\n",
    "        R = np.eye(3) + np.sin(angle) * K + (1 - np.cos(angle)) * (K @ K)\n",
    "        \n",
    "        # Apply the rotation to the original rotation matrix\n",
    "        new_rotation = rotation @ R\n",
    "    \n",
    "    # Create the new transformation matrix\n",
    "    new_transform = np.eye(4)\n",
    "    new_transform[:3, :3] = new_rotation\n",
    "    new_transform[:3, 3] = translation\n",
    "    \n",
    "    # Validate the result\n",
    "    result_z = new_transform[:3, :3][:, 2]\n",
    "    alignment_quality = np.dot(result_z, target_z)\n",
    "    print(f\"Z-axis alignment quality: {alignment_quality:.6f} (closer to 1 is better)\")\n",
    "    \n",
    "    return new_transform\n",
    "\n",
    "def align_transform_z_axis_preserving_xy(transform):\n",
    "    \"\"\"\n",
    "    Align the Z-axis of the transformation matrix to point downward (world -Z axis),\n",
    "    while preserving the original XY plane orientation as much as possible.\n",
    "\n",
    "    This function ensures:\n",
    "    1. The Z-axis points downward (aligned with the world -Z axis)\n",
    "    2. The XY plane is preserved as much as possible to minimize disruption to the original grasp direction\n",
    "\n",
    "    Args:\n",
    "        transform: A 4x4 transformation matrix\n",
    "\n",
    "    Returns:\n",
    "        A corrected transformation matrix with the Z-axis pointing downward and XY directions as close as possible to the original\n",
    "    \"\"\"\n",
    "    # Extract the original rotation matrix and translation vector\n",
    "    rotation = transform[:3, :3]\n",
    "    translation = transform[:3, 3]\n",
    "\n",
    "    # Extract current axes directions from the rotation matrix\n",
    "    x_axis = rotation[:, 0]  # X-axis is the first column\n",
    "    y_axis = rotation[:, 1]  # Y-axis is the second column\n",
    "    z_axis = rotation[:, 2]  # Z-axis is the third column\n",
    "\n",
    "    # Target direction for Z-axis (world -Z direction)\n",
    "    target_z = np.array([0, 0, -1])\n",
    "\n",
    "    # Check if the current Z-axis is already aligned with the target\n",
    "    aligned = np.isclose(np.abs(np.dot(z_axis, target_z)), 1.0, atol=1e-6)\n",
    "    if aligned and np.dot(z_axis, target_z) > 0:\n",
    "        # Already aligned, no changes needed\n",
    "        return transform\n",
    "\n",
    "    # Set new Z-axis to point down\n",
    "    new_z = target_z\n",
    "\n",
    "    # Compute a new X-axis that is orthogonal to the new Z-axis\n",
    "    # Do this by projecting the original X-axis onto the plane perpendicular to new Z\n",
    "\n",
    "    new_x = x_axis - np.dot(x_axis, new_z) * new_z\n",
    "\n",
    "    # If the projection is too small (i.e., X was almost aligned with Z), try using original Y\n",
    "    if np.linalg.norm(new_x) < 1e-6:\n",
    "        new_x = y_axis - np.dot(y_axis, new_z) * new_z\n",
    "\n",
    "    # If still too small, pick an arbitrary vector that's not parallel to Z\n",
    "    if np.linalg.norm(new_x) < 1e-6:\n",
    "        temp = np.array([1, 0, 0]) if abs(new_z[0]) < 0.9 else np.array([0, 1, 0])\n",
    "        new_x = temp - np.dot(temp, new_z) * new_z\n",
    "\n",
    "    # Normalize the new X-axis\n",
    "    new_x = new_x / np.linalg.norm(new_x)\n",
    "\n",
    "    # Calculate the new Y-axis using the cross product (ensures orthogonality)\n",
    "    new_y = np.cross(new_z, new_x)\n",
    "\n",
    "    # Construct the new rotation matrix from the orthonormal axes\n",
    "    new_rotation = np.column_stack((new_x, new_y, new_z))\n",
    "\n",
    "    # Assemble the final transformation matrix\n",
    "    new_transform = np.eye(4)\n",
    "    new_transform[:3, :3] = new_rotation\n",
    "    new_transform[:3, 3] = translation\n",
    "\n",
    "    # Evaluate alignment quality of the new Z-axis\n",
    "    result_z = new_transform[:3, :3][:, 2]\n",
    "    alignment_quality = np.dot(result_z, target_z)\n",
    "    print(f\"Z-axis alignment quality: {alignment_quality:.6f} (closer to 1 is better)\")\n",
    "\n",
    "    # Evaluate how well the XY plane orientation is preserved\n",
    "    original_xy_normal = np.cross(x_axis, y_axis)\n",
    "    original_xy_normal = original_xy_normal / np.linalg.norm(original_xy_normal)\n",
    "\n",
    "    new_xy_normal = np.cross(new_x, new_y)\n",
    "    new_xy_normal = new_xy_normal / np.linalg.norm(new_xy_normal)\n",
    "\n",
    "    xy_preservation = np.abs(np.dot(original_xy_normal, new_xy_normal))\n",
    "    print(f\"XY plane preservation quality: {xy_preservation:.6f} (closer to 1 is better)\")\n",
    "\n",
    "    return new_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76d4d048-43c7-4a8a-ba1b-9c8e9fadcfa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import rospy\n",
    "import tf\n",
    "import numpy as np\n",
    "from geometry_msgs.msg import TransformStamped\n",
    "\n",
    "class TransformBroadcaster:\n",
    "    def __init__(self):\n",
    "        # Try to initialize the ROS node, avoid initializing multiple times\n",
    "        try:\n",
    "            rospy.init_node('tf_broadcaster_node')\n",
    "        except rospy.exceptions.ROSException:\n",
    "            pass  # If the node is already initialized, do nothing\n",
    "\n",
    "        # Create a TransformBroadcaster instance\n",
    "        self.br = tf.TransformBroadcaster()\n",
    "\n",
    "        # Assume T is the given 4x4 transformation matrix\n",
    "        self.T = np.ones((4, 4))  # Set to a 4x4 matrix\n",
    "\n",
    "        # Set a timer to call the broadcast_transform function every 100 milliseconds\n",
    "        self.timer = rospy.Timer(rospy.Duration(0.1), self.broadcast_transform)\n",
    "\n",
    "        # Store the timestamp of the last sent transformation\n",
    "        self.last_sent_time = None\n",
    "\n",
    "    def broadcast_transform(self, event):\n",
    "        try:\n",
    "            # Extract the translation and rotation parts from the 4x4 matrix\n",
    "            translation = self.T[0:3, 3]  # Translation part (x, y, z)\n",
    "            rotation_matrix = self.T[0:3, 0:3]  # Rotation matrix part\n",
    "\n",
    "            # Create a complete 4x4 matrix, including rotation and homogeneous coordinates\n",
    "            full_matrix = np.eye(4)\n",
    "            full_matrix[0:3, 0:3] = rotation_matrix\n",
    "            full_matrix[0:3, 3] = translation\n",
    "\n",
    "            # Create a quaternion to represent the rotation\n",
    "            quaternion = tf.transformations.quaternion_from_matrix(full_matrix)\n",
    "\n",
    "            # Get the current timestamp\n",
    "            current_time = rospy.Time.now()\n",
    "\n",
    "            # Check if the last sent timestamp and the current timestamp are the same\n",
    "            if self.last_sent_time is None or current_time != self.last_sent_time:\n",
    "                # Publish the transformation\n",
    "                self.br.sendTransform(\n",
    "                    (translation[0], translation[1], translation[2]),  # Translation part\n",
    "                    (quaternion[0], quaternion[1], quaternion[2], quaternion[3]),  # Rotation part (quaternion)\n",
    "                    current_time,  # Use the current timestamp\n",
    "                    \"cube\",  # Child frame name\n",
    "                    \"world\"   # Parent frame name\n",
    "                )\n",
    "                # Update the last sent timestamp\n",
    "                self.last_sent_time = current_time\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def update(self, T):\n",
    "        self.T = T\n",
    "        \n",
    "    def stop(self):\n",
    "        # Stop the timer\n",
    "        self.timer.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4cef69-b1d4-40d5-92e0-ad3c6f59fc93",
   "metadata": {},
   "source": [
    "## 2.2 Grasp Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "155cbbb8-ad8b-4146-a453-4cdaf9afbad5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import *  # Assuming the create_grasp_mesh function is in utils.py\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "\n",
    "def generate_gripper_from_transform(T: np.ndarray):\n",
    "    \"\"\"\n",
    "    Generates a robotic gripper mesh from a given 4x4 transformation matrix,\n",
    "    with additional rotations around x and y axes.\n",
    "\n",
    "    Args:\n",
    "        T: 4x4 transformation matrix (numpy array).\n",
    "        \n",
    "    Returns:\n",
    "        gripper_meshes: List of meshes representing the gripper.\n",
    "    \"\"\"\n",
    "    # Extract the rotation matrix (3x3)\n",
    "    rotation_matrix = T[:3, :3]\n",
    "\n",
    "    # Extract the translation vector\n",
    "    translation = T[:3, 3]\n",
    "\n",
    "    # Set the gripper's center point position, usually the translation vector\n",
    "    center_point = translation\n",
    "\n",
    "    # Create a rotation matrix for -90 degrees around the x-axis\n",
    "    R_x = np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, np.cos(-np.pi/2), -np.sin(-np.pi/2)],\n",
    "        [0, np.sin(-np.pi/2), np.cos(-np.pi/2)]\n",
    "    ])\n",
    "\n",
    "    # Create a rotation matrix for 90 degrees around the y-axis\n",
    "    R_y = np.array([\n",
    "        [np.cos(np.pi/2), 0, np.sin(np.pi/2)],\n",
    "        [0, 1, 0],\n",
    "        [-np.sin(np.pi/2), 0, np.cos(np.pi/2)]\n",
    "    ])\n",
    "    \n",
    "    R_z = np.array([\n",
    "        [0, -1, 0],\n",
    "        [1, 0, 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    # Combine rotation matrices, first rotate around the x-axis, then around the y-axis\n",
    "    combined_rotation = R_z @ rotation_matrix @ R_x \n",
    "\n",
    "    # Call create_grasp_mesh function to generate the gripper\n",
    "    gripper_meshes = create_grasp_mesh(\n",
    "        center_point=center_point, \n",
    "        rotation_matrix=combined_rotation,\n",
    "        width=0.25\n",
    "    )\n",
    "    # Call create_grasp_mesh function to generate the gripper with a different rotation\n",
    "    gripper_meshes_rotate = create_grasp_mesh(\n",
    "        center_point=center_point, \n",
    "        rotation_matrix=rotation_matrix @ R_x,\n",
    "        width=0.25\n",
    "    )\n",
    "\n",
    "    return gripper_meshes, gripper_meshes_rotate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47315293-9f5e-4dd0-a1aa-9efab7885c1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test code: Pass in a 4x4 transformation matrix\n",
    "T = np.array([\n",
    "    [1, 0, 0, 0.1],  # Rotation matrix and translation\n",
    "    [0, 1, 0, 0.2],\n",
    "    [0, 0, 1, 0.3],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "# Call the function to generate the gripper\n",
    "gripper_meshes, _ = generate_gripper_from_transform(T)\n",
    "\n",
    "# Visualize the generated gripper\n",
    "# o3d.visualization.draw_geometries(gripper_meshes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8067b9a2-8d15-4f9e-9842-e1d6815c3d34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_grasp_collision(\n",
    "    grasp_meshes: Sequence[o3d.geometry.TriangleMesh],\n",
    "    object_pcd: o3d.geometry.TriangleMesh,\n",
    "    num_colisions: int = 10,\n",
    "    tolerance: float = 0.00001\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Checks for collisions between a gripper grasp pose and target object\n",
    "    using point cloud sampling.\n",
    "\n",
    "    Args:\n",
    "        grasp_meshes: List of mesh geometries representing the gripper components\n",
    "        object_mesh: Triangle mesh of the target object\n",
    "        num_collisions: Threshold on how many points to check\n",
    "        tolerance: Distance threshold for considering a collision (in meters)\n",
    "\n",
    "    Returns:\n",
    "        bool: True if collision detected between gripper and object, False otherwise\n",
    "    \"\"\"\n",
    "    # Combine gripper meshes\n",
    "    combined_gripper = o3d.geometry.TriangleMesh()\n",
    "    for mesh in grasp_meshes:\n",
    "        combined_gripper += mesh  # Combine multiple gripper meshes\n",
    "\n",
    "    # Sample points from both meshes\n",
    "    num_points = 5000  # Sample 5000 points from both gripper and target object\n",
    "    #######################TODO#######################\n",
    "    # Uniformly sample point clouds from both the gripper and object meshes\n",
    "    gripper_pcd = combined_gripper.sample_points_uniformly(number_of_points=num_points)\n",
    "    gripper_points = np.asarray(gripper_pcd.points)  # Point coordinates of the gripper point cloud\n",
    "    object_points = np.asarray(object_pcd.points)  # Point coordinates of the target object point cloud\n",
    "    ##################################################\n",
    "    \n",
    "    # Build KDTree for object points\n",
    "    is_collision = False\n",
    "    #######################TODO#######################\n",
    "    collision_count = 0\n",
    "    # Build a KDTree for the target object point cloud\n",
    "    object_kdtree = o3d.geometry.KDTreeFlann(object_pcd)\n",
    "    for gripper_point in gripper_points:\n",
    "        # For each gripper point, find the nearest point in the target object point cloud\n",
    "        _, _, distances = object_kdtree.search_knn_vector_3d(gripper_point, 1)  # Find the nearest neighbor\n",
    "        \n",
    "        # If the distance to the nearest neighbor is less than the tolerance, consider it a collision\n",
    "        if distances[0] <= tolerance:\n",
    "            collision_count += 1\n",
    "            \n",
    "            # Exit early if enough collisions are detected\n",
    "            if collision_count >= num_colisions:\n",
    "                is_collision = True\n",
    "                break\n",
    "    #######################TODO#######################\n",
    "\n",
    "    return is_collision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed7a038-40aa-4a06-a50d-9abae0aba06f",
   "metadata": {},
   "source": [
    "## 2.3 identify image of cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9fcf074-462c-450a-921d-b9589f4b627f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# from io import BytesIO\n",
    "# from PIL import Image\n",
    "# import base64\n",
    "# import json\n",
    "# SYSTEM_PROMPT = \"\"\"Please act as an image recognition agent. \n",
    "# You will be given a square face of a block, \n",
    "# which is projected from a point cloud. \n",
    "# Your task is to recognize the following:\n",
    "\n",
    "# Determine if this is a block face.\n",
    "# Each face contains only one letter, \n",
    "# one pattern (just detect whether it's a pattern, no need to identify the exact pattern), \n",
    "# or is blank (only wood texture). \n",
    "# Please detect whether it is a letter, \n",
    "# a pattern, or blank. \n",
    "# Each of these may be rotated. \n",
    "# Please analyze all possible rotations in a clockwise direction: 0°, 90°, 180°, and 270°.\n",
    "# There might be a circular border around the face. \n",
    "# Please detect if this border exists. \n",
    "# It's confirmed that the color of the border matches the color of the letter or pattern.\n",
    "# The expected output is a JSON in the following format:\n",
    "# {\n",
    "#     \"check\": true/false, \n",
    "#     \"c\": char/\"pattern\"/\"blank\", \n",
    "#     \"color\": \"green\"/\"yellow\"/\"red\"/\"blue\"/\"None\", \n",
    "#     \"rotation\": 0/90/180/270, \n",
    "#     \"circle\": true/false\n",
    "# }\n",
    "# \"\"\"\n",
    "\n",
    "# api_key=\"\"\n",
    "# client = OpenAI(api_key=api_key)\n",
    "\n",
    "# def encode_image(image, quality=100):\n",
    "#     if image.mode != 'RGB':\n",
    "#         image = image.convert('RGB')  # Convert to RGB\n",
    "#     buffered = BytesIO()\n",
    "#     image.save(buffered, format=\"JPEG\", quality=quality) \n",
    "#     return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "# def gpt4o_analysis(image_path, quality=50):\n",
    "#     with Image.open(image_path) as img:\n",
    "#         img_b64_str = encode_image(img, quality=quality)\n",
    "#     img_type = \"image/jpeg\"\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=\"gpt-4o-mini\",\n",
    "#         messages=[\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": [\n",
    "#                     {\"type\": \"text\", \"text\": SYSTEM_PROMPT},\n",
    "#                     {\n",
    "#                         \"type\": \"image_url\",\n",
    "#                         \"image_url\": {\"url\": f\"data:{img_type};base64,{img_b64_str}\"},\n",
    "#                     },\n",
    "#                 ],\n",
    "#             }from openai import OpenAI\n",
    "# from io import BytesIO\n",
    "# from PIL import Image\n",
    "# import base64\n",
    "# import json\n",
    "# SYSTEM_PROMPT = \"\"\"Please act as an image recognition agent. \n",
    "# You will be given a square face of a block, \n",
    "# which is projected from a point cloud. \n",
    "# Your task is to recognize the following:\n",
    "\n",
    "# Determine if this is a block face.\n",
    "# Each face contains only one letter, \n",
    "# one pattern (just detect whether it's a pattern, no need to identify the exact pattern), \n",
    "# or is blank (only wood texture). \n",
    "# Please detect whether it is a letter, \n",
    "# a pattern, or blank. \n",
    "# Each of these may be rotated. \n",
    "# Please analyze all possible rotations in a clockwise direction: 0°, 90°, 180°, and 270°.\n",
    "# There might be a circular border around the face. \n",
    "# Please detect if this border exists. \n",
    "# It's confirmed that the color of the border matches the color of the letter or pattern.\n",
    "# The expected output is a JSON in the following format:\n",
    "# {\n",
    "#     \"check\": true/false, \n",
    "#     \"c\": char/\"pattern\"/\"blank\", \n",
    "#     \"color\": \"green\"/\"yellow\"/\"red\"/\"blue\"/\"None\", \n",
    "#     \"rotation\": 0/90/180/270, \n",
    "#     \"circle\": true/false\n",
    "# }\n",
    "# \"\"\"\n",
    "\n",
    "# api_key=\"\"\n",
    "# client = OpenAI(api_key=api_key)\n",
    "\n",
    "# def encode_image(image, quality=100):\n",
    "#     if image.mode != 'RGB':\n",
    "#         image = image.convert('RGB')  # Convert to RGB\n",
    "#     buffered = BytesIO()\n",
    "#     image.save(buffered, format=\"JPEG\", quality=quality) \n",
    "#     return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "# def gpt4o_analysis(image_path, quality=50):\n",
    "#     with Image.open(image_path) as img:\n",
    "#         img_b64_str = encode_image(img, quality=quality)\n",
    "#     img_type = \"image/jpeg\"\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=\"gpt-4o-mini\",\n",
    "#         messages=[\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": [\n",
    "#                     {\"type\": \"text\", \"text\": SYSTEM_PROMPT},\n",
    "#                     {\n",
    "#                         \"type\": \"image_url\",\n",
    "#                         \"image_url\": {\"url\": f\"data:{img_type};base64,{img_b64_str}\"},\n",
    "#                     },\n",
    "#                 ],\n",
    "#             }\n",
    "#         ],\n",
    "#     )\n",
    "#     return response.choices[0].message.content\n",
    "#         ],\n",
    "#     )\n",
    "#     return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348b83d4-ff09-4bbd-a7f0-b5a606c423ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PointCloud2Image import enlarge_points_as_cubes, max_downsample_image, pointcloud_to_top_view_image_color, interpolate_sparse_image, pointcloud_to_colored_image_with_filling, triangle_mesh_to_image\n",
    "\n",
    "try:\n",
    "    broadcaster\n",
    "except NameError:\n",
    "    broadcaster = TransformBroadcaster()           \n",
    "    \n",
    "cube_num = 26\n",
    "def pointcloud_process(point_cloud, slice_tolerance=0.005):\n",
    "    '''\n",
    "        Identify the cubes\n",
    "        Returns:\n",
    "            [\n",
    "                json,\n",
    "                T\n",
    "            ]\n",
    "    '''\n",
    "    orignal_point_cloud = copy.deepcopy(point_cloud)\n",
    "    T = []\n",
    "    remaining_pointcloud_count = 10000\n",
    "    countdown = 50\n",
    "    # Use open3d to visualize the point cloud\n",
    "    # o3d.visualization.draw_geometries([layer_point_cloud], window_name=f\"Layer {layer} (Z range: {z_min:.4f} to {z_max:.4f})\")\n",
    "    movecount = 0\n",
    "    while remaining_pointcloud_count > 50 and countdown > 0:\n",
    "        cube_point_cloud = cube_mesh.sample_points_uniformly(number_of_points=50000) \n",
    "        cube_point_cloud = filter_point_cloud_by_depth(cube_point_cloud, depth_threshold=-0.01)\n",
    "        transform, remaining_pointcloud, deleted_pointcloud, fitness = register_and_filter(point_cloud, cube_point_cloud)\n",
    "        remaining_pointcloud_count = len(remaining_pointcloud.points)\n",
    "        # return transform, remaining_pointcloud, deleted_pointcloud\n",
    "        if fitness > 0.01:\n",
    "            countdown = countdown - 1\n",
    "        print(fitness)\n",
    "        if check_transform_z_axis_alignment(transform) and fitness > 0.50 and np.array_equal(transform, align_transform_z_axis(transform)):\n",
    "        # if fitness > 0.70:\n",
    "            if movecount >= cube_num:\n",
    "                break\n",
    "            print(movecount)\n",
    "            \n",
    "\n",
    "            broadcaster.update(transform)\n",
    "            cube_coordinate_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(\n",
    "                size=0.1,  # Coordinate axis size, can be adjusted as needed\n",
    "                origin=[0, 0, 0]  # Origin of the coordinate axis\n",
    "            )\n",
    "            theta = np.radians(45)  # Convert angle to radians\n",
    "            transform_matrix_x_180 = np.array([\n",
    "                [1, 0, 0, 0],\n",
    "                [0, -1, 0, 0],\n",
    "                [0, 0, -1, 0],\n",
    "                [0, 0, 0, 1]\n",
    "            ])\n",
    "            transform_matrix_z_90 = np.array([\n",
    "                [0, -1, 0, 0],\n",
    "                [1, 0, 0, 0],\n",
    "                [0, 0, 1, 0],\n",
    "                [0, 0, 0, 1]\n",
    "            ])\n",
    "            graps_transform = transform @ transform_matrix_x_180\n",
    "            graps_transform_rotate = transform @ transform_matrix_x_180 @ transform_matrix_z_90\n",
    "                        # o3d.visualization.draw_geometries([coordinate_frame, remaining_pointcloud], window_name=\"remaining_pointcloud\")\n",
    "            cube_point_cloud_transormed = copy.deepcopy(deleted_pointcloud)\n",
    "            cube_point_cloud_transormed = cube_point_cloud_transormed.transform(np.linalg.inv(graps_transform))\n",
    "            cube_point_cloud_transormed_cubes = enlarge_points_as_cubes(cube_point_cloud_transormed)\n",
    "\n",
    "            cube_top_image = triangle_mesh_to_image(cube_point_cloud_transormed_cubes, image_size=(100, 100))\n",
    "            cube_top_image = (cube_top_image / cube_top_image.max() * 255).astype(np.uint8)\n",
    "            grasp_coordinate_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(\n",
    "                size=0.1,  # Coordinate axis size, can be adjusted as needed\n",
    "                origin=[0, 0, 0]  # Origin of the coordinate axis\n",
    "            )\n",
    "            grasp_mesh, gripper_meshes_rotate = generate_gripper_from_transform(graps_transform)\n",
    "            # Apply transformation matrix to the coordinate frame\n",
    "            grasp_coordinate_frame.transform(graps_transform)\n",
    "            grasp_final_matrix = None\n",
    "            # o3d.visualization.draw_geometries(grasp_mesh+gripper_meshes_rotate+[orignal_point_cloud, coordinate_frame], window_name=\"remaining_pointcloud\")\n",
    "            if check_grasp_collision(grasp_mesh, orignal_point_cloud):\n",
    "            # if check_grasp_collision(grasp_mesh, deleted_pointcloud):\n",
    "                # If collision\n",
    "                grasp_mesh = []\n",
    "            else:\n",
    "                grasp_final_matrix = graps_transform\n",
    "            if check_grasp_collision(gripper_meshes_rotate, orignal_point_cloud):\n",
    "            # if check_grasp_collision(gripper_meshes_rotate, deleted_pointcloud):\n",
    "                gripper_meshes_rotate = []\n",
    "            else:\n",
    "                grasp_final_matrix = graps_transform_rotate\n",
    "            # o3d.visualization.draw_geometries(grasp_mesh+gripper_meshes_rotate+[grasp_coordinate_frame, deleted_pointcloud, coordinate_frame], window_name=\"deleted_pointcloud\")\n",
    "            # o3d.visualization.draw_geometries(grasp_mesh+gripper_meshes_rotate+[grasp_coordinate_frame, remaining_pointcloud, coordinate_frame], window_name=\"deleted_pointcloud\")\n",
    "            if grasp_final_matrix is not None: # Can move\n",
    "                countdown = 50\n",
    "                print(grasp_final_matrix)\n",
    "                movecount += 1\n",
    "                pick_rpy, pick_pos = matrix_to_rpy_and_translation(grasp_final_matrix)\n",
    "                pick_pos_ = [a + b for a, b in zip(pick_pos, [0, 0, 0.10])]\n",
    "                pick_rpy = [a + b for a, b in zip(pick_rpy, [0, 0, 0])]\n",
    "                # pick_place.move(pick_pos_, pick_rpy)\n",
    "                print(pick_pos_, pick_rpy)\n",
    "                # cube_top_image = point_cloud_to_image(cube_point_cloud_transormed)\n",
    "                plt.imsave(f\"test_{movecount}.png\", cube_top_image)\n",
    "                plt.show()\n",
    "                point_cloud = remaining_pointcloud\n",
    "                # TODO: Identify the first face\n",
    "            \n",
    "                T.append(grasp_final_matrix)\n",
    "        else:\n",
    "            cube_point_cloud = cube_mesh.sample_points_uniformly(number_of_points=50000)\n",
    "            cube_point_cloud = filter_point_cloud_by_depth(cube_point_cloud, depth_threshold=-0.01)\n",
    "    print(T)\n",
    "    return T\n",
    "\n",
    "# def pointcloud_process(point_cloud, slice_tolerance=0.005, cube_num=cube_num):\n",
    "#     '''\n",
    "#     Identify the cubes in a point cloud and determine their transformation matrices for grasping.\n",
    "#     Handles alignment of Z-axis in any orientation for proper grasping.\n",
    "    \n",
    "#     Args:\n",
    "#         point_cloud: The input point cloud containing cubes to be identified\n",
    "#         slice_tolerance: Tolerance for slicing the point cloud\n",
    "#         cube_num: Maximum number of cubes to detect\n",
    "        \n",
    "#     Returns:\n",
    "#         A list of transformation matrices for grasping the identified cubes\n",
    "#     '''\n",
    "#     original_point_cloud = copy.deepcopy(point_cloud)\n",
    "#     T = []\n",
    "#     remaining_pointcloud = point_cloud\n",
    "#     remaining_pointcloud_count = len(point_cloud.points)\n",
    "#     countdown = 50\n",
    "#     movecount = 0\n",
    "    \n",
    "#     # Define standard transformation matrices for grasping\n",
    "#     transform_matrix_x_180 = np.array([\n",
    "#         [1, 0, 0, 0],\n",
    "#         [0, -1, 0, 0],\n",
    "#         [0, 0, -1, 0],\n",
    "#         [0, 0, 0, 1]\n",
    "#     ])\n",
    "    \n",
    "#     transform_matrix_z_90 = np.array([\n",
    "#         [0, -1, 0, 0],\n",
    "#         [1, 0, 0, 0],\n",
    "#         [0, 0, 1, 0],\n",
    "#         [0, 0, 0, 1]\n",
    "#     ])\n",
    "    \n",
    "#     # Function to check if a cube's z-axis points approximately downward\n",
    "#     def is_z_axis_pointing_down(transform):\n",
    "#         z_axis = transform[:3, 2]  # Extract z-axis vector\n",
    "#         return np.dot(z_axis, np.array([0, 0, -1])) > 0.8  # Check if close to pointing down\n",
    "    \n",
    "#     while remaining_pointcloud_count > 50 and countdown > 0 and movecount < cube_num:\n",
    "#         # Sample points from the cube mesh model\n",
    "#         cube_point_cloud = cube_mesh.sample_points_uniformly(number_of_points=50000) \n",
    "#         cube_point_cloud = filter_point_cloud_by_depth(cube_point_cloud, depth_threshold=-0.01)\n",
    "        \n",
    "#         # Register the cube in the point cloud\n",
    "#         transform, remaining_pointcloud, deleted_pointcloud, fitness = register_and_filter(\n",
    "#             remaining_pointcloud, cube_point_cloud\n",
    "#         )\n",
    "        \n",
    "#         remaining_pointcloud_count = len(remaining_pointcloud.points)\n",
    "        \n",
    "#         # Decrement countdown if the fitness is above a minimum threshold\n",
    "#         if fitness > 0.01:\n",
    "#             countdown -= 1\n",
    "            \n",
    "#         print(f\"Iteration {50-countdown}: Fitness = {fitness:.4f}, Remaining points: {remaining_pointcloud_count}\")\n",
    "        \n",
    "#         # Check if the fitness is above the acceptable threshold\n",
    "#         if fitness > 0.40:  # Lowered threshold slightly for more detections\n",
    "#             print(f\"Processing cube #{movecount+1}\")\n",
    "            \n",
    "#             # Align the Z-axis to point downward regardless of original orientation\n",
    "#             aligned_transform = align_transform_z_axis_any_orientation(transform)\n",
    "            \n",
    "#             # Update the transform broadcaster for visualization\n",
    "#             if 'broadcaster' in globals():\n",
    "#                 broadcaster.update(aligned_transform)\n",
    "            \n",
    "#             # Create grasp transforms\n",
    "#             grasp_transform = aligned_transform @ transform_matrix_x_180\n",
    "#             grasp_transform_rotate = aligned_transform @ transform_matrix_x_180 @ transform_matrix_z_90\n",
    "            \n",
    "#             # Transform the detected cube point cloud for visualization\n",
    "#             cube_point_cloud_transformed = copy.deepcopy(deleted_pointcloud)\n",
    "#             cube_point_cloud_transformed = cube_point_cloud_transformed.transform(np.linalg.inv(grasp_transform))\n",
    "            \n",
    "#             # For generating a visual representation as a top-down view\n",
    "#             try:\n",
    "#                 cube_point_cloud_transformed_cubes = enlarge_points_as_cubes(cube_point_cloud_transformed)\n",
    "#                 cube_top_image = triangle_mesh_to_image(cube_point_cloud_transformed_cubes, image_size=(100, 100))\n",
    "#                 cube_top_image = (cube_top_image / cube_top_image.max() * 255).astype(np.uint8)\n",
    "#                 plt.imsave(f\"cube_{movecount+1}_top_view.png\", cube_top_image)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Warning: Could not generate top view image: {e}\")\n",
    "            \n",
    "#             # Generate gripper meshes for collision checking\n",
    "#             grasp_mesh, gripper_meshes_rotate = generate_gripper_from_transform(grasp_transform)\n",
    "            \n",
    "#             # Find a valid grasp without collisions\n",
    "#             grasp_final_matrix = None\n",
    "            \n",
    "#             # Check first grasping orientation\n",
    "#             if not check_grasp_collision(grasp_mesh, original_point_cloud):\n",
    "#                 grasp_final_matrix = grasp_transform\n",
    "#                 print(\"Using standard grasp orientation\")\n",
    "#             # Check alternative grasping orientation (rotated by 90° around Z)\n",
    "#             elif not check_grasp_collision(gripper_meshes_rotate, original_point_cloud):\n",
    "#                 grasp_final_matrix = grasp_transform_rotate\n",
    "#                 print(\"Using 90° rotated grasp orientation\")\n",
    "#             else:\n",
    "#                 print(\"Both grasp orientations have collisions, skipping this cube\")\n",
    "                \n",
    "#             # If a valid grasp pose was found\n",
    "#             if grasp_final_matrix is not None:\n",
    "#                 # Reset countdown\n",
    "#                 countdown = 50\n",
    "                \n",
    "#                 # Calculate RPY and position for robot movement\n",
    "#                 pick_rpy, pick_pos = matrix_to_rpy_and_translation(grasp_final_matrix)\n",
    "#                 approach_offset = [0, 0, 0.10]  # 10cm above the cube for approach\n",
    "#                 pick_pos_approach = [a + b for a, b in zip(pick_pos, approach_offset)]\n",
    "                \n",
    "#                 print(f\"Cube #{movecount+1} grasp position: {pick_pos}\")\n",
    "#                 print(f\"Approach position: {pick_pos_approach}\")\n",
    "#                 print(f\"RPY orientation: {pick_rpy}\")\n",
    "                \n",
    "#                 # Add the transform to our results\n",
    "#                 T.append(grasp_final_matrix)\n",
    "#                 movecount += 1\n",
    "                \n",
    "#                 # Optional visualization of the detected cube\n",
    "#                 # o3d.visualization.draw_geometries([deleted_pointcloud], window_name=f\"Detected Cube #{movecount}\")\n",
    "                \n",
    "#                 # Optional visualization of the grasp\n",
    "#                 # vis_geometries = grasp_mesh + [create_coordinate_frame(0.1)]\n",
    "#                 # o3d.visualization.draw_geometries(vis_geometries, window_name=f\"Grasp Visualization #{movecount}\")\n",
    "#         else:\n",
    "#             # If fitness is too low, don't change the countdown\n",
    "#             pass\n",
    "    \n",
    "#     print(f\"Found {len(T)} cubes from {50-countdown} iterations\")\n",
    "    \n",
    "#     # Print the final transforms\n",
    "#     for i, transform in enumerate(T):\n",
    "#         print(f\"Cube #{i+1} Transform:\")\n",
    "#         print(transform)\n",
    "        \n",
    "#     return T\n",
    "def pointcloud_process_2(point_cloud, slice_tolerance=0.005, cube_num=4):\n",
    "    '''\n",
    "    在点云中识别立方体并确定抓取的变换矩阵。\n",
    "    使用改进的Z轴对齐方法，保持XY平面的一致性，以减少对齐误差。\n",
    "    \n",
    "    参数：\n",
    "        point_cloud: 输入的点云，包含要检测的立方体\n",
    "        slice_tolerance: 用于切割点云的容差值\n",
    "        cube_num: 最大检测的立方体数量\n",
    "        \n",
    "    返回：\n",
    "        与检测到的可抓取立方体对应的变换矩阵列表\n",
    "    '''\n",
    "    original_point_cloud = copy.deepcopy(point_cloud)\n",
    "    T = []\n",
    "    remaining_pointcloud = point_cloud\n",
    "    remaining_pointcloud_count = len(point_cloud.points)\n",
    "    countdown = 50\n",
    "    movecount = 0\n",
    "\n",
    "    # 定义用于抓取方向的标准变换矩阵\n",
    "    transform_matrix_x_180 = np.array([\n",
    "        [1, 0, 0, 0],\n",
    "        [0, -1, 0, 0],\n",
    "        [0, 0, -1, 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    transform_matrix_z_90 = np.array([\n",
    "        [0, -1, 0, 0],\n",
    "        [1,  0, 0, 0],\n",
    "        [0,  0, 1, 0],\n",
    "        [0,  0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    # 迭代查找并提取立方体\n",
    "    while remaining_pointcloud_count > 50 and countdown > 0 and movecount < cube_num:\n",
    "        # 从立方体网格模型中采样点\n",
    "        cube_point_cloud = cube_mesh.sample_points_uniformly(number_of_points=50000) \n",
    "        cube_point_cloud = filter_point_cloud_by_depth(cube_point_cloud, depth_threshold=-0.01)\n",
    "\n",
    "        # 将立方体与当前剩余的场景点云进行配准\n",
    "        transform, remaining_pointcloud, deleted_pointcloud, fitness = register_and_filter(\n",
    "            remaining_pointcloud, cube_point_cloud\n",
    "        )\n",
    "\n",
    "        remaining_pointcloud_count = len(remaining_pointcloud.points)\n",
    "\n",
    "        # 如果匹配度超过最小阈值，则减少倒计时\n",
    "        if fitness > 0.01:\n",
    "            countdown -= 1\n",
    "\n",
    "        print(f\"迭代 {50 - countdown}: 匹配度 = {fitness:.4f}, 剩余点数: {remaining_pointcloud_count}\")\n",
    "\n",
    "        # 仅在匹配足够好时继续\n",
    "        if fitness > 0.40:  # 略微降低阈值以检测更多立方体\n",
    "            print(f\"正在处理立方体 #{movecount+1}\")\n",
    "\n",
    "            # 对齐Z轴向下，同时保持XY方向\n",
    "            aligned_transform = align_transform_z_axis_preserving_xy(transform)\n",
    "\n",
    "            # 更新变换广播器（用于RViz等可视化）\n",
    "            if 'broadcaster' in globals():\n",
    "                broadcaster.update(aligned_transform)\n",
    "\n",
    "            # 创建抓取变换\n",
    "            grasp_transform = aligned_transform @ transform_matrix_x_180\n",
    "            # grasp_transform_rotate = aligned_transform @ transform_matrix_x_180 @ transform_matrix_z_90\n",
    "            grasp_transform_rotate = aligned_transform @ transform_matrix_z_90  # DEBUG!\n",
    "            grasp_transform = aligned_transform         # DEBUG!\n",
    "\n",
    "            # 转换检测到的立方体点云（用于可视化或检查）\n",
    "            cube_point_cloud_transformed = copy.deepcopy(deleted_pointcloud)\n",
    "            cube_point_cloud_transformed = cube_point_cloud_transformed.transform(np.linalg.inv(grasp_transform))\n",
    "\n",
    "            # 尝试生成检测到的立方体的顶部视图图像\n",
    "            try:\n",
    "                cube_point_cloud_transformed_cubes = enlarge_points_as_cubes(cube_point_cloud_transformed)\n",
    "                cube_top_image = triangle_mesh_to_image(cube_point_cloud_transformed_cubes, image_size=(100, 100))\n",
    "                cube_top_image = (cube_top_image / cube_top_image.max() * 255).astype(np.uint8)\n",
    "                plt.imsave(f\"cube_{movecount+1}_top_view.png\", cube_top_image)\n",
    "            except Exception as e:\n",
    "                print(f\"警告: 无法生成顶部视图图像: {e}\")\n",
    "\n",
    "            # 为碰撞检查生成抓取器网格\n",
    "            grasp_mesh, gripper_meshes_rotate = generate_gripper_from_transform(grasp_transform)\n",
    "\n",
    "            # 尝试两种抓取方向，并选择没有碰撞的一种\n",
    "            grasp_final_matrix = None\n",
    "            \n",
    "            # 尝试标准抓取方向\n",
    "            if not check_grasp_collision(grasp_mesh, original_point_cloud):\n",
    "                grasp_final_matrix = grasp_transform\n",
    "                print(\"使用标准抓取方向\")\n",
    "            # 尝试旋转90°的抓取方向\n",
    "            elif not check_grasp_collision(gripper_meshes_rotate, original_point_cloud):\n",
    "                grasp_final_matrix = grasp_transform_rotate\n",
    "                print(\"使用旋转90°的抓取方向\")\n",
    "            else:\n",
    "                print(\"两种抓取方向都发生碰撞，跳过此立方体\")\n",
    "\n",
    "            # 如果找到有效的抓取，则保存该变换矩阵\n",
    "            if grasp_final_matrix is not None:\n",
    "                countdown = 50  # 重置倒计时\n",
    "\n",
    "                # 计算用于运动规划的RPY和位置\n",
    "                pick_rpy, pick_pos = matrix_to_rpy_and_translation(grasp_final_matrix)\n",
    "                approach_offset = [0, 0, 0.10]  # 立方体上方10cm处\n",
    "                pick_pos_approach = [a + b for a, b in zip(pick_pos, approach_offset)]\n",
    "\n",
    "                print(f\"立方体 #{movecount+1} 抓取位置: {pick_pos}\")\n",
    "                print(f\"接近位置: {pick_pos_approach}\")\n",
    "                print(f\"RPY方向: {pick_rpy}\")\n",
    "\n",
    "                # 将变换矩阵添加到结果列表\n",
    "                T.append(grasp_final_matrix)\n",
    "                movecount += 1\n",
    "\n",
    "                # 可选：显示抓取变换矩阵\n",
    "                print(\"抓取变换矩阵:\")\n",
    "                print(grasp_final_matrix)\n",
    "\n",
    "                # 可选：可视化抓取\n",
    "                vis_geometries = grasp_mesh\n",
    "                vis_geometries.append(create_coordinate_frame(0.1))\n",
    "                aligned_coordinate_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1)\n",
    "                # aligned_coordinate_frame.transform(aligned_transform)\n",
    "                aligned_coordinate_frame.transform(grasp_transform_rotate)\n",
    "                vis_geometries.append(aligned_coordinate_frame)\n",
    "                # vis_geometries.append(deleted_pointcloud)\n",
    "                # vis_geometries.append(remaining_pointcloud)\n",
    "                vis_geometries.append(original_point_cloud)\n",
    "\n",
    "                # 调用Open3D的可视化函数，传入正确格式的列表\n",
    "                o3d.visualization.draw_geometries(vis_geometries, window_name=f\"grasp visualization #{movecount}\")\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"在 {50 - countdown} 次迭代中找到 {len(T)} 个立方体\")\n",
    "\n",
    "    # 打印最终的抓取变换矩阵\n",
    "    for i, transform in enumerate(T):\n",
    "        print(f\"立方体 #{i+1} 变换:\")\n",
    "        print(transform)\n",
    "\n",
    "    return T\n",
    "           \n",
    "Ts = pointcloud_process(filtered_point_cloud)\n",
    "broadcaster.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a266f54-a76e-4263-8be7-958b208f8df2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Ts.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7be0151b-983a-4f9f-b066-f8220e1f2f6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import copy\n",
    "\n",
    "def generate_pascal_triangle_transforms(t, T):\n",
    "    \"\"\"\n",
    "    Generate a list of transformation matrices for a Pascal triangle arrangement.\n",
    "    Each cube's center position is used as the translation part of the transform matrix.\n",
    "    \"\"\"\n",
    "    level = 1\n",
    "    total = 1\n",
    "    while total < t:\n",
    "        level += 1\n",
    "        total += level\n",
    "\n",
    "    transforms = []\n",
    "    cube_size = 0.045  # 4.5 cm\n",
    "    spacing_xy = cube_size * 1.25  # Increase spacing to 1.25 times the cube size\n",
    "    spacing_z = cube_size * 1.05   # Use the same spacing in the vertical direction\n",
    "\n",
    "    current_pos = 0\n",
    "    for row in range(level-1, -1, -1):\n",
    "        for col in range(row + 1):\n",
    "            if current_pos >= t:\n",
    "                break\n",
    "\n",
    "            center_x = 0\n",
    "            center_y = (col - row/2) * spacing_xy\n",
    "            center_z = (level - 1 - row) * spacing_z\n",
    "\n",
    "            # Create local transformation matrix\n",
    "            local_transform = np.eye(4)\n",
    "            local_transform[:3, 3] = [center_x, center_y, center_z]\n",
    "\n",
    "            # Combine local transformation with T transformation\n",
    "            transform = np.dot(T, local_transform)\n",
    "            transforms.append(transform)\n",
    "            current_pos += 1\n",
    "\n",
    "        if current_pos >= t:\n",
    "            break\n",
    "\n",
    "    return transforms\n",
    "\n",
    "def create_coordinate_frame(size=0.1, transform=None):\n",
    "    frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=size)\n",
    "    if transform is not None:\n",
    "        frame.transform(transform)\n",
    "    return frame\n",
    "\n",
    "def visualize_pascal_triangle(transforms, T):\n",
    "    # Create a cube centered at the origin\n",
    "    cube = o3d.geometry.TriangleMesh.create_box(\n",
    "        width=0.045,\n",
    "        height=0.045, \n",
    "        depth=0.045\n",
    "    )\n",
    "    # Move the cube to be centered at the origin\n",
    "    cube.translate([-0.045/2, -0.045/2, -0.045/2])\n",
    "    cube.compute_vertex_normals()\n",
    "\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window()\n",
    "\n",
    "    # Add world coordinate frame\n",
    "    world_frame = create_coordinate_frame(size=0.2)\n",
    "    vis.add_geometry(world_frame)\n",
    "\n",
    "    # Add T coordinate frame\n",
    "    t_frame = create_coordinate_frame(size=0.2, transform=T)\n",
    "    vis.add_geometry(t_frame)\n",
    "\n",
    "    # Add all cubes and their local coordinate frames\n",
    "    for transform in transforms:\n",
    "        # Add cube\n",
    "        cube_copy = copy.deepcopy(cube)\n",
    "        cube_copy.transform(transform)\n",
    "        vis.add_geometry(cube_copy)\n",
    "        \n",
    "        # Add local coordinate frame\n",
    "        local_frame = create_coordinate_frame(size=0.05, transform=transform)\n",
    "        vis.add_geometry(local_frame)\n",
    "\n",
    "    opt = vis.get_render_option()\n",
    "    opt.background_color = np.asarray([0.5, 0.5, 0.5])\n",
    "\n",
    "    ctr = vis.get_view_control()\n",
    "    ctr.set_zoom(0.2)  # Adjust zoom to fit larger spacing\n",
    "    ctr.set_front([-0.8, -0.5, 0.5])\n",
    "    ctr.set_lookat([0, 0, 0])\n",
    "    ctr.set_up([0, 0, 1])\n",
    "\n",
    "    vis.run()\n",
    "    vis.destroy_window()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f396100-4bfb-4740-a839-6a5cef0fbb07",
   "metadata": {},
   "source": [
    "# 3. Motion Planning, movement, and grasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0ff91c9-6108-48f9-8132-1d623b646c89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import generate_pascal_triangle_transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67d90518-6b07-4d34-9b10-13a9a97c7c26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = Ts.__len__()\n",
    "# t = 2\n",
    "# 创建T矩阵（示例：绕Z轴旋转45度并平移）\n",
    "T = np.eye(4)\n",
    "theta = 0\n",
    "T[:3, :3] = np.array([\n",
    "    [np.cos(theta), -np.sin(theta), 0],\n",
    "    [np.sin(theta), np.cos(theta), 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "T[:3, 3] = [0.3, 0.00, 0]\n",
    "\n",
    "aim_transforms = generate_pascal_triangle_transforms(t, T)\n",
    "visualize_pascal_triangle(aim_transforms, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7bf932-3c72-44a4-90b3-559ba899174c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aim_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf7d104-3c62-454c-8cdc-25427abc1061",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8276ec04-7fa0-42e1-ae1c-32d03f30fbd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# demostrate the movement to the head of identified cubes\n",
    "# for T in Ts:\n",
    "#     pick_rpy, pick_pos = matrix_to_rpy_and_translation(T)\n",
    "#     pick_pos_ = [a + b for a, b in zip(pick_pos, [0.04, 0.00, 0.10])]\n",
    "#     pick_rpy = [a + b for a, b in zip(pick_rpy, [0, 0, 0])]\n",
    "#     print(pick_rpy, pick_pos)\n",
    "#     pick_place.move(pick_pos_, pick_rpy)\n",
    "# Modified movement demonstration code\n",
    "def demonstrate_movement_to_cubes(Ts, pick_place, offset=[0.00, 0.00, 0.10]):\n",
    "    for i, T in enumerate(Ts):\n",
    "        try:\n",
    "            pick_rpy, pick_pos = matrix_to_rpy_and_translation(T)\n",
    "            pick_pos_approach = [a + b for a, b in zip(pick_pos, offset)]\n",
    "            \n",
    "            try:\n",
    "                pick_place.move(pick_pos_approach, pick_rpy)\n",
    "                print(f\"Successfully moved to approach position for cube #{i+1}\")\n",
    "            except ValueError as e:\n",
    "                if \"too many values to unpack\" in str(e):\n",
    "                    print(f\"Note: Handled unpacking error in move operation for cube #{i+1}\")\n",
    "                else:\n",
    "                    raise\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing cube #{i+1}: {e}\")\n",
    "\n",
    "# Use the modified function\n",
    "demonstrate_movement_to_cubes(Ts, pick_place)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1ff911-c41c-4330-be82-95130ec626fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# move the minipulator to the aim position\n",
    "for T in aim_transforms:\n",
    "    transform_matrix_x_180 = np.array([\n",
    "        [1, 0, 0, 0],\n",
    "        [0, -1, 0, 0],\n",
    "        [0, 0, -1, 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "    transform_matrix_z_90 = np.array([\n",
    "        [0, -1, 0, 0],\n",
    "        [1, 0, 0, 0],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "    pick_rpy, pick_pos = matrix_to_rpy_and_translation(T@transform_matrix_x_180@transform_matrix_z_90)\n",
    "    pick_pos_ = [a + b for a, b in zip(pick_pos, [0.0, 0, 0.13])]\n",
    "    pick_rpy = [a + b for a, b in zip(pick_rpy, [0, 0, 0])]\n",
    "    pick_place.move(pick_pos_, pick_rpy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6b4027-9a96-484e-b6b0-c423c42cd075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # pick and place the first cube \n",
    "# transform_matrix_x_180 = np.array([\n",
    "#     [1, 0, 0, 0],\n",
    "#     [0, -1, 0, 0],\n",
    "#     [0, 0, -1, 0],\n",
    "#     [0, 0, 0, 1]\n",
    "# ])\n",
    "# transform_matrix_z_90 = np.array([\n",
    "#     [0, -1, 0, 0],\n",
    "#     [1, 0, 0, 0],\n",
    "#     [0, 0, 1, 0],\n",
    "#     [0, 0, 0, 1]\n",
    "# ])\n",
    "\n",
    "# pick_rpy, pick_pos = matrix_to_rpy_and_translation(Ts[0])\n",
    "# pick_pos_ = [a + b for a, b in zip(pick_pos, [0.00, 0, 0.00])]\n",
    "# pick_rpy = [a + b for a, b in zip(pick_rpy, [0, 0, 0])]\n",
    "    \n",
    "# place_rpy, place_pos = matrix_to_rpy_and_translation(aim_transforms[0]@transform_matrix_x_180@transform_matrix_z_90)\n",
    "# place_pos_ = [a + b for a, b in zip(place_pos, [0.0, 0, 0.04])]\n",
    "\n",
    "# pick_place.pick_and_place(\n",
    "#     pick_pos=pick_pos_,\n",
    "#     pick_rpy=pick_rpy,\n",
    "#     place_pos=place_pos_,\n",
    "#     place_rpy=place_rpy\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a662392-36bb-46d1-a2b3-ac8562db28ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [1742706222.450947, 8456.408000]: Table 'constraint_table' removed from the planning scene.\n",
      "[INFO] [1742706222.455592, 8456.413000]: Table 'constraint_table' removed from the planning scene.\n",
      "[INFO] [1742706222.457270, 8456.417000]: Move successful to position: [0.6423677718718348, -0.0995385747702704, 0.3300982383283828] and RPY: [-3.1343062117129175, -0.002137936669504681, 0.046522682292409384]\n",
      "[INFO] [1742706222.458194, 8456.419000]: Target position set to: [0.6423677718718348, -0.0995385747702704, 0.3300982383283828] and RPY: [-3.1343062117129175, -0.002137936669504681, 0.046522682292409384]\n",
      "[INFO] [1742706222.458993, 8456.420000]: Attempt 1 to plan Cartesian path...\n",
      "[INFO] [1742706222.461645, 8456.422000]: Path planning completed successfully!\n",
      "[INFO] [1742706222.462375, 8456.423000]: Target position set to: [0.6423677718718348, -0.0995385747702704, 0.030098238328382806] and RPY: [-3.1343062117129175, -0.002137936669504681, 0.046522682292409384]\n",
      "[INFO] [1742706222.463040, 8456.424000]: Executing Cartesian path...\n"
     ]
    }
   ],
   "source": [
    "# pick and place following cubes \n",
    "for i in range(0, Ts.__len__()):\n",
    "    ptransform_matrix_x_180 = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, -1, 0, 0],\n",
    "    [0, 0, -1, 0],\n",
    "    [0, 0, 0, 1]\n",
    "    ])\n",
    "    transform_matrix_z_90 = np.array([\n",
    "        [0, -1, 0, 0],\n",
    "        [1, 0, 0, 0],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    pick_rpy, pick_pos = matrix_to_rpy_and_translation(Ts[i])\n",
    "    pick_pos_ = [a + b for a, b in zip(pick_pos, [0.0, 0, 0.008])]\n",
    "    pick_rpy = [a + b for a, b in zip(pick_rpy, [0, 0, 0])]\n",
    "\n",
    "    place_rpy, place_pos = matrix_to_rpy_and_translation(aim_transforms[i]@transform_matrix_x_180@transform_matrix_z_90)\n",
    "    place_pos_ = [a + b for a, b in zip(place_pos, [0.0, 0, 0.05])]\n",
    "\n",
    "    pick_place.pick_and_place(\n",
    "        pick_pos=pick_pos_,\n",
    "        pick_rpy=pick_rpy,\n",
    "        place_pos=place_pos_,\n",
    "        place_rpy=place_rpy\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae12895-f189-48d9-98a8-7f5b4ce8f8dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28a61a9-5a2d-46d9-85f8-6e0d2d2e66e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb28b7de-2a7b-447b-b92c-f9487c4a8977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344eea87-4f39-40cd-adcb-2d3490950865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b49a45-b76a-4109-a1b9-92473eb0cc5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90752b1-45fd-40c6-95e6-5c641111580b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e628f0-7d4c-4612-8453-cc769bb96ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0b93ec-fc14-4c5b-a51b-ddd187495ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cf24a9-2107-4c18-8b75-410152602d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97c4cfa-2497-47fb-abcc-b4d8102d5a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97a75a1-7a1b-4df9-87e5-f22fc098e059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916923c9-2c36-417b-bb2a-7a07e665e320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1aa9b1-890b-4a21-a656-5c54a75d5c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e1942-89b9-460e-8013-c3406e5ec660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f97b18-cf6e-4683-b2b0-b18f61f0c335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84bdaf3-eb89-4acf-aa35-34d10d9ff810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72f5be6-10ab-4a45-9860-f3cdf9737da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc74575-a184-436e-b10f-6e5cbd701094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
