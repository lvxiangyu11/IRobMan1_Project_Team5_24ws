{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95482eee-ad8f-423f-a497-8990e8ac372a",
   "metadata": {},
   "source": [
    "# 1. record point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb0fd4c1-97e3-4984-a270-edddc5a05230",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Python Path:\n",
      "\n",
      "/opt/ros_ws/devel/lib/python3/dist-packages\n",
      "/opt/ros/noetic/lib/python3/dist-packages\n",
      "/usr/lib/python38.zip\n",
      "/usr/lib/python3.8\n",
      "/usr/lib/python3.8/lib-dynload\n",
      "/opt/ros_ws/irobmanenv/lib/python3.8/site-packages\n",
      "/opt/ros_ws/src/franka_zed_gazebo/scripts/mycode_clean/3_move\n",
      "/opt/ros_ws/src/franka_zed_gazebo/scripts/mycode_clean/1_getPointCloud\n",
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Failed to import pyassimp, see https://github.com/moveit/moveit/issues/86 for more info\n",
      "[INFO] [1742505547.305343, 0.000000]: Waiting for move_group action server...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[ WARN] [1742505547.326312375]: Link zed2_holder has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m[ INFO] [1742505547.309115297]: Loading robot model 'panda'...\u001b[0m\n",
      "\u001b[0m[ INFO] [1742505548.458332576, 65.830000000]: Ready to take commands for planning group panda_manipulator.\u001b[0m\n",
      "[INFO] [1742505548.461734, 0.000000]: Table added to the scene to prevent collision below z = 0.001\n",
      "[INFO] [1742505548.466850, 0.000000]: Wall 'wall_right' added to the scene with rotation theta=-0.6283185307179586 radians at position: 0.0, 0.8, 0.0\n",
      "[INFO] [1742505548.471585, 0.000000]: Wall 'wall_left' added to the scene with rotation theta=0.6283185307179586 radians at position: 0.0, -0.8, 0.0\n",
      "[INFO] [1742505548.473098, 65.844000]: MoveRobot initialized successfully.\n",
      "[INFO] [1742505548.697006, 66.053000]: Current joint values: [-0.00012665259453914501, -0.7841008669968561, 2.384224113693989e-05, -2.3586074891815416, 5.028194695277932e-06, 1.571337459363649, 0.7853932313514482]\n",
      "[INFO] [1742505548.710177, 66.081000]: Waiting for gripper action servers...\n",
      "[INFO] [1742505549.015616, 66.386000]: Gripper action servers ready.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory (used to replace __file__)\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "target_path1 = os.path.abspath(os.path.join(current_dir, '../3_move'))\n",
    "target_path2 = os.path.abspath(os.path.join(current_dir, '../1_getPointCloud'))\n",
    "\n",
    "# Add the paths to sys.path\n",
    "if target_path1 not in sys.path:\n",
    "    sys.path.append(target_path1)\n",
    "    \n",
    "if target_path2 not in sys.path:\n",
    "    sys.path.append(target_path2)\n",
    "    \n",
    "# Check if the paths were added successfully\n",
    "print(\"Current Python Path:\")\n",
    "print(\"\\n\".join(sys.path))\n",
    "\n",
    "from ImageRecognizer import ImageRecognizer\n",
    "image_recognizer = ImageRecognizer(top_dir=\"/opt/ros_ws/src/franka_zed_gazebo/scripts/mycode_clean/2_perception/cubes/\")\n",
    "\n",
    "from utils import matrix_to_rpy_and_translation\n",
    "\n",
    "from PickAndPlace import PickAndPlace\n",
    "pick_place = PickAndPlace(approach_distance=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c575264-32a4-4fc0-9469-5ce80c9a98fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [1742505557.702605, 75.057000]: Waiting for data...\n",
      "[INFO] [1742505557.881164, 75.227000]: Received image message.\n",
      "[INFO] [1742505557.924291, 75.242000]: Received point cloud data.\n",
      "[INFO] [1742505560.753144, 77.888000]: Color range - Min: [0. 0. 0.], Max: [0. 0. 0.]\n",
      "[INFO] [1742505560.757512, 77.894000]: Requesting transform from world to left_camera_link_optical...\n",
      "[INFO] [1742505560.760732, 77.896000]: Transform found: header: \n",
      "  seq: 0\n",
      "  stamp: \n",
      "    secs: 77\n",
      "    nsecs: 867000000\n",
      "  frame_id: \"world\"\n",
      "child_frame_id: \"left_camera_link_optical\"\n",
      "transform: \n",
      "  translation: \n",
      "    x: 0.209730500767132\n",
      "    y: -0.06001897684886969\n",
      "    z: 0.5616907560719661\n",
      "  rotation: \n",
      "    x: 0.6588839359701193\n",
      "    y: 0.6588178777650942\n",
      "    z: 0.25672275854739307\n",
      "    w: 0.25675744994340016\n",
      "[INFO] [1742505561.154503, 78.257000]: Transformed point cloud saved to /opt/ros_ws/src/franka_zed_gazebo/scripts/mycode_clean/2_perception/mesh/zed_point_cloud_world3.ply\n"
     ]
    }
   ],
   "source": [
    "from save_point_cloud import PointCloudSaver\n",
    "import open3d as o3d\n",
    "import rospy\n",
    "point_cloud_saver = PointCloudSaver()\n",
    "\n",
    "# Wait for data to be ready\n",
    "rospy.loginfo(\"Waiting for data...\")\n",
    "rospy.sleep(1)  # Wait for topic data to be published\n",
    "\n",
    "# Save the point cloud\n",
    "world_file = \"/opt/ros_ws/src/franka_zed_gazebo/scripts/mycode_clean/2_perception/mesh/zed_point_cloud_world3.ply\"\n",
    "point_cloud_saver.save_point_clouds(world_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ebdb0eb-9e99-4745-88a8-52bfcc7225d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from utils import filter_point_cloud_by_depth_and_range, filter_point_cloud_by_depth\n",
    "\n",
    "# zed_ply_path = \"mesh/zed_point_cloud_world3.ply\"\n",
    "zed_ply_path = \"/opt/ros_ws/src/franka_zed_gazebo/scripts/mycode_clean/2_perception/mesh/zed_point_cloud_world3.ply\"\n",
    "\n",
    "coordinate_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(\n",
    "    size=0.1,  # Size of the coordinate axes, can be adjusted as needed\n",
    "    origin=[0, 0, 0]  # Origin of the coordinate axes\n",
    ")\n",
    "\n",
    "# Read the point cloud file\n",
    "point_cloud = o3d.io.read_point_cloud(zed_ply_path)\n",
    "if not point_cloud.has_points():\n",
    "    raise ValueError(f\"Failed to read point cloud from {zed_ply_path}\")\n",
    "filtered_point_cloud = filter_point_cloud_by_depth_and_range(point_cloud, depth_threshold=0.02, range=[0.001, -0.8, 1, 1.6])\n",
    "o3d.visualization.draw_geometries([filtered_point_cloud, coordinate_frame], window_name=\"Filtered Point Cloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9883e9d-4bce-412e-ad0a-224f0121b514",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from utils import calculate_max_layer\n",
    "\n",
    "# # Example usage\n",
    "# max_layer = calculate_max_layer(filtered_point_cloud, layer_height=0.04)\n",
    "# print(f\"MaxLayer: {max_layer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d5b48e-3edb-4683-a774-6ef27ecad23b",
   "metadata": {},
   "source": [
    "# 2. Perception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f7d2ab-d25d-4387-ac97-81de3ec741dd",
   "metadata": {},
   "source": [
    "## 2.1 Coarse and Fine registration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88fe104e-b675-4313-93a1-417cdc00c8cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "def register_and_filter(pointcloud, mesh, voxel_size=0.01):\n",
    "    # Convert mesh to point cloud\n",
    "    if isinstance(mesh, o3d.geometry.TriangleMesh):\n",
    "        mesh_pointcloud = mesh.sample_points_uniformly(number_of_points=1000)\n",
    "    elif isinstance(mesh, o3d.geometry.PointCloud):\n",
    "        mesh_pointcloud = copy.deepcopy(mesh)\n",
    "    \n",
    "    # Downsample the point cloud and compute features\n",
    "    def preprocess_point_cloud(pcd, voxel_size):\n",
    "        pcd_down = pcd.voxel_down_sample(voxel_size)\n",
    "        pcd_down.estimate_normals(\n",
    "            search_param=o3d.geometry.KDTreeSearchParamHybrid(\n",
    "                radius=voxel_size*2, \n",
    "                max_nn=30\n",
    "            )\n",
    "        )\n",
    "        pcd_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "            pcd_down,\n",
    "            search_param=o3d.geometry.KDTreeSearchParamHybrid(\n",
    "                radius=voxel_size*5, \n",
    "                max_nn=100\n",
    "            )\n",
    "        )\n",
    "        return pcd_down, pcd_fpfh\n",
    "\n",
    "    # Coarse registration\n",
    "    def execute_global_registration(source_down, target_down, source_fpfh, target_fpfh, voxel_size):\n",
    "        distance_threshold = voxel_size * 1.5\n",
    "        result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
    "            source_down, target_down, source_fpfh, target_fpfh, True,\n",
    "            distance_threshold,\n",
    "            o3d.pipelines.registration.TransformationEstimationPointToPoint(False),\n",
    "            4,\n",
    "            [\n",
    "                o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(0.9),\n",
    "                o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(distance_threshold)\n",
    "            ],\n",
    "            o3d.pipelines.registration.RANSACConvergenceCriteria(4000000, 500)\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    # Fine registration\n",
    "    def refine_registration(source, target, initial_transformation, voxel_size):\n",
    "        distance_threshold = voxel_size * 1  # Reduce the threshold for higher accuracy\n",
    "        result = o3d.pipelines.registration.registration_icp(\n",
    "            source, target, distance_threshold, initial_transformation,\n",
    "            o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "            o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=1000000)\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    # Execute point cloud preprocessing\n",
    "    source_down, source_fpfh = preprocess_point_cloud(mesh_pointcloud, voxel_size)\n",
    "    target_down, target_fpfh = preprocess_point_cloud(pointcloud, voxel_size)\n",
    "\n",
    "    # Execute registration\n",
    "    coarse_result = execute_global_registration(source_down, target_down, source_fpfh, target_fpfh, voxel_size)\n",
    "    refined_result = refine_registration(mesh_pointcloud, pointcloud, coarse_result.transformation, voxel_size)\n",
    "\n",
    "    # Transform the mesh point cloud\n",
    "    transform = refined_result.transformation\n",
    "    transformed_mesh_pointcloud = mesh_pointcloud.transform(transform)\n",
    "\n",
    "    # Create bounding box\n",
    "    oriented_bounding_box = transformed_mesh_pointcloud.get_oriented_bounding_box()\n",
    "    center = oriented_bounding_box.center\n",
    "    extent = oriented_bounding_box.extent\n",
    "    rotation_matrix = oriented_bounding_box.R\n",
    "\n",
    "    # Expand the bounding box\n",
    "    margin = voxel_size\n",
    "    expanded_extent = extent + 0.5 * margin\n",
    "    expanded_bounding_box = o3d.geometry.OrientedBoundingBox(\n",
    "        center=center,\n",
    "        extent=expanded_extent,\n",
    "        R=rotation_matrix\n",
    "    )\n",
    "\n",
    "    # Filter the point cloud\n",
    "    indices_inside_box = expanded_bounding_box.get_point_indices_within_bounding_box(pointcloud.points)\n",
    "    indices_outside_box = list(set(range(len(pointcloud.points))) - set(indices_inside_box))\n",
    "\n",
    "    # Separate the point cloud\n",
    "    remaining_pointcloud = pointcloud.select_by_index(indices_outside_box)\n",
    "    deleted_pointcloud = pointcloud.select_by_index(indices_inside_box)\n",
    "\n",
    "    return transform, remaining_pointcloud, deleted_pointcloud, refined_result.fitness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97050aea-fb60-4b73-b5ee-14d1b5891787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the cube\n",
    "# Define file paths\n",
    "cube_obj_path = \"mesh/cube_0.obj\"\n",
    "zed_ply_path = \"mesh/zed_point_cloud_world3.ply\"\n",
    "\n",
    "# Read the cube_0.obj mesh\n",
    "cube_mesh = o3d.io.read_triangle_mesh(cube_obj_path)\n",
    "cube_mesh.compute_vertex_normals()  # Compute normals for better visualization\n",
    "# cube_point_cloud = cube_mesh.sample_points_uniformly(number_of_points=50000)  # Convert to point cloud\n",
    "# cube_point_cloud = cube_mesh.sample_points_poisson_disk(number_of_points=1000)\n",
    "\n",
    "# Remove the lower part of the cube to prevent flipping along the z-axis\n",
    "# cube_point_cloud = filter_point_cloud_by_depth(cube_point_cloud, depth_threshold=-0.015)\n",
    "# o3d.visualization.draw_geometries([cube_point_cloud])\n",
    "\n",
    "\n",
    "coordinate_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(\n",
    "    size=0.1,  # Size of the coordinate axes, can be adjusted as needed\n",
    "    origin=[0, 0, 0]  # Origin of the coordinate axes\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54f3fe34-f397-4e76-8452-b24a6cf4e9f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_transform_z_axis_alignment(transform, tolerance=0.1):\n",
    "    \"\"\"\n",
    "    Check if the Z-axis of the transform is parallel to the Z-axis of the world coordinate system.\n",
    "    Allows a certain tolerance range to check if it is parallel or anti-parallel.\n",
    "    \n",
    "    Args:\n",
    "    - transform: 4x4 transformation matrix\n",
    "    - tolerance: Tolerance range for checking, default is 0.1\n",
    "    \n",
    "    Returns:\n",
    "    - True: If the Z-axis is parallel or anti-parallel\n",
    "    - False: If the Z-axis is not parallel\n",
    "    - The corrected transform\n",
    "    \"\"\"\n",
    "    z_axis = np.array([0, 0, 1])  # Z-axis of the world coordinate system\n",
    "    transform_z_axis = transform[:3, 2]  # Get the Z-axis of the transform (i.e., the third column of the rotation matrix)\n",
    "\n",
    "    # Compute the angle between the transform's Z-axis and the world coordinate system's Z-axis\n",
    "    dot_product = np.dot(transform_z_axis, z_axis)\n",
    "    # Compute the cosine of the angle, if close to 1 or -1, it means parallel or anti-parallel\n",
    "    if np.abs(dot_product) > (1 - tolerance):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def align_transform_z_axis(transform):\n",
    "    \"\"\"\n",
    "    If the Z-axis of the transform is anti-parallel to the Z-axis of the world coordinate system,\n",
    "    rotate by 180 degrees around the X-axis (np.pi) to flip the direction of the Z-axis.\n",
    "    \n",
    "    Args:\n",
    "    - transform: 4x4 transformation matrix\n",
    "    \n",
    "    Returns:\n",
    "    - The corrected transform matrix\n",
    "    \"\"\"\n",
    "    z_axis_world = np.array([0, 0, 1])  # Z-axis of the world coordinate system\n",
    "    transform_z_axis = transform[:3, 2]  # Get the Z-axis of the transform (the third column of the rotation matrix)\n",
    "\n",
    "    # Check if the Z-axis is anti-parallel to the world coordinate system's Z-axis\n",
    "    if np.dot(transform_z_axis, z_axis_world) < 0:  # Z-axis is anti-parallel\n",
    "        print(\"Correcting Z axis\")\n",
    "        # Create a rotation matrix to rotate 180 degrees around the X-axis\n",
    "        rotation_matrix = np.eye(4)\n",
    "        rotation_matrix[1, 1] = -1  # Rotate the matrix by 180 degrees around the X-axis\n",
    "        rotation_matrix[2, 2] = -1  # Rotate the matrix by 180 degrees around the X-axis\n",
    "        \n",
    "        # Perform matrix multiplication, applying the rotation matrix to the original transform\n",
    "        transform = np.dot(rotation_matrix, transform)\n",
    "\n",
    "    return transform\n",
    "\n",
    "def align_transform_z_axis_any_orientation(transform):\n",
    "    \"\"\"\n",
    "    Aligns the Z-axis of the transform with the negative Z-axis of the world coordinate system,\n",
    "    no matter what the original orientation is.\n",
    "    \n",
    "    Args:\n",
    "    - transform: 4x4 transformation matrix\n",
    "    \n",
    "    Returns:\n",
    "    - The corrected transform matrix with Z-axis pointing downward\n",
    "    \"\"\"\n",
    "    # Extract the original rotation matrix and translation vector\n",
    "    rotation = transform[:3, :3]\n",
    "    translation = transform[:3, 3]\n",
    "    \n",
    "    # Extract the current z-axis direction (third column of rotation matrix)\n",
    "    current_z = rotation[:, 2]\n",
    "    \n",
    "    # Target z-axis direction (downward in world coordinates)\n",
    "    target_z = np.array([0, 0, -1])\n",
    "    \n",
    "    # Check if already aligned within a small tolerance\n",
    "    if np.allclose(current_z, target_z, atol=1e-6):\n",
    "        return transform\n",
    "    \n",
    "    # For completely anti-aligned case (pointing directly up), simple 180° rotation works\n",
    "    if np.allclose(current_z, -target_z, atol=1e-6):\n",
    "        # Rotate 180 degrees around x-axis\n",
    "        r = np.eye(3)\n",
    "        r[1, 1] = -1\n",
    "        r[2, 2] = -1\n",
    "        new_rotation = rotation @ r\n",
    "    else:\n",
    "        # For any other orientation, we need to find the rotation that aligns vectors\n",
    "        # Compute the rotation axis (cross product of current and target z-axes)\n",
    "        rotation_axis = np.cross(current_z, target_z)\n",
    "        \n",
    "        # If current_z and target_z are parallel or anti-parallel, rotation_axis might be zero\n",
    "        # In that case, choose any perpendicular axis\n",
    "        if np.allclose(rotation_axis, 0, atol=1e-10):\n",
    "            # Find a non-zero component in current_z\n",
    "            if abs(current_z[0]) > 1e-10:\n",
    "                rotation_axis = np.array([current_z[1], -current_z[0], 0])\n",
    "            else:\n",
    "                rotation_axis = np.array([0, current_z[2], -current_z[1]])\n",
    "        \n",
    "        # Normalize the rotation axis\n",
    "        rotation_axis = rotation_axis / np.linalg.norm(rotation_axis)\n",
    "        \n",
    "        # Compute the rotation angle (dot product gives cosine of angle)\n",
    "        # We want the shorter arc between the vectors\n",
    "        cos_angle = np.dot(current_z, target_z)\n",
    "        angle = np.arccos(np.clip(cos_angle, -1.0, 1.0))\n",
    "        \n",
    "        # Rodrigues' rotation formula to get rotation matrix\n",
    "        K = np.array([\n",
    "            [0, -rotation_axis[2], rotation_axis[1]],\n",
    "            [rotation_axis[2], 0, -rotation_axis[0]],\n",
    "            [-rotation_axis[1], rotation_axis[0], 0]\n",
    "        ])\n",
    "        R = np.eye(3) + np.sin(angle) * K + (1 - np.cos(angle)) * (K @ K)\n",
    "        \n",
    "        # Apply the rotation to the original rotation matrix\n",
    "        new_rotation = rotation @ R\n",
    "    \n",
    "    # Create the new transformation matrix\n",
    "    new_transform = np.eye(4)\n",
    "    new_transform[:3, :3] = new_rotation\n",
    "    new_transform[:3, 3] = translation\n",
    "    \n",
    "    # Validate the result\n",
    "    result_z = new_transform[:3, :3][:, 2]\n",
    "    alignment_quality = np.dot(result_z, target_z)\n",
    "    print(f\"Z-axis alignment quality: {alignment_quality:.6f} (closer to 1 is better)\")\n",
    "    \n",
    "    return new_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76d4d048-43c7-4a8a-ba1b-9c8e9fadcfa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import rospy\n",
    "import tf\n",
    "import numpy as np\n",
    "from geometry_msgs.msg import TransformStamped\n",
    "\n",
    "class TransformBroadcaster:\n",
    "    def __init__(self):\n",
    "        # Try to initialize the ROS node, avoid initializing multiple times\n",
    "        try:\n",
    "            rospy.init_node('tf_broadcaster_node')\n",
    "        except rospy.exceptions.ROSException:\n",
    "            pass  # If the node is already initialized, do nothing\n",
    "\n",
    "        # Create a TransformBroadcaster instance\n",
    "        self.br = tf.TransformBroadcaster()\n",
    "\n",
    "        # Assume T is the given 4x4 transformation matrix\n",
    "        self.T = np.ones((4, 4))  # Set to a 4x4 matrix\n",
    "\n",
    "        # Set a timer to call the broadcast_transform function every 100 milliseconds\n",
    "        self.timer = rospy.Timer(rospy.Duration(0.1), self.broadcast_transform)\n",
    "\n",
    "        # Store the timestamp of the last sent transformation\n",
    "        self.last_sent_time = None\n",
    "\n",
    "    def broadcast_transform(self, event):\n",
    "        try:\n",
    "            # Extract the translation and rotation parts from the 4x4 matrix\n",
    "            translation = self.T[0:3, 3]  # Translation part (x, y, z)\n",
    "            rotation_matrix = self.T[0:3, 0:3]  # Rotation matrix part\n",
    "\n",
    "            # Create a complete 4x4 matrix, including rotation and homogeneous coordinates\n",
    "            full_matrix = np.eye(4)\n",
    "            full_matrix[0:3, 0:3] = rotation_matrix\n",
    "            full_matrix[0:3, 3] = translation\n",
    "\n",
    "            # Create a quaternion to represent the rotation\n",
    "            quaternion = tf.transformations.quaternion_from_matrix(full_matrix)\n",
    "\n",
    "            # Get the current timestamp\n",
    "            current_time = rospy.Time.now()\n",
    "\n",
    "            # Check if the last sent timestamp and the current timestamp are the same\n",
    "            if self.last_sent_time is None or current_time != self.last_sent_time:\n",
    "                # Publish the transformation\n",
    "                self.br.sendTransform(\n",
    "                    (translation[0], translation[1], translation[2]),  # Translation part\n",
    "                    (quaternion[0], quaternion[1], quaternion[2], quaternion[3]),  # Rotation part (quaternion)\n",
    "                    current_time,  # Use the current timestamp\n",
    "                    \"cube\",  # Child frame name\n",
    "                    \"world\"   # Parent frame name\n",
    "                )\n",
    "                # Update the last sent timestamp\n",
    "                self.last_sent_time = current_time\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def update(self, T):\n",
    "        self.T = T\n",
    "        \n",
    "    def stop(self):\n",
    "        # Stop the timer\n",
    "        self.timer.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4cef69-b1d4-40d5-92e0-ad3c6f59fc93",
   "metadata": {},
   "source": [
    "## 2.2 Grasp Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "155cbbb8-ad8b-4146-a453-4cdaf9afbad5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import *  # Assuming the create_grasp_mesh function is in utils.py\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "\n",
    "def generate_gripper_from_transform(T: np.ndarray):\n",
    "    \"\"\"\n",
    "    Generates a robotic gripper mesh from a given 4x4 transformation matrix,\n",
    "    with additional rotations around x and y axes.\n",
    "\n",
    "    Args:\n",
    "        T: 4x4 transformation matrix (numpy array).\n",
    "        \n",
    "    Returns:\n",
    "        gripper_meshes: List of meshes representing the gripper.\n",
    "    \"\"\"\n",
    "    # Extract the rotation matrix (3x3)\n",
    "    rotation_matrix = T[:3, :3]\n",
    "\n",
    "    # Extract the translation vector\n",
    "    translation = T[:3, 3]\n",
    "\n",
    "    # Set the gripper's center point position, usually the translation vector\n",
    "    center_point = translation\n",
    "\n",
    "    # Create a rotation matrix for -90 degrees around the x-axis\n",
    "    R_x = np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, np.cos(-np.pi/2), -np.sin(-np.pi/2)],\n",
    "        [0, np.sin(-np.pi/2), np.cos(-np.pi/2)]\n",
    "    ])\n",
    "\n",
    "    # Create a rotation matrix for 90 degrees around the y-axis\n",
    "    R_y = np.array([\n",
    "        [np.cos(np.pi/2), 0, np.sin(np.pi/2)],\n",
    "        [0, 1, 0],\n",
    "        [-np.sin(np.pi/2), 0, np.cos(np.pi/2)]\n",
    "    ])\n",
    "    \n",
    "    R_z = np.array([\n",
    "        [0, -1, 0],\n",
    "        [1, 0, 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    # Combine rotation matrices, first rotate around the x-axis, then around the y-axis\n",
    "    combined_rotation = R_z @ rotation_matrix @ R_x \n",
    "\n",
    "    # Call create_grasp_mesh function to generate the gripper\n",
    "    gripper_meshes = create_grasp_mesh(\n",
    "        center_point=center_point, \n",
    "        rotation_matrix=combined_rotation,\n",
    "        width=0.25\n",
    "    )\n",
    "    # Call create_grasp_mesh function to generate the gripper with a different rotation\n",
    "    gripper_meshes_rotate = create_grasp_mesh(\n",
    "        center_point=center_point, \n",
    "        rotation_matrix=rotation_matrix @ R_x,\n",
    "        width=0.25\n",
    "    )\n",
    "\n",
    "    return gripper_meshes, gripper_meshes_rotate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47315293-9f5e-4dd0-a1aa-9efab7885c1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    }
   ],
   "source": [
    "# Test code: Pass in a 4x4 transformation matrix\n",
    "T = np.array([\n",
    "    [1, 0, 0, 0.1],  # Rotation matrix and translation\n",
    "    [0, 1, 0, 0.2],\n",
    "    [0, 0, 1, 0.3],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "# Call the function to generate the gripper\n",
    "gripper_meshes, _ = generate_gripper_from_transform(T)\n",
    "\n",
    "# Visualize the generated gripper\n",
    "o3d.visualization.draw_geometries(gripper_meshes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8067b9a2-8d15-4f9e-9842-e1d6815c3d34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_grasp_collision(\n",
    "    grasp_meshes: Sequence[o3d.geometry.TriangleMesh],\n",
    "    object_pcd: o3d.geometry.TriangleMesh,\n",
    "    num_colisions: int = 10,\n",
    "    tolerance: float = 0.00001\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Checks for collisions between a gripper grasp pose and target object\n",
    "    using point cloud sampling.\n",
    "\n",
    "    Args:\n",
    "        grasp_meshes: List of mesh geometries representing the gripper components\n",
    "        object_mesh: Triangle mesh of the target object\n",
    "        num_collisions: Threshold on how many points to check\n",
    "        tolerance: Distance threshold for considering a collision (in meters)\n",
    "\n",
    "    Returns:\n",
    "        bool: True if collision detected between gripper and object, False otherwise\n",
    "    \"\"\"\n",
    "    # Combine gripper meshes\n",
    "    combined_gripper = o3d.geometry.TriangleMesh()\n",
    "    for mesh in grasp_meshes:\n",
    "        combined_gripper += mesh  # Combine multiple gripper meshes\n",
    "\n",
    "    # Sample points from both meshes\n",
    "    num_points = 5000  # Sample 5000 points from both gripper and target object\n",
    "    #######################TODO#######################\n",
    "    # Uniformly sample point clouds from both the gripper and object meshes\n",
    "    gripper_pcd = combined_gripper.sample_points_uniformly(number_of_points=num_points)\n",
    "    gripper_points = np.asarray(gripper_pcd.points)  # Point coordinates of the gripper point cloud\n",
    "    object_points = np.asarray(object_pcd.points)  # Point coordinates of the target object point cloud\n",
    "    ##################################################\n",
    "    \n",
    "    # Build KDTree for object points\n",
    "    is_collision = False\n",
    "    #######################TODO#######################\n",
    "    collision_count = 0\n",
    "    # Build a KDTree for the target object point cloud\n",
    "    object_kdtree = o3d.geometry.KDTreeFlann(object_pcd)\n",
    "    for gripper_point in gripper_points:\n",
    "        # For each gripper point, find the nearest point in the target object point cloud\n",
    "        _, _, distances = object_kdtree.search_knn_vector_3d(gripper_point, 1)  # Find the nearest neighbor\n",
    "        \n",
    "        # If the distance to the nearest neighbor is less than the tolerance, consider it a collision\n",
    "        if distances[0] <= tolerance:\n",
    "            collision_count += 1\n",
    "            \n",
    "            # Exit early if enough collisions are detected\n",
    "            if collision_count >= num_colisions:\n",
    "                is_collision = True\n",
    "                break\n",
    "    #######################TODO#######################\n",
    "\n",
    "    return is_collision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed7a038-40aa-4a06-a50d-9abae0aba06f",
   "metadata": {},
   "source": [
    "## 2.3 identify image of cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9fcf074-462c-450a-921d-b9589f4b627f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# from io import BytesIO\n",
    "# from PIL import Image\n",
    "# import base64\n",
    "# import json\n",
    "# SYSTEM_PROMPT = \"\"\"Please act as an image recognition agent. \n",
    "# You will be given a square face of a block, \n",
    "# which is projected from a point cloud. \n",
    "# Your task is to recognize the following:\n",
    "\n",
    "# Determine if this is a block face.\n",
    "# Each face contains only one letter, \n",
    "# one pattern (just detect whether it's a pattern, no need to identify the exact pattern), \n",
    "# or is blank (only wood texture). \n",
    "# Please detect whether it is a letter, \n",
    "# a pattern, or blank. \n",
    "# Each of these may be rotated. \n",
    "# Please analyze all possible rotations in a clockwise direction: 0°, 90°, 180°, and 270°.\n",
    "# There might be a circular border around the face. \n",
    "# Please detect if this border exists. \n",
    "# It's confirmed that the color of the border matches the color of the letter or pattern.\n",
    "# The expected output is a JSON in the following format:\n",
    "# {\n",
    "#     \"check\": true/false, \n",
    "#     \"c\": char/\"pattern\"/\"blank\", \n",
    "#     \"color\": \"green\"/\"yellow\"/\"red\"/\"blue\"/\"None\", \n",
    "#     \"rotation\": 0/90/180/270, \n",
    "#     \"circle\": true/false\n",
    "# }\n",
    "# \"\"\"\n",
    "\n",
    "# api_key=\"\"\n",
    "# client = OpenAI(api_key=api_key)\n",
    "\n",
    "# def encode_image(image, quality=100):\n",
    "#     if image.mode != 'RGB':\n",
    "#         image = image.convert('RGB')  # Convert to RGB\n",
    "#     buffered = BytesIO()\n",
    "#     image.save(buffered, format=\"JPEG\", quality=quality) \n",
    "#     return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "# def gpt4o_analysis(image_path, quality=50):\n",
    "#     with Image.open(image_path) as img:\n",
    "#         img_b64_str = encode_image(img, quality=quality)\n",
    "#     img_type = \"image/jpeg\"\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=\"gpt-4o-mini\",\n",
    "#         messages=[\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": [\n",
    "#                     {\"type\": \"text\", \"text\": SYSTEM_PROMPT},\n",
    "#                     {\n",
    "#                         \"type\": \"image_url\",\n",
    "#                         \"image_url\": {\"url\": f\"data:{img_type};base64,{img_b64_str}\"},\n",
    "#                     },\n",
    "#                 ],\n",
    "#             }from openai import OpenAI\n",
    "# from io import BytesIO\n",
    "# from PIL import Image\n",
    "# import base64\n",
    "# import json\n",
    "# SYSTEM_PROMPT = \"\"\"Please act as an image recognition agent. \n",
    "# You will be given a square face of a block, \n",
    "# which is projected from a point cloud. \n",
    "# Your task is to recognize the following:\n",
    "\n",
    "# Determine if this is a block face.\n",
    "# Each face contains only one letter, \n",
    "# one pattern (just detect whether it's a pattern, no need to identify the exact pattern), \n",
    "# or is blank (only wood texture). \n",
    "# Please detect whether it is a letter, \n",
    "# a pattern, or blank. \n",
    "# Each of these may be rotated. \n",
    "# Please analyze all possible rotations in a clockwise direction: 0°, 90°, 180°, and 270°.\n",
    "# There might be a circular border around the face. \n",
    "# Please detect if this border exists. \n",
    "# It's confirmed that the color of the border matches the color of the letter or pattern.\n",
    "# The expected output is a JSON in the following format:\n",
    "# {\n",
    "#     \"check\": true/false, \n",
    "#     \"c\": char/\"pattern\"/\"blank\", \n",
    "#     \"color\": \"green\"/\"yellow\"/\"red\"/\"blue\"/\"None\", \n",
    "#     \"rotation\": 0/90/180/270, \n",
    "#     \"circle\": true/false\n",
    "# }\n",
    "# \"\"\"\n",
    "\n",
    "# api_key=\"\"\n",
    "# client = OpenAI(api_key=api_key)\n",
    "\n",
    "# def encode_image(image, quality=100):\n",
    "#     if image.mode != 'RGB':\n",
    "#         image = image.convert('RGB')  # Convert to RGB\n",
    "#     buffered = BytesIO()\n",
    "#     image.save(buffered, format=\"JPEG\", quality=quality) \n",
    "#     return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "# def gpt4o_analysis(image_path, quality=50):\n",
    "#     with Image.open(image_path) as img:\n",
    "#         img_b64_str = encode_image(img, quality=quality)\n",
    "#     img_type = \"image/jpeg\"\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=\"gpt-4o-mini\",\n",
    "#         messages=[\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": [\n",
    "#                     {\"type\": \"text\", \"text\": SYSTEM_PROMPT},\n",
    "#                     {\n",
    "#                         \"type\": \"image_url\",\n",
    "#                         \"image_url\": {\"url\": f\"data:{img_type};base64,{img_b64_str}\"},\n",
    "#                     },\n",
    "#                 ],\n",
    "#             }\n",
    "#         ],\n",
    "#     )\n",
    "#     return response.choices[0].message.content\n",
    "#         ],\n",
    "#     )\n",
    "#     return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "348b83d4-ff09-4bbd-a7f0-b5a606c423ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Fitness = 0.8324, Remaining points: 17421\n",
      "Processing cube #1\n",
      "Z-axis alignment quality: 0.999979 (closer to 1 is better)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_876/3785830200.py:197: RuntimeWarning: invalid value encountered in divide\n",
      "  cube_top_image = (cube_top_image / cube_top_image.max() * 255).astype(np.uint8)\n",
      "/tmp/ipykernel_876/3785830200.py:197: RuntimeWarning: invalid value encountered in cast\n",
      "  cube_top_image = (cube_top_image / cube_top_image.max() * 255).astype(np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard grasp orientation\n",
      "Cube #1 grasp position: [0.6851293178540133, -0.3406099302639588, 0.024498871659610524]\n",
      "Approach position: [0.6851293178540133, -0.3406099302639588, 0.12449887165961053]\n",
      "RPY orientation: [-0.003112851786532983, -0.005643497784581841, 1.0081124094416802]\n",
      "Iteration 1: Fitness = 0.8362, Remaining points: 14857\n",
      "Processing cube #2\n",
      "Z-axis alignment quality: 0.999691 (closer to 1 is better)\n",
      "Using standard grasp orientation\n",
      "Cube #2 grasp position: [0.4427146443847968, -0.26896561372133, 0.02437337127848509]\n",
      "Approach position: [0.4427146443847968, -0.26896561372133, 0.12437337127848509]\n",
      "RPY orientation: [0.023215237541848066, 0.00886739433363437, 2.4118933790490904]\n",
      "Iteration 1: Fitness = 0.8369, Remaining points: 13378\n",
      "Processing cube #3\n",
      "Z-axis alignment quality: 0.999950 (closer to 1 is better)\n",
      "Using standard grasp orientation\n",
      "Cube #3 grasp position: [0.6438079738591959, -0.09478051749292818, 0.024590067037501304]\n",
      "Approach position: [0.6438079738591959, -0.09478051749292818, 0.1245900670375013]\n",
      "RPY orientation: [0.008017426027565243, 0.005995410621520536, 1.8574149655453356]\n",
      "\u001b[1;33m[Open3D WARNING] Too few correspondences (5) after mutual filter, fall back to original correspondences.\u001b[0;m\n",
      "Iteration 1: Fitness = 0.8324, Remaining points: 11532\n",
      "Processing cube #4\n",
      "Z-axis alignment quality: 0.999116 (closer to 1 is better)\n",
      "Both grasp orientations have collisions, skipping this cube\n",
      "\u001b[1;33m[Open3D WARNING] Too few correspondences (6) after mutual filter, fall back to original correspondences.\u001b[0;m\n",
      "Iteration 2: Fitness = 0.7682, Remaining points: 10154\n",
      "Processing cube #4\n",
      "Z-axis alignment quality: 0.814326 (closer to 1 is better)\n",
      "Both grasp orientations have collisions, skipping this cube\n",
      "\u001b[1;33m[Open3D WARNING] Too few correspondences (5) after mutual filter, fall back to original correspondences.\u001b[0;m\n",
      "Iteration 3: Fitness = 0.8227, Remaining points: 7840\n",
      "Processing cube #4\n",
      "Z-axis alignment quality: 0.999997 (closer to 1 is better)\n",
      "Using standard grasp orientation\n",
      "Cube #4 grasp position: [0.46463803178202234, 0.012769148434300411, 0.02446576801921222]\n",
      "Approach position: [0.46463803178202234, 0.012769148434300411, 0.12446576801921222]\n",
      "RPY orientation: [-0.0013483129962332439, 0.0018840826918431475, -1.2422828175640135]\n",
      "Found 4 cubes from 0 iterations\n",
      "Cube #1 Transform:\n",
      "[[ 0.53344975 -0.84581293 -0.00564347  0.68512932]\n",
      " [ 0.84581293  0.53347052 -0.0031128  -0.34060993]\n",
      " [ 0.00564347 -0.0031128   0.99997923  0.02449887]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Cube #2 Transform:\n",
      "[[-0.74534561 -0.6666193   0.00886728  0.44271464]\n",
      " [ 0.6666193  -0.74503684  0.02321224 -0.26896561]\n",
      " [-0.00886728  0.02321224  0.99969123  0.02437337]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Cube #3 Transform:\n",
      "[[-0.28270535 -0.95918806  0.00599537  0.64380797]\n",
      " [ 0.95918806 -0.28265524  0.0080172  -0.09478052]\n",
      " [-0.00599537  0.0080172   0.99994989  0.02459007]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Cube #4 Transform:\n",
      "[[ 0.32263582  0.94652131  0.00188408  0.46463803]\n",
      " [-0.94652131  0.3226385  -0.00134831  0.01276915]\n",
      " [-0.00188408 -0.00134831  0.99999732  0.02446577]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PointCloud2Image import enlarge_points_as_cubes, max_downsample_image, pointcloud_to_top_view_image_color, interpolate_sparse_image, pointcloud_to_colored_image_with_filling, triangle_mesh_to_image\n",
    "\n",
    "try:\n",
    "    broadcaster\n",
    "except NameError:\n",
    "    broadcaster = TransformBroadcaster()           \n",
    "    \n",
    "cube_num = 4\n",
    "# def pointcloud_process(point_cloud, slice_tolerance=0.005):\n",
    "#     '''\n",
    "#         Identify the cubes\n",
    "#         Returns:\n",
    "#             [\n",
    "#                 json,\n",
    "#                 T\n",
    "#             ]\n",
    "#     '''\n",
    "#     orignal_point_cloud = copy.deepcopy(point_cloud)\n",
    "#     T = []\n",
    "#     remaining_pointcloud_count = 10000\n",
    "#     countdown = 50\n",
    "#     # Use open3d to visualize the point cloud\n",
    "#     # o3d.visualization.draw_geometries([layer_point_cloud], window_name=f\"Layer {layer} (Z range: {z_min:.4f} to {z_max:.4f})\")\n",
    "#     movecount = 0\n",
    "#     while remaining_pointcloud_count > 50 and countdown > 0:\n",
    "#         cube_point_cloud = cube_mesh.sample_points_uniformly(number_of_points=50000) \n",
    "#         cube_point_cloud = filter_point_cloud_by_depth(cube_point_cloud, depth_threshold=-0.01)\n",
    "#         transform, remaining_pointcloud, deleted_pointcloud, fitness = register_and_filter(point_cloud, cube_point_cloud)\n",
    "#         remaining_pointcloud_count = len(remaining_pointcloud.points)\n",
    "#         # return transform, remaining_pointcloud, deleted_pointcloud\n",
    "#         if fitness > 0.01:\n",
    "#             countdown = countdown - 1\n",
    "#         print(fitness)\n",
    "#         if check_transform_z_axis_alignment(transform) and fitness > 0.50 and np.array_equal(transform, align_transform_z_axis(transform)):\n",
    "#         # if fitness > 0.70:\n",
    "#             if movecount >= cube_num-1:\n",
    "#                 break\n",
    "#             print(movecount)\n",
    "            \n",
    "\n",
    "#             broadcaster.update(transform)\n",
    "#             cube_coordinate_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(\n",
    "#                 size=0.1,  # Coordinate axis size, can be adjusted as needed\n",
    "#                 origin=[0, 0, 0]  # Origin of the coordinate axis\n",
    "#             )\n",
    "#             theta = np.radians(45)  # Convert angle to radians\n",
    "#             transform_matrix_x_180 = np.array([\n",
    "#                 [1, 0, 0, 0],\n",
    "#                 [0, -1, 0, 0],\n",
    "#                 [0, 0, -1, 0],\n",
    "#                 [0, 0, 0, 1]\n",
    "#             ])\n",
    "#             transform_matrix_z_90 = np.array([\n",
    "#                 [0, -1, 0, 0],\n",
    "#                 [1, 0, 0, 0],\n",
    "#                 [0, 0, 1, 0],\n",
    "#                 [0, 0, 0, 1]\n",
    "#             ])\n",
    "#             graps_transform = transform @ transform_matrix_x_180\n",
    "#             graps_transform_rotate = transform @ transform_matrix_x_180 @ transform_matrix_z_90\n",
    "#                         # o3d.visualization.draw_geometries([coordinate_frame, remaining_pointcloud], window_name=\"remaining_pointcloud\")\n",
    "#             cube_point_cloud_transormed = copy.deepcopy(deleted_pointcloud)\n",
    "#             cube_point_cloud_transormed = cube_point_cloud_transormed.transform(np.linalg.inv(graps_transform))\n",
    "#             cube_point_cloud_transormed_cubes = enlarge_points_as_cubes(cube_point_cloud_transormed)\n",
    "\n",
    "#             cube_top_image = triangle_mesh_to_image(cube_point_cloud_transormed_cubes, image_size=(100, 100))\n",
    "#             cube_top_image = (cube_top_image / cube_top_image.max() * 255).astype(np.uint8)\n",
    "#             grasp_coordinate_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(\n",
    "#                 size=0.1,  # Coordinate axis size, can be adjusted as needed\n",
    "#                 origin=[0, 0, 0]  # Origin of the coordinate axis\n",
    "#             )\n",
    "#             grasp_mesh, gripper_meshes_rotate = generate_gripper_from_transform(graps_transform)\n",
    "#             # Apply transformation matrix to the coordinate frame\n",
    "#             grasp_coordinate_frame.transform(graps_transform)\n",
    "#             grasp_final_matrix = None\n",
    "#             # o3d.visualization.draw_geometries(grasp_mesh+gripper_meshes_rotate+[orignal_point_cloud, coordinate_frame], window_name=\"remaining_pointcloud\")\n",
    "#             if check_grasp_collision(grasp_mesh, orignal_point_cloud):\n",
    "#             # if check_grasp_collision(grasp_mesh, deleted_pointcloud):\n",
    "#                 # If collision\n",
    "#                 grasp_mesh = []\n",
    "#             else:\n",
    "#                 grasp_final_matrix = graps_transform\n",
    "#             if check_grasp_collision(gripper_meshes_rotate, orignal_point_cloud):\n",
    "#             # if check_grasp_collision(gripper_meshes_rotate, deleted_pointcloud):\n",
    "#                 gripper_meshes_rotate = []\n",
    "#             else:\n",
    "#                 grasp_final_matrix = graps_transform_rotate\n",
    "#             # o3d.visualization.draw_geometries(grasp_mesh+gripper_meshes_rotate+[grasp_coordinate_frame, deleted_pointcloud, coordinate_frame], window_name=\"deleted_pointcloud\")\n",
    "#             # o3d.visualization.draw_geometries(grasp_mesh+gripper_meshes_rotate+[grasp_coordinate_frame, remaining_pointcloud, coordinate_frame], window_name=\"deleted_pointcloud\")\n",
    "#             if grasp_final_matrix is not None: # Can move\n",
    "#                 countdown = 50\n",
    "#                 print(grasp_final_matrix)\n",
    "#                 movecount += 1\n",
    "#                 pick_rpy, pick_pos = matrix_to_rpy_and_translation(grasp_final_matrix)\n",
    "#                 pick_pos_ = [a + b for a, b in zip(pick_pos, [0, 0, 0.10])]\n",
    "#                 pick_rpy = [a + b for a, b in zip(pick_rpy, [0, 0, 0])]\n",
    "#                 # pick_place.move(pick_pos_, pick_rpy)\n",
    "#                 print(pick_pos_, pick_rpy)\n",
    "#                 # cube_top_image = point_cloud_to_image(cube_point_cloud_transormed)\n",
    "#                 plt.imsave(f\"test_{movecount}.png\", cube_top_image)\n",
    "#                 plt.show()\n",
    "#                 point_cloud = remaining_pointcloud\n",
    "#                 # TODO: Identify the first face\n",
    "            \n",
    "#                 T.append(grasp_final_matrix)\n",
    "#         else:\n",
    "#             cube_point_cloud = cube_mesh.sample_points_uniformly(number_of_points=50000)\n",
    "#             cube_point_cloud = filter_point_cloud_by_depth(cube_point_cloud, depth_threshold=-0.01)\n",
    "#     print(T)\n",
    "#     return T\n",
    "\n",
    "def pointcloud_process(point_cloud, slice_tolerance=0.005, cube_num=4):\n",
    "    '''\n",
    "    Identify the cubes in a point cloud and determine their transformation matrices for grasping.\n",
    "    Handles alignment of Z-axis in any orientation for proper grasping.\n",
    "    \n",
    "    Args:\n",
    "        point_cloud: The input point cloud containing cubes to be identified\n",
    "        slice_tolerance: Tolerance for slicing the point cloud\n",
    "        cube_num: Maximum number of cubes to detect\n",
    "        \n",
    "    Returns:\n",
    "        A list of transformation matrices for grasping the identified cubes\n",
    "    '''\n",
    "    original_point_cloud = copy.deepcopy(point_cloud)\n",
    "    T = []\n",
    "    remaining_pointcloud = point_cloud\n",
    "    remaining_pointcloud_count = len(point_cloud.points)\n",
    "    countdown = 50\n",
    "    movecount = 0\n",
    "    \n",
    "    # Define standard transformation matrices for grasping\n",
    "    transform_matrix_x_180 = np.array([\n",
    "        [1, 0, 0, 0],\n",
    "        [0, -1, 0, 0],\n",
    "        [0, 0, -1, 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "    transform_matrix_z_90 = np.array([\n",
    "        [0, -1, 0, 0],\n",
    "        [1, 0, 0, 0],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "    # Function to check if a cube's z-axis points approximately downward\n",
    "    def is_z_axis_pointing_down(transform):\n",
    "        z_axis = transform[:3, 2]  # Extract z-axis vector\n",
    "        return np.dot(z_axis, np.array([0, 0, -1])) > 0.8  # Check if close to pointing down\n",
    "    \n",
    "    while remaining_pointcloud_count > 50 and countdown > 0 and movecount < cube_num:\n",
    "        # Sample points from the cube mesh model\n",
    "        cube_point_cloud = cube_mesh.sample_points_uniformly(number_of_points=50000) \n",
    "        cube_point_cloud = filter_point_cloud_by_depth(cube_point_cloud, depth_threshold=-0.01)\n",
    "        \n",
    "        # Register the cube in the point cloud\n",
    "        transform, remaining_pointcloud, deleted_pointcloud, fitness = register_and_filter(\n",
    "            remaining_pointcloud, cube_point_cloud\n",
    "        )\n",
    "        \n",
    "        remaining_pointcloud_count = len(remaining_pointcloud.points)\n",
    "        \n",
    "        # Decrement countdown if the fitness is above a minimum threshold\n",
    "        if fitness > 0.01:\n",
    "            countdown -= 1\n",
    "            \n",
    "        print(f\"Iteration {50-countdown}: Fitness = {fitness:.4f}, Remaining points: {remaining_pointcloud_count}\")\n",
    "        \n",
    "        # Check if the fitness is above the acceptable threshold\n",
    "        if fitness > 0.40:  # Lowered threshold slightly for more detections\n",
    "            print(f\"Processing cube #{movecount+1}\")\n",
    "            \n",
    "            # Align the Z-axis to point downward regardless of original orientation\n",
    "            aligned_transform = align_transform_z_axis_any_orientation(transform)\n",
    "            \n",
    "            # Update the transform broadcaster for visualization\n",
    "            if 'broadcaster' in globals():\n",
    "                broadcaster.update(aligned_transform)\n",
    "            \n",
    "            # Create grasp transforms\n",
    "            grasp_transform = aligned_transform @ transform_matrix_x_180\n",
    "            grasp_transform_rotate = aligned_transform @ transform_matrix_x_180 @ transform_matrix_z_90\n",
    "            \n",
    "            # Transform the detected cube point cloud for visualization\n",
    "            cube_point_cloud_transformed = copy.deepcopy(deleted_pointcloud)\n",
    "            cube_point_cloud_transformed = cube_point_cloud_transformed.transform(np.linalg.inv(grasp_transform))\n",
    "            \n",
    "            # For generating a visual representation as a top-down view\n",
    "            try:\n",
    "                cube_point_cloud_transformed_cubes = enlarge_points_as_cubes(cube_point_cloud_transformed)\n",
    "                cube_top_image = triangle_mesh_to_image(cube_point_cloud_transformed_cubes, image_size=(100, 100))\n",
    "                cube_top_image = (cube_top_image / cube_top_image.max() * 255).astype(np.uint8)\n",
    "                plt.imsave(f\"cube_{movecount+1}_top_view.png\", cube_top_image)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not generate top view image: {e}\")\n",
    "            \n",
    "            # Generate gripper meshes for collision checking\n",
    "            grasp_mesh, gripper_meshes_rotate = generate_gripper_from_transform(grasp_transform)\n",
    "            \n",
    "            # Find a valid grasp without collisions\n",
    "            grasp_final_matrix = None\n",
    "            \n",
    "            # Check first grasping orientation\n",
    "            if not check_grasp_collision(grasp_mesh, original_point_cloud):\n",
    "                grasp_final_matrix = grasp_transform\n",
    "                print(\"Using standard grasp orientation\")\n",
    "            # Check alternative grasping orientation (rotated by 90° around Z)\n",
    "            elif not check_grasp_collision(gripper_meshes_rotate, original_point_cloud):\n",
    "                grasp_final_matrix = grasp_transform_rotate\n",
    "                print(\"Using 90° rotated grasp orientation\")\n",
    "            else:\n",
    "                print(\"Both grasp orientations have collisions, skipping this cube\")\n",
    "                \n",
    "            # If a valid grasp pose was found\n",
    "            if grasp_final_matrix is not None:\n",
    "                # Reset countdown\n",
    "                countdown = 50\n",
    "                \n",
    "                # Calculate RPY and position for robot movement\n",
    "                pick_rpy, pick_pos = matrix_to_rpy_and_translation(grasp_final_matrix)\n",
    "                approach_offset = [0, 0, 0.10]  # 10cm above the cube for approach\n",
    "                pick_pos_approach = [a + b for a, b in zip(pick_pos, approach_offset)]\n",
    "                \n",
    "                print(f\"Cube #{movecount+1} grasp position: {pick_pos}\")\n",
    "                print(f\"Approach position: {pick_pos_approach}\")\n",
    "                print(f\"RPY orientation: {pick_rpy}\")\n",
    "                \n",
    "                # Add the transform to our results\n",
    "                T.append(grasp_final_matrix)\n",
    "                movecount += 1\n",
    "                \n",
    "                # Optional visualization of the detected cube\n",
    "                # o3d.visualization.draw_geometries([deleted_pointcloud], window_name=f\"Detected Cube #{movecount}\")\n",
    "                \n",
    "                # Optional visualization of the grasp\n",
    "                # vis_geometries = grasp_mesh + [create_coordinate_frame(0.1)]\n",
    "                # o3d.visualization.draw_geometries(vis_geometries, window_name=f\"Grasp Visualization #{movecount}\")\n",
    "        else:\n",
    "            # If fitness is too low, don't change the countdown\n",
    "            pass\n",
    "    \n",
    "    print(f\"Found {len(T)} cubes from {50-countdown} iterations\")\n",
    "    \n",
    "    # Print the final transforms\n",
    "    for i, transform in enumerate(T):\n",
    "        print(f\"Cube #{i+1} Transform:\")\n",
    "        print(transform)\n",
    "        \n",
    "    return T\n",
    "            \n",
    "Ts = pointcloud_process(filtered_point_cloud)\n",
    "broadcaster.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a266f54-a76e-4263-8be7-958b208f8df2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ts.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7be0151b-983a-4f9f-b066-f8220e1f2f6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import copy\n",
    "\n",
    "def generate_pascal_triangle_transforms(t, T):\n",
    "    \"\"\"\n",
    "    Generate a list of transformation matrices for a Pascal triangle arrangement.\n",
    "    Each cube's center position is used as the translation part of the transform matrix.\n",
    "    \"\"\"\n",
    "    level = 1\n",
    "    total = 1\n",
    "    while total < t:\n",
    "        level += 1\n",
    "        total += level\n",
    "\n",
    "    transforms = []\n",
    "    cube_size = 0.045  # 4.5 cm\n",
    "    spacing_xy = cube_size * 1.25  # Increase spacing to 1.25 times the cube size\n",
    "    spacing_z = cube_size * 1.05   # Use the same spacing in the vertical direction\n",
    "\n",
    "    current_pos = 0\n",
    "    for row in range(level-1, -1, -1):\n",
    "        for col in range(row + 1):\n",
    "            if current_pos >= t:\n",
    "                break\n",
    "\n",
    "            center_x = 0\n",
    "            center_y = (col - row/2) * spacing_xy\n",
    "            center_z = (level - 1 - row) * spacing_z\n",
    "\n",
    "            # Create local transformation matrix\n",
    "            local_transform = np.eye(4)\n",
    "            local_transform[:3, 3] = [center_x, center_y, center_z]\n",
    "\n",
    "            # Combine local transformation with T transformation\n",
    "            transform = np.dot(T, local_transform)\n",
    "            transforms.append(transform)\n",
    "            current_pos += 1\n",
    "\n",
    "        if current_pos >= t:\n",
    "            break\n",
    "\n",
    "    return transforms\n",
    "\n",
    "def create_coordinate_frame(size=0.1, transform=None):\n",
    "    frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=size)\n",
    "    if transform is not None:\n",
    "        frame.transform(transform)\n",
    "    return frame\n",
    "\n",
    "def visualize_pascal_triangle(transforms, T):\n",
    "    # Create a cube centered at the origin\n",
    "    cube = o3d.geometry.TriangleMesh.create_box(\n",
    "        width=0.045,\n",
    "        height=0.045, \n",
    "        depth=0.045\n",
    "    )\n",
    "    # Move the cube to be centered at the origin\n",
    "    cube.translate([-0.045/2, -0.045/2, -0.045/2])\n",
    "    cube.compute_vertex_normals()\n",
    "\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window()\n",
    "\n",
    "    # Add world coordinate frame\n",
    "    world_frame = create_coordinate_frame(size=0.2)\n",
    "    vis.add_geometry(world_frame)\n",
    "\n",
    "    # Add T coordinate frame\n",
    "    t_frame = create_coordinate_frame(size=0.2, transform=T)\n",
    "    vis.add_geometry(t_frame)\n",
    "\n",
    "    # Add all cubes and their local coordinate frames\n",
    "    for transform in transforms:\n",
    "        # Add cube\n",
    "        cube_copy = copy.deepcopy(cube)\n",
    "        cube_copy.transform(transform)\n",
    "        vis.add_geometry(cube_copy)\n",
    "        \n",
    "        # Add local coordinate frame\n",
    "        local_frame = create_coordinate_frame(size=0.05, transform=transform)\n",
    "        vis.add_geometry(local_frame)\n",
    "\n",
    "    opt = vis.get_render_option()\n",
    "    opt.background_color = np.asarray([0.5, 0.5, 0.5])\n",
    "\n",
    "    ctr = vis.get_view_control()\n",
    "    ctr.set_zoom(0.2)  # Adjust zoom to fit larger spacing\n",
    "    ctr.set_front([-0.8, -0.5, 0.5])\n",
    "    ctr.set_lookat([0, 0, 0])\n",
    "    ctr.set_up([0, 0, 1])\n",
    "\n",
    "    vis.run()\n",
    "    vis.destroy_window()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f396100-4bfb-4740-a839-6a5cef0fbb07",
   "metadata": {},
   "source": [
    "# 3. Motion Planning, movement, and grasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0ff91c9-6108-48f9-8132-1d623b646c89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import generate_pascal_triangle_transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67d90518-6b07-4d34-9b10-13a9a97c7c26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# t = Ts.__len__()\n",
    "t = 2\n",
    "# 创建T矩阵（示例：绕Z轴旋转45度并平移）\n",
    "T = np.eye(4)\n",
    "theta = 0\n",
    "T[:3, :3] = np.array([\n",
    "    [np.cos(theta), -np.sin(theta), 0],\n",
    "    [np.sin(theta), np.cos(theta), 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "T[:3, 3] = [0.3, 0.00, 0]\n",
    "\n",
    "aim_transforms = generate_pascal_triangle_transforms(t, T)\n",
    "# visualize_pascal_triangle(aim_transforms, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed7bf932-3c72-44a4-90b3-559ba899174c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.      ,  0.      ,  0.      ,  0.3     ],\n",
       "        [ 0.      ,  1.      ,  0.      , -0.028125],\n",
       "        [ 0.      ,  0.      ,  1.      ,  0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      ,  1.      ]]),\n",
       " array([[1.      , 0.      , 0.      , 0.3     ],\n",
       "        [0.      , 1.      , 0.      , 0.028125],\n",
       "        [0.      , 0.      , 1.      , 0.      ],\n",
       "        [0.      , 0.      , 0.      , 1.      ]])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aim_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcf7d104-3c62-454c-8cdc-25427abc1061",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.53344975, -0.84581293, -0.00564347,  0.68512932],\n",
       "        [ 0.84581293,  0.53347052, -0.0031128 , -0.34060993],\n",
       "        [ 0.00564347, -0.0031128 ,  0.99997923,  0.02449887],\n",
       "        [ 0.        ,  0.        ,  0.        ,  1.        ]]),\n",
       " array([[-0.74534561, -0.6666193 ,  0.00886728,  0.44271464],\n",
       "        [ 0.6666193 , -0.74503684,  0.02321224, -0.26896561],\n",
       "        [-0.00886728,  0.02321224,  0.99969123,  0.02437337],\n",
       "        [ 0.        ,  0.        ,  0.        ,  1.        ]]),\n",
       " array([[-0.28270535, -0.95918806,  0.00599537,  0.64380797],\n",
       "        [ 0.95918806, -0.28265524,  0.0080172 , -0.09478052],\n",
       "        [-0.00599537,  0.0080172 ,  0.99994989,  0.02459007],\n",
       "        [ 0.        ,  0.        ,  0.        ,  1.        ]]),\n",
       " array([[ 0.32263582,  0.94652131,  0.00188408,  0.46463803],\n",
       "        [-0.94652131,  0.3226385 , -0.00134831,  0.01276915],\n",
       "        [-0.00188408, -0.00134831,  0.99999732,  0.02446577],\n",
       "        [ 0.        ,  0.        ,  0.        ,  1.        ]])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c562e6c-a8e0-463e-8ccf-a661995771dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# debug:\n",
    "import numpy as np\n",
    "\n",
    "Ts = [np.array([[-0.30936624, -0.95090032,  0.00900657,  0.46123814],\n",
    "        [-0.95038183,  0.30949514,  0.03141855, -0.07133785],\n",
    "        [-0.0326634 ,  0.00116016, -0.99946574,  0.03173297],\n",
    "        [ 0.        ,  0.        ,  0.        ,  1.        ]]),\n",
    " np.array([[ 0.9979552 , -0.0391174 , -0.05054952,  0.61442553],\n",
    "        [-0.04106496, -0.99843052, -0.03808115,  0.10012986],\n",
    "        [-0.04898055,  0.0400791 , -0.99799528,  0.01518867],\n",
    "        [ 0.        ,  0.        ,  0.        ,  1.        ]]),\n",
    " # np.array([[ 0.97671909, -0.17776618,  0.12007919,  0.4728227 ],\n",
    " #        [-0.19286849, -0.97274425,  0.12872592,  0.08761278],\n",
    " #        [ 0.09392323, -0.14888855, -0.98438337,  0.02699751],\n",
    " #        [ 0.        ,  0.        ,  0.        ,  1.        ]])\n",
    "     ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8276ec04-7fa0-42e1-ae1c-32d03f30fbd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] [1742507762.167548, 2160.685000]: Error in move operation: too many values to unpack (expected 2)\n",
      "\u001b[33m[ WARN] [1742507762.166469364, 2160.685000000]: Fail: ABORTED: TIMED_OUT\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully moved to approach position for cube #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] [1742507787.374537, 2184.408000]: Error in move operation: too many values to unpack (expected 2)\n",
      "\u001b[33m[ WARN] [1742507787.368794128, 2184.419000000]: Fail: ABORTED: TIMED_OUT\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully moved to approach position for cube #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[ WARN] [1742507812.589722792, 2208.202000000]: Fail: ABORTED: TIMED_OUT\u001b[0m\n",
      "[ERROR] [1742507812.589968, 2208.201000]: Error in move operation: too many values to unpack (expected 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully moved to approach position for cube #3\n",
      "Successfully moved to approach position for cube #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] [1742507837.743736, 2231.754000]: Error in move operation: too many values to unpack (expected 2)\n",
      "\u001b[33m[ WARN] [1742507837.743324959, 2231.754000000]: Fail: ABORTED: TIMED_OUT\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[ WARN] [1742508641.206922173, 2994.935000000]: New joint state for joint 'panda_joint1' is not newer than the previous state. Assuming your rosbag looped.\u001b[0m\n",
      "\u001b[33m[ WARN] [1742508651.217950025, 3004.171000000]: New joint state for joint 'panda_joint1' is not newer than the previous state. Assuming your rosbag looped.\u001b[0m\n",
      "\u001b[33m[ WARN] [1742508652.009435708, 3004.906000000]: New joint state for joint 'panda_joint1' is not newer than the previous state. Assuming your rosbag looped.\u001b[0m\n",
      "\u001b[33m[ WARN] [1742508663.963966070, 3016.102000000]: New joint state for joint 'panda_joint1' is not newer than the previous state. Assuming your rosbag looped.\u001b[0m\n",
      "[WARN] [1742508684.304650, 0.013000]: Detected jump back in time of 3035.054000s. Clearing TF buffer.\n",
      "\u001b[33m[ WARN] [1742508684.303710864, 0.013000000]: New joint state for joint 'panda_joint1' is not newer than the previous state. Assuming your rosbag looped.\u001b[0m\n",
      "\u001b[33m[ WARN] [1742508684.338871997, 0.048000000]: Detected jump back in time of 3035.04s. Clearing TF buffer.\u001b[0m\n",
      "\u001b[33m[ WARN] [1742508694.487525370, 0.002000000]: New joint state for joint 'panda_joint1' is not newer than the previous state. Assuming your rosbag looped.\u001b[0m\n",
      "[WARN] [1742508694.516566, 0.021000]: Detected jump back in time of 9.503000s. Clearing TF buffer.\n",
      "\u001b[33m[ WARN] [1742508694.512372347, 0.036000000]: Detected jump back in time of 9.49s. Clearing TF buffer.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# demostrate the movement to the head of identified cubes\n",
    "# for T in Ts:\n",
    "#     pick_rpy, pick_pos = matrix_to_rpy_and_translation(T)\n",
    "#     pick_pos_ = [a + b for a, b in zip(pick_pos, [0.04, 0.00, 0.10])]\n",
    "#     pick_rpy = [a + b for a, b in zip(pick_rpy, [0, 0, 0])]\n",
    "#     print(pick_rpy, pick_pos)\n",
    "#     pick_place.move(pick_pos_, pick_rpy)\n",
    "# Modified movement demonstration code\n",
    "def demonstrate_movement_to_cubes(Ts, pick_place, offset=[0.04, 0.00, 0.10]):\n",
    "    for i, T in enumerate(Ts):\n",
    "        try:\n",
    "            pick_rpy, pick_pos = matrix_to_rpy_and_translation(T)\n",
    "            pick_pos_approach = [a + b for a, b in zip(pick_pos, offset)]\n",
    "            \n",
    "            try:\n",
    "                pick_place.move(pick_pos_approach, pick_rpy)\n",
    "                print(f\"Successfully moved to approach position for cube #{i+1}\")\n",
    "            except ValueError as e:\n",
    "                if \"too many values to unpack\" in str(e):\n",
    "                    print(f\"Note: Handled unpacking error in move operation for cube #{i+1}\")\n",
    "                else:\n",
    "                    raise\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing cube #{i+1}: {e}\")\n",
    "\n",
    "# Use the modified function\n",
    "demonstrate_movement_to_cubes(Ts, pick_place)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c1ff911-c41c-4330-be82-95130ec626fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] [1742507588.354013, 1996.771000]: Error in move operation: too many values to unpack (expected 2)\n",
      "[ERROR] [1742507588.419048, 1996.844000]: Error in move operation: too many values to unpack (expected 2)\n"
     ]
    }
   ],
   "source": [
    "# move the minipulator to the aim position\n",
    "for T in aim_transforms:\n",
    "    transform_matrix_x_180 = np.array([\n",
    "        [1, 0, 0, 0],\n",
    "        [0, -1, 0, 0],\n",
    "        [0, 0, -1, 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "    transform_matrix_z_90 = np.array([\n",
    "        [0, -1, 0, 0],\n",
    "        [1, 0, 0, 0],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "    pick_rpy, pick_pos = matrix_to_rpy_and_translation(T@transform_matrix_x_180@transform_matrix_z_90)\n",
    "    pick_pos_ = [a + b for a, b in zip(pick_pos, [0.0, 0, 0.13])]\n",
    "    pick_rpy = [a + b for a, b in zip(pick_rpy, [0, 0, 0])]\n",
    "    pick_place.move(pick_pos_, pick_rpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da6b4027-9a96-484e-b6b0-c423c42cd075",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [1740577596.943161]: Sending open goal: width: 0.08\n",
      "speed: 0.1\n",
      "[INFO] [1740577598.455938]: Gripper opened successfully.\n",
      "[INFO] [1740577603.855068]: Move successful to position: [0.50123814, -0.07133785, 0.23173297] and RPY: [3.1404318752863194, 0.03266921091879871, -1.8854965226642404]\n",
      "[INFO] [1740577603.855993]: Path constraints cleared.\n",
      "[INFO] [1740577607.561705]: Move successful to position: [0.50123814, -0.07133785, 0.03173297] and RPY: [3.1404318752863194, 0.03266921091879871, -1.8854965226642404]\n",
      "[INFO] [1740577607.562629]: Path constraints cleared.\n",
      "[INFO] [1740577607.863689]: Sending grasp goal: width: 0.04\n",
      "epsilon: \n",
      "  inner: 0.02\n",
      "  outer: 0.02\n",
      "speed: 0.1\n",
      "force: 1.0\n",
      "[INFO] [1740577611.612078]: Grasp successful.\n",
      "[INFO] [1740577611.913415]: Attempt 1 to plan Cartesian path...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] [1740577611.914229]: Exception in grasp_approach method: Unable to set path constraints, unknown constraint type <class 'float'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [1740577618.795077]: Move successful to position: [0.3, -0.028124999999999997, 0.23] and RPY: [3.141592653589793, 0.0, -1.5707963267948966]\n",
      "[INFO] [1740577618.796056]: Path constraints cleared.\n",
      "[INFO] [1740577619.296957]: Attempt 1 to plan Cartesian path...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] [1740577619.297934]: Exception in grasp_approach method: Unable to set path constraints, unknown constraint type <class 'float'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [1740577619.798996]: Sending open goal: width: 0.07\n",
      "speed: 0.1\n",
      "[INFO] [1740577622.075863]: Gripper opened successfully.\n",
      "[INFO] [1740577622.577023]: Attempt 1 to plan Cartesian path...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] [1740577622.578059]: Exception in grasp_approach method: Unable to set path constraints, unknown constraint type <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "# pick and place the first cube \n",
    "transform_matrix_x_180 = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, -1, 0, 0],\n",
    "    [0, 0, -1, 0],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "transform_matrix_z_90 = np.array([\n",
    "    [0, -1, 0, 0],\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "pick_rpy, pick_pos = matrix_to_rpy_and_translation(Ts[0])\n",
    "pick_pos_ = [a + b for a, b in zip(pick_pos, [0.04, 0, 0.00])]\n",
    "pick_rpy = [a + b for a, b in zip(pick_rpy, [0, 0, 0])]\n",
    "    \n",
    "place_rpy, place_pos = matrix_to_rpy_and_translation(aim_transforms[0]@transform_matrix_x_180@transform_matrix_z_90)\n",
    "place_pos_ = [a + b for a, b in zip(place_pos, [0.0, 0, 0.03])]\n",
    "\n",
    "pick_place.pick_and_place(\n",
    "    pick_pos=pick_pos_,\n",
    "    pick_rpy=pick_rpy,\n",
    "    place_pos=place_pos_,\n",
    "    place_rpy=place_rpy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a662392-36bb-46d1-a2b3-ac8562db28ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pick and place following cubes \n",
    "for i in range(4, Ts.__len__()-1):\n",
    "    ptransform_matrix_x_180 = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, -1, 0, 0],\n",
    "    [0, 0, -1, 0],\n",
    "    [0, 0, 0, 1]\n",
    "    ])\n",
    "    transform_matrix_z_90 = np.array([\n",
    "        [0, -1, 0, 0],\n",
    "        [1, 0, 0, 0],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    pick_rpy, pick_pos = matrix_to_rpy_and_translation(Ts[i])\n",
    "    pick_pos_ = [a + b for a, b in zip(pick_pos, [0.0, 0, 0.008])]\n",
    "    pick_rpy = [a + b for a, b in zip(pick_rpy, [0, 0, 0])]\n",
    "\n",
    "    place_rpy, place_pos = matrix_to_rpy_and_translation(aim_transforms[i]@transform_matrix_x_180@transform_matrix_z_90)\n",
    "    place_pos_ = [a + b for a, b in zip(place_pos, [0.0, 0, 0.03])]\n",
    "\n",
    "    pick_place.pick_and_place(\n",
    "        pick_pos=pick_pos_,\n",
    "        pick_rpy=pick_rpy,\n",
    "        place_pos=place_pos_,\n",
    "        place_rpy=place_rpy\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae12895-f189-48d9-98a8-7f5b4ce8f8dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28a61a9-5a2d-46d9-85f8-6e0d2d2e66e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb28b7de-2a7b-447b-b92c-f9487c4a8977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344eea87-4f39-40cd-adcb-2d3490950865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b49a45-b76a-4109-a1b9-92473eb0cc5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90752b1-45fd-40c6-95e6-5c641111580b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e628f0-7d4c-4612-8453-cc769bb96ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0b93ec-fc14-4c5b-a51b-ddd187495ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cf24a9-2107-4c18-8b75-410152602d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97c4cfa-2497-47fb-abcc-b4d8102d5a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97a75a1-7a1b-4df9-87e5-f22fc098e059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916923c9-2c36-417b-bb2a-7a07e665e320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1aa9b1-890b-4a21-a656-5c54a75d5c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e1942-89b9-460e-8013-c3406e5ec660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f97b18-cf6e-4683-b2b0-b18f61f0c335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84bdaf3-eb89-4acf-aa35-34d10d9ff810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72f5be6-10ab-4a45-9860-f3cdf9737da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc74575-a184-436e-b10f-6e5cbd701094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (irobmanenv)",
   "language": "python",
   "name": "irobmanenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
