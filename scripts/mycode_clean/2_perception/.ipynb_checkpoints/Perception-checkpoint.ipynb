{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95482eee-ad8f-423f-a497-8990e8ac372a",
   "metadata": {},
   "source": [
    "# 1. record point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb0fd4c1-97e3-4984-a270-edddc5a05230",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Python Path:\n",
      "\n",
      "/opt/ros_ws/devel/lib/python3/dist-packages\n",
      "/opt/ros/noetic/lib/python3/dist-packages\n",
      "/usr/lib/python38.zip\n",
      "/usr/lib/python3.8\n",
      "/usr/lib/python3.8/lib-dynload\n",
      "/usr/local/lib/python3.8/dist-packages\n",
      "/usr/lib/python3/dist-packages\n",
      "/opt/ros_ws/src/franka_zed_gazebo/scripts/mycode_clean/3_move\n",
      "/opt/ros_ws/src/franka_zed_gazebo/scripts/mycode_clean/1_getPointCloud\n",
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2025-02-04 19:18:10,455 - topics - topicmanager initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [1738696690.742824, 0.000000]: Waiting for move_group action server...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[ WARN] [1738696690.776106802]: Link zed2_holder has visual geometry but no collision geometry. Collision geometry will be left empty. Fix your URDF file by explicitly specifying collision geometry.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [1738696692.075963, 0.000000]: MoveRobot initialized successfully.\n",
      "[INFO] [1738696692.108600, 0.000000]: Waiting for gripper action servers...\n",
      "[INFO] [1738696693.020319, 250.463000]: Gripper action servers ready.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory (used to replace __file__)\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "target_path1 = os.path.abspath(os.path.join(current_dir, '../3_move'))\n",
    "target_path2 = os.path.abspath(os.path.join(current_dir, '../1_getPointCloud'))\n",
    "\n",
    "# Add the paths to sys.path\n",
    "if target_path1 not in sys.path:\n",
    "    sys.path.append(target_path1)\n",
    "    \n",
    "if target_path2 not in sys.path:\n",
    "    sys.path.append(target_path2)\n",
    "    \n",
    "# Check if the paths were added successfully\n",
    "print(\"Current Python Path:\")\n",
    "print(\"\\n\".join(sys.path))\n",
    "\n",
    "from ImageRecognizer import ImageRecognizer\n",
    "image_recognizer = ImageRecognizer(top_dir=\"/opt/ros_ws/src/franka_zed_gazebo/scripts/mycode_clean/2_perception/cubes/\")\n",
    "\n",
    "from utils import matrix_to_rpy_and_translation\n",
    "\n",
    "from PickAndPlace import PickAndPlace\n",
    "pick_place = PickAndPlace(approach_distance=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c575264-32a4-4fc0-9469-5ce80c9a98fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [1738696693.189771, 250.463000]: Waiting for data...\n",
      "[INFO] [1738696694.161727, 251.549000]: Received image message.\n",
      "[INFO] [1738696694.328224, 251.681000]: Received point cloud data.\n",
      "[INFO] [1738696696.226164, 253.465000]: Color range - Min: [0.10196078 0.10196078 0.10196078], Max: [0.70588235 0.60784314 0.60784314]\n",
      "[INFO] [1738696696.227612, 253.465000]: Requesting transform from world to left_camera_link_optical...\n",
      "[INFO] [1738696696.228926, 253.465000]: Transform found: header: \n",
      "  seq: 0\n",
      "  stamp: \n",
      "    secs: 253\n",
      "    nsecs: 631000000\n",
      "  frame_id: \"world\"\n",
      "child_frame_id: \"left_camera_link_optical\"\n",
      "transform: \n",
      "  translation: \n",
      "    x: 0.2097592918628224\n",
      "    y: -0.059977553326671075\n",
      "    z: 0.5616404884305867\n",
      "  rotation: \n",
      "    x: 0.6588366455241963\n",
      "    y: 0.6589138459481626\n",
      "    z: 0.25668657655283\n",
      "    w: 0.256668696084687\n",
      "[INFO] [1738696696.569442, 253.780000]: Transformed point cloud saved to /opt/ros_ws/src/franka_zed_gazebo/scripts/mycode_clean/2_perception/mesh/zed_point_cloud_world3.ply\n"
     ]
    }
   ],
   "source": [
    "from save_point_cloud import PointCloudSaver\n",
    "import open3d as o3d\n",
    "import rospy\n",
    "point_cloud_saver = PointCloudSaver()\n",
    "\n",
    "# Wait for data to be ready\n",
    "rospy.loginfo(\"Waiting for data...\")\n",
    "rospy.sleep(1)  # Wait for topic data to be published\n",
    "\n",
    "# Save the point cloud\n",
    "world_file = \"/opt/ros_ws/src/franka_zed_gazebo/scripts/mycode_clean/2_perception/mesh/zed_point_cloud_world3.ply\"\n",
    "point_cloud_saver.save_point_clouds(world_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ebdb0eb-9e99-4745-88a8-52bfcc7225d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from utils import filter_point_cloud_by_depth_and_range, filter_point_cloud_by_depth\n",
    "\n",
    "zed_ply_path = \"mesh/zed_point_cloud_world3.ply\"\n",
    "\n",
    "coordinate_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(\n",
    "    size=0.1,  # Size of the coordinate axes, can be adjusted as needed\n",
    "    origin=[0, 0, 0]  # Origin of the coordinate axes\n",
    ")\n",
    "\n",
    "# Read the point cloud file\n",
    "point_cloud = o3d.io.read_point_cloud(zed_ply_path)\n",
    "if not point_cloud.has_points():\n",
    "    raise ValueError(f\"Failed to read point cloud from {zed_ply_path}\")\n",
    "filtered_point_cloud = filter_point_cloud_by_depth_and_range(point_cloud, depth_threshold=0.005, range=[0.001, -0.8, 1, 1.6])\n",
    "o3d.visualization.draw_geometries([filtered_point_cloud, coordinate_frame], window_name=\"Filtered Point Cloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9883e9d-4bce-412e-ad0a-224f0121b514",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from utils import calculate_max_layer\n",
    "\n",
    "# # Example usage\n",
    "# max_layer = calculate_max_layer(filtered_point_cloud, layer_height=0.04)\n",
    "# print(f\"MaxLayer: {max_layer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d5b48e-3edb-4683-a774-6ef27ecad23b",
   "metadata": {},
   "source": [
    "# 2. Perception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f7d2ab-d25d-4387-ac97-81de3ec741dd",
   "metadata": {},
   "source": [
    "## 2.1 Coarse and Fine registration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88fe104e-b675-4313-93a1-417cdc00c8cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "def register_and_filter(pointcloud, mesh, voxel_size=0.01):\n",
    "    # Convert mesh to point cloud\n",
    "    if isinstance(mesh, o3d.geometry.TriangleMesh):\n",
    "        mesh_pointcloud = mesh.sample_points_uniformly(number_of_points=1000)\n",
    "    elif isinstance(mesh, o3d.geometry.PointCloud):\n",
    "        mesh_pointcloud = copy.deepcopy(mesh)\n",
    "    \n",
    "    # Downsample the point cloud and compute features\n",
    "    def preprocess_point_cloud(pcd, voxel_size):\n",
    "        pcd_down = pcd.voxel_down_sample(voxel_size)\n",
    "        pcd_down.estimate_normals(\n",
    "            search_param=o3d.geometry.KDTreeSearchParamHybrid(\n",
    "                radius=voxel_size*2, \n",
    "                max_nn=30\n",
    "            )\n",
    "        )\n",
    "        pcd_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "            pcd_down,\n",
    "            search_param=o3d.geometry.KDTreeSearchParamHybrid(\n",
    "                radius=voxel_size*5, \n",
    "                max_nn=100\n",
    "            )\n",
    "        )\n",
    "        return pcd_down, pcd_fpfh\n",
    "\n",
    "    # Coarse registration\n",
    "    def execute_global_registration(source_down, target_down, source_fpfh, target_fpfh, voxel_size):\n",
    "        distance_threshold = voxel_size * 1.5\n",
    "        result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
    "            source_down, target_down, source_fpfh, target_fpfh, True,\n",
    "            distance_threshold,\n",
    "            o3d.pipelines.registration.TransformationEstimationPointToPoint(False),\n",
    "            4,\n",
    "            [\n",
    "                o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(0.9),\n",
    "                o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(distance_threshold)\n",
    "            ],\n",
    "            o3d.pipelines.registration.RANSACConvergenceCriteria(4000000, 500)\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    # Fine registration\n",
    "    def refine_registration(source, target, initial_transformation, voxel_size):\n",
    "        distance_threshold = voxel_size * 1  # Reduce the threshold for higher accuracy\n",
    "        result = o3d.pipelines.registration.registration_icp(\n",
    "            source, target, distance_threshold, initial_transformation,\n",
    "            o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "            o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=1000000)\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    # Execute point cloud preprocessing\n",
    "    source_down, source_fpfh = preprocess_point_cloud(mesh_pointcloud, voxel_size)\n",
    "    target_down, target_fpfh = preprocess_point_cloud(pointcloud, voxel_size)\n",
    "\n",
    "    # Execute registration\n",
    "    coarse_result = execute_global_registration(source_down, target_down, source_fpfh, target_fpfh, voxel_size)\n",
    "    refined_result = refine_registration(mesh_pointcloud, pointcloud, coarse_result.transformation, voxel_size)\n",
    "\n",
    "    # Transform the mesh point cloud\n",
    "    transform = refined_result.transformation\n",
    "    transformed_mesh_pointcloud = mesh_pointcloud.transform(transform)\n",
    "\n",
    "    # Create bounding box\n",
    "    oriented_bounding_box = transformed_mesh_pointcloud.get_oriented_bounding_box()\n",
    "    center = oriented_bounding_box.center\n",
    "    extent = oriented_bounding_box.extent\n",
    "    rotation_matrix = oriented_bounding_box.R\n",
    "\n",
    "    # Expand the bounding box\n",
    "    margin = voxel_size\n",
    "    expanded_extent = extent + 0.5 * margin\n",
    "    expanded_bounding_box = o3d.geometry.OrientedBoundingBox(\n",
    "        center=center,\n",
    "        extent=expanded_extent,\n",
    "        R=rotation_matrix\n",
    "    )\n",
    "\n",
    "    # Filter the point cloud\n",
    "    indices_inside_box = expanded_bounding_box.get_point_indices_within_bounding_box(pointcloud.points)\n",
    "    indices_outside_box = list(set(range(len(pointcloud.points))) - set(indices_inside_box))\n",
    "\n",
    "    # Separate the point cloud\n",
    "    remaining_pointcloud = pointcloud.select_by_index(indices_outside_box)\n",
    "    deleted_pointcloud = pointcloud.select_by_index(indices_inside_box)\n",
    "\n",
    "    return transform, remaining_pointcloud, deleted_pointcloud, refined_result.fitness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97050aea-fb60-4b73-b5ee-14d1b5891787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the cube\n",
    "# Define file paths\n",
    "cube_obj_path = \"mesh/cube_0.obj\"\n",
    "zed_ply_path = \"mesh/zed_point_cloud_world3.ply\"\n",
    "\n",
    "# Read the cube_0.obj mesh\n",
    "cube_mesh = o3d.io.read_triangle_mesh(cube_obj_path)\n",
    "cube_mesh.compute_vertex_normals()  # Compute normals for better visualization\n",
    "# cube_point_cloud = cube_mesh.sample_points_uniformly(number_of_points=50000)  # Convert to point cloud\n",
    "# cube_point_cloud = cube_mesh.sample_points_poisson_disk(number_of_points=1000)\n",
    "\n",
    "# Remove the lower part of the cube to prevent flipping along the z-axis\n",
    "# cube_point_cloud = filter_point_cloud_by_depth(cube_point_cloud, depth_threshold=-0.015)\n",
    "# o3d.visualization.draw_geometries([cube_point_cloud])\n",
    "\n",
    "\n",
    "coordinate_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(\n",
    "    size=0.1,  # Size of the coordinate axes, can be adjusted as needed\n",
    "    origin=[0, 0, 0]  # Origin of the coordinate axes\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54f3fe34-f397-4e76-8452-b24a6cf4e9f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_transform_z_axis_alignment(transform, tolerance=0.1):\n",
    "    \"\"\"\n",
    "    Check if the Z-axis of the transform is parallel to the Z-axis of the world coordinate system.\n",
    "    Allows a certain tolerance range to check if it is parallel or anti-parallel.\n",
    "    \n",
    "    Args:\n",
    "    - transform: 4x4 transformation matrix\n",
    "    - tolerance: Tolerance range for checking, default is 0.1\n",
    "    \n",
    "    Returns:\n",
    "    - True: If the Z-axis is parallel or anti-parallel\n",
    "    - False: If the Z-axis is not parallel\n",
    "    - The corrected transform\n",
    "    \"\"\"\n",
    "    z_axis = np.array([0, 0, 1])  # Z-axis of the world coordinate system\n",
    "    transform_z_axis = transform[:3, 2]  # Get the Z-axis of the transform (i.e., the third column of the rotation matrix)\n",
    "\n",
    "    # Compute the angle between the transform's Z-axis and the world coordinate system's Z-axis\n",
    "    dot_product = np.dot(transform_z_axis, z_axis)\n",
    "    # Compute the cosine of the angle, if close to 1 or -1, it means parallel or anti-parallel\n",
    "    if np.abs(dot_product) > (1 - tolerance):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def align_transform_z_axis(transform):\n",
    "    \"\"\"\n",
    "    If the Z-axis of the transform is anti-parallel to the Z-axis of the world coordinate system,\n",
    "    rotate by 180 degrees around the X-axis (np.pi) to flip the direction of the Z-axis.\n",
    "    \n",
    "    Args:\n",
    "    - transform: 4x4 transformation matrix\n",
    "    \n",
    "    Returns:\n",
    "    - The corrected transform matrix\n",
    "    \"\"\"\n",
    "    z_axis_world = np.array([0, 0, 1])  # Z-axis of the world coordinate system\n",
    "    transform_z_axis = transform[:3, 2]  # Get the Z-axis of the transform (the third column of the rotation matrix)\n",
    "\n",
    "    # Check if the Z-axis is anti-parallel to the world coordinate system's Z-axis\n",
    "    if np.dot(transform_z_axis, z_axis_world) < 0:  # Z-axis is anti-parallel\n",
    "        print(\"Correcting Z axis\")\n",
    "        # Create a rotation matrix to rotate 180 degrees around the X-axis\n",
    "        rotation_matrix = np.eye(4)\n",
    "        rotation_matrix[1, 1] = -1  # Rotate the matrix by 180 degrees around the X-axis\n",
    "        rotation_matrix[2, 2] = -1  # Rotate the matrix by 180 degrees around the X-axis\n",
    "        \n",
    "        # Perform matrix multiplication, applying the rotation matrix to the original transform\n",
    "        transform = np.dot(rotation_matrix, transform)\n",
    "\n",
    "    return transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76d4d048-43c7-4a8a-ba1b-9c8e9fadcfa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import rospy\n",
    "import tf\n",
    "import numpy as np\n",
    "from geometry_msgs.msg import TransformStamped\n",
    "\n",
    "class TransformBroadcaster:\n",
    "    def __init__(self):\n",
    "        # Try to initialize the ROS node, avoid initializing multiple times\n",
    "        try:\n",
    "            rospy.init_node('tf_broadcaster_node')\n",
    "        except rospy.exceptions.ROSException:\n",
    "            pass  # If the node is already initialized, do nothing\n",
    "\n",
    "        # Create a TransformBroadcaster instance\n",
    "        self.br = tf.TransformBroadcaster()\n",
    "\n",
    "        # Assume T is the given 4x4 transformation matrix\n",
    "        self.T = np.ones((4, 4))  # Set to a 4x4 matrix\n",
    "\n",
    "        # Set a timer to call the broadcast_transform function every 100 milliseconds\n",
    "        self.timer = rospy.Timer(rospy.Duration(0.1), self.broadcast_transform)\n",
    "\n",
    "        # Store the timestamp of the last sent transformation\n",
    "        self.last_sent_time = None\n",
    "\n",
    "    def broadcast_transform(self, event):\n",
    "        try:\n",
    "            # Extract the translation and rotation parts from the 4x4 matrix\n",
    "            translation = self.T[0:3, 3]  # Translation part (x, y, z)\n",
    "            rotation_matrix = self.T[0:3, 0:3]  # Rotation matrix part\n",
    "\n",
    "            # Create a complete 4x4 matrix, including rotation and homogeneous coordinates\n",
    "            full_matrix = np.eye(4)\n",
    "            full_matrix[0:3, 0:3] = rotation_matrix\n",
    "            full_matrix[0:3, 3] = translation\n",
    "\n",
    "            # Create a quaternion to represent the rotation\n",
    "            quaternion = tf.transformations.quaternion_from_matrix(full_matrix)\n",
    "\n",
    "            # Get the current timestamp\n",
    "            current_time = rospy.Time.now()\n",
    "\n",
    "            # Check if the last sent timestamp and the current timestamp are the same\n",
    "            if self.last_sent_time is None or current_time != self.last_sent_time:\n",
    "                # Publish the transformation\n",
    "                self.br.sendTransform(\n",
    "                    (translation[0], translation[1], translation[2]),  # Translation part\n",
    "                    (quaternion[0], quaternion[1], quaternion[2], quaternion[3]),  # Rotation part (quaternion)\n",
    "                    current_time,  # Use the current timestamp\n",
    "                    \"cube\",  # Child frame name\n",
    "                    \"world\"   # Parent frame name\n",
    "                )\n",
    "                # Update the last sent timestamp\n",
    "                self.last_sent_time = current_time\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def update(self, T):\n",
    "        self.T = T\n",
    "        \n",
    "    def stop(self):\n",
    "        # Stop the timer\n",
    "        self.timer.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4cef69-b1d4-40d5-92e0-ad3c6f59fc93",
   "metadata": {},
   "source": [
    "## 2.2 Grasp Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "155cbbb8-ad8b-4146-a453-4cdaf9afbad5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import *  # Assuming the create_grasp_mesh function is in utils.py\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "\n",
    "def generate_gripper_from_transform(T: np.ndarray):\n",
    "    \"\"\"\n",
    "    Generates a robotic gripper mesh from a given 4x4 transformation matrix,\n",
    "    with additional rotations around x and y axes.\n",
    "\n",
    "    Args:\n",
    "        T: 4x4 transformation matrix (numpy array).\n",
    "        \n",
    "    Returns:\n",
    "        gripper_meshes: List of meshes representing the gripper.\n",
    "    \"\"\"\n",
    "    # Extract the rotation matrix (3x3)\n",
    "    rotation_matrix = T[:3, :3]\n",
    "\n",
    "    # Extract the translation vector\n",
    "    translation = T[:3, 3]\n",
    "\n",
    "    # Set the gripper's center point position, usually the translation vector\n",
    "    center_point = translation\n",
    "\n",
    "    # Create a rotation matrix for -90 degrees around the x-axis\n",
    "    R_x = np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, np.cos(-np.pi/2), -np.sin(-np.pi/2)],\n",
    "        [0, np.sin(-np.pi/2), np.cos(-np.pi/2)]\n",
    "    ])\n",
    "\n",
    "    # Create a rotation matrix for 90 degrees around the y-axis\n",
    "    R_y = np.array([\n",
    "        [np.cos(np.pi/2), 0, np.sin(np.pi/2)],\n",
    "        [0, 1, 0],\n",
    "        [-np.sin(np.pi/2), 0, np.cos(np.pi/2)]\n",
    "    ])\n",
    "    \n",
    "    R_z = np.array([\n",
    "        [0, -1, 0],\n",
    "        [1, 0, 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    # Combine rotation matrices, first rotate around the x-axis, then around the y-axis\n",
    "    combined_rotation = R_z @ rotation_matrix @ R_x \n",
    "\n",
    "    # Call create_grasp_mesh function to generate the gripper\n",
    "    gripper_meshes = create_grasp_mesh(\n",
    "        center_point=center_point, \n",
    "        rotation_matrix=combined_rotation,\n",
    "        width=0.25\n",
    "    )\n",
    "    # Call create_grasp_mesh function to generate the gripper with a different rotation\n",
    "    gripper_meshes_rotate = create_grasp_mesh(\n",
    "        center_point=center_point, \n",
    "        rotation_matrix=rotation_matrix @ R_x,\n",
    "        width=0.25\n",
    "    )\n",
    "\n",
    "    return gripper_meshes, gripper_meshes_rotate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47315293-9f5e-4dd0-a1aa-9efab7885c1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test code: Pass in a 4x4 transformation matrix\n",
    "T = np.array([\n",
    "    [1, 0, 0, 0.1],  # Rotation matrix and translation\n",
    "    [0, 1, 0, 0.2],\n",
    "    [0, 0, 1, 0.3],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "# Call the function to generate the gripper\n",
    "gripper_meshes, _ = generate_gripper_from_transform(T)\n",
    "\n",
    "# Visualize the generated gripper\n",
    "o3d.visualization.draw_geometries(gripper_meshes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8067b9a2-8d15-4f9e-9842-e1d6815c3d34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_grasp_collision(\n",
    "    grasp_meshes: Sequence[o3d.geometry.TriangleMesh],\n",
    "    object_pcd: o3d.geometry.TriangleMesh,\n",
    "    num_colisions: int = 10,\n",
    "    tolerance: float = 0.00001\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Checks for collisions between a gripper grasp pose and target object\n",
    "    using point cloud sampling.\n",
    "\n",
    "    Args:\n",
    "        grasp_meshes: List of mesh geometries representing the gripper components\n",
    "        object_mesh: Triangle mesh of the target object\n",
    "        num_collisions: Threshold on how many points to check\n",
    "        tolerance: Distance threshold for considering a collision (in meters)\n",
    "\n",
    "    Returns:\n",
    "        bool: True if collision detected between gripper and object, False otherwise\n",
    "    \"\"\"\n",
    "    # Combine gripper meshes\n",
    "    combined_gripper = o3d.geometry.TriangleMesh()\n",
    "    for mesh in grasp_meshes:\n",
    "        combined_gripper += mesh  # Combine multiple gripper meshes\n",
    "\n",
    "    # Sample points from both meshes\n",
    "    num_points = 5000  # Sample 5000 points from both gripper and target object\n",
    "    #######################TODO#######################\n",
    "    # Uniformly sample point clouds from both the gripper and object meshes\n",
    "    gripper_pcd = combined_gripper.sample_points_uniformly(number_of_points=num_points)\n",
    "    gripper_points = np.asarray(gripper_pcd.points)  # Point coordinates of the gripper point cloud\n",
    "    object_points = np.asarray(object_pcd.points)  # Point coordinates of the target object point cloud\n",
    "    ##################################################\n",
    "    \n",
    "    # Build KDTree for object points\n",
    "    is_collision = False\n",
    "    #######################TODO#######################\n",
    "    collision_count = 0\n",
    "    # Build a KDTree for the target object point cloud\n",
    "    object_kdtree = o3d.geometry.KDTreeFlann(object_pcd)\n",
    "    for gripper_point in gripper_points:\n",
    "        # For each gripper point, find the nearest point in the target object point cloud\n",
    "        _, _, distances = object_kdtree.search_knn_vector_3d(gripper_point, 1)  # Find the nearest neighbor\n",
    "        \n",
    "        # If the distance to the nearest neighbor is less than the tolerance, consider it a collision\n",
    "        if distances[0] <= tolerance:\n",
    "            collision_count += 1\n",
    "            \n",
    "            # Exit early if enough collisions are detected\n",
    "            if collision_count >= num_colisions:\n",
    "                is_collision = True\n",
    "                break\n",
    "    #######################TODO#######################\n",
    "\n",
    "    return is_collision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed7a038-40aa-4a06-a50d-9abae0aba06f",
   "metadata": {},
   "source": [
    "## 2.3 identify image of cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9fcf074-462c-450a-921d-b9589f4b627f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import base64\n",
    "import json\n",
    "SYSTEM_PROMPT = \"\"\"Please act as an image recognition agent. \n",
    "You will be given a square face of a block, \n",
    "which is projected from a point cloud. \n",
    "Your task is to recognize the following:\n",
    "\n",
    "Determine if this is a block face.\n",
    "Each face contains only one letter, \n",
    "one pattern (just detect whether it's a pattern, no need to identify the exact pattern), \n",
    "or is blank (only wood texture). \n",
    "Please detect whether it is a letter, \n",
    "a pattern, or blank. \n",
    "Each of these may be rotated. \n",
    "Please analyze all possible rotations in a clockwise direction: 0°, 90°, 180°, and 270°.\n",
    "There might be a circular border around the face. \n",
    "Please detect if this border exists. \n",
    "It's confirmed that the color of the border matches the color of the letter or pattern.\n",
    "The expected output is a JSON in the following format:\n",
    "{\n",
    "    \"check\": true/false, \n",
    "    \"c\": char/\"pattern\"/\"blank\", \n",
    "    \"color\": \"green\"/\"yellow\"/\"red\"/\"blue\"/\"None\", \n",
    "    \"rotation\": 0/90/180/270, \n",
    "    \"circle\": true/false\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "api_key=\"\"\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def encode_image(image, quality=100):\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')  # Convert to RGB\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"JPEG\", quality=quality) \n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "def gpt4o_analysis(image_path, quality=50):\n",
    "    with Image.open(image_path) as img:\n",
    "        img_b64_str = encode_image(img, quality=quality)\n",
    "    img_type = \"image/jpeg\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": SYSTEM_PROMPT},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:{img_type};base64,{img_b64_str}\"},\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "348b83d4-ff09-4bbd-a7f0-b5a606c423ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[ WARN] [1738697339.256808958, 876.005000000]: TF_REPEATED_DATA ignoring data with redundant timestamp for frame panda_leftfinger (parent panda_hand) at time 876.005000 according to authority unknown_publisher\u001b[0m\n",
      "\u001b[33m[ WARN] [1738697339.256929387, 876.005000000]: TF_REPEATED_DATA ignoring data with redundant timestamp for frame panda_rightfinger (parent panda_hand) at time 876.005000 according to authority unknown_publisher\u001b[0m\n",
      "\u001b[33m[ WARN] [1738697339.257001531, 876.005000000]: TF_REPEATED_DATA ignoring data with redundant timestamp for frame panda_link1 (parent panda_link0) at time 876.005000 according to authority unknown_publisher\u001b[0m\n",
      "\u001b[33m[ WARN] [1738697339.257014599, 876.005000000]: TF_REPEATED_DATA ignoring data with redundant timestamp for frame panda_link2 (parent panda_link1) at time 876.005000 according to authority unknown_publisher\u001b[0m\n",
      "\u001b[33m[ WARN] [1738697339.257026635, 876.005000000]: TF_REPEATED_DATA ignoring data with redundant timestamp for frame panda_link3 (parent panda_link2) at time 876.005000 according to authority unknown_publisher\u001b[0m\n",
      "\u001b[33m[ WARN] [1738697339.257038242, 876.005000000]: TF_REPEATED_DATA ignoring data with redundant timestamp for frame panda_link4 (parent panda_link3) at time 876.005000 according to authority unknown_publisher\u001b[0m\n",
      "\u001b[33m[ WARN] [1738697339.257049609, 876.005000000]: TF_REPEATED_DATA ignoring data with redundant timestamp for frame panda_link5 (parent panda_link4) at time 876.005000 according to authority unknown_publisher\u001b[0m\n",
      "\u001b[33m[ WARN] [1738697339.257060973, 876.005000000]: TF_REPEATED_DATA ignoring data with redundant timestamp for frame panda_link6 (parent panda_link5) at time 876.005000 according to authority unknown_publisher\u001b[0m\n",
      "\u001b[33m[ WARN] [1738697339.257077098, 876.005000000]: TF_REPEATED_DATA ignoring data with redundant timestamp for frame panda_link7 (parent panda_link6) at time 876.005000 according to authority unknown_publisher\u001b[0m\n",
      "\u001b[33m[ WARN] [1738697339.257892436, 876.005000000]: TF_REPEATED_DATA ignoring data with redundant timestamp for frame panda_leftfinger (parent panda_hand) at time 876.005000 according to authority /robot_state_publisher\u001b[0m\n",
      "\u001b[33m[ WARN] [1738697339.257936599, 876.005000000]: TF_REPEATED_DATA ignoring data with redundant timestamp for frame panda_rightfinger (parent panda_hand) at time 876.005000 according to authority /robot_state_publisher\u001b[0m\n",
      "\u001b[33m[ WARN] [1738697339.257955557, 876.005000000]: TF_REPEATED_DATA ignoring data with redundant timestamp for frame panda_link1 (parent panda_link0) at time 876.005000 according to authority /robot_state_publisher\u001b[0m\n",
      "\u001b[33m[ WARN] [1738697339.257971248, 876.005000000]: TF_REPEATED_DATA ignoring data with redundant timestamp for frame panda_link2 (parent panda_link1) at time 876.005000 according to authority /robot_state_publisher\u001b[0m\n",
      "\u001b[33m[ WARN] [1738697339.257986931, 876.005000000]: TF_REPEATED_DATA ignoring data with redundant timestamp for frame panda_link3 (parent panda_link2) at time 876.005000 according to authority /robot_state_publisher\u001b[0m\n",
      "\u001b[33m[ WARN] [1738697339.258001826, 876.005000000]: TF_REPEATED_DATA ignoring data with redundant timestamp for frame panda_link4 (parent panda_link3) at time 876.005000 according to authority /robot_state_publisher\u001b[0m\n",
      "\u001b[33m[ WARN] [1738697339.258016203, 876.005000000]: TF_REPEATED_DATA ignoring data with redundant timestamp for frame panda_link5 (parent panda_link4) at time 876.005000 according to authority /robot_state_publisher\u001b[0m\n",
      "\u001b[33m[ WARN] [1738697339.258030478, 876.005000000]: TF_REPEATED_DATA ignoring data with redundant timestamp for frame panda_link6 (parent panda_link5) at time 876.005000 according to authority /robot_state_publisher\u001b[0m\n",
      "\u001b[33m[ WARN] [1738697339.258044536, 876.005000000]: TF_REPEATED_DATA ignoring data with redundant timestamp for frame panda_link7 (parent panda_link6) at time 876.005000 according to authority /robot_state_publisher\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[ 0.99178994  0.0543546   0.11575098  0.70006641]\n",
      " [ 0.06103386 -0.99662136 -0.0549612  -0.39853327]\n",
      " [ 0.1123725   0.0615747  -0.99175651  0.02090872]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "[0.7000664120432076, -0.39853326900983227, 0.12090872392137775] [3.0795857377428915, -0.11261035731720659, 0.061461593483156246]\n",
      "1\n",
      "[[-0.03912899 -0.99792591  0.05111558  0.69798912]\n",
      " [-0.99492167  0.03416163 -0.09467766 -0.20022832]\n",
      " [ 0.0927351  -0.05456064 -0.99419482  0.02091544]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "[0.6979891217428389, -0.20022831669240093, 0.12091543729222774] [-3.0867684291130955, -0.09286853287786756, -1.6101047823493848]\n",
      "2\n",
      "[[ 0.99345503  0.04550642  0.10476769  0.70006371]\n",
      " [ 0.05131262 -0.99725358 -0.05340709 -0.49847667]\n",
      " [ 0.10204959  0.05843345 -0.99306164  0.02085475]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "[0.7000637144928734, -0.49847666893273934, 0.12085474880227026] [3.082818711405606, -0.102227554265089, 0.05160481208685065]\n",
      "3\n",
      "[[-0.0514598  -0.99743961  0.04965991  0.69827517]\n",
      " [-0.99300288  0.04581196 -0.10884182 -0.3002676 ]\n",
      " [ 0.10628812 -0.05491341 -0.99281788  0.020863  ]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "[0.6982751706580705, -0.3002676031873368, 0.1208629965884051] [-3.0863382966105637, -0.10648927225487625, -1.6225724219611175]\n",
      "4\n",
      "[[ 3.64660191e-02 -9.98539525e-01  3.98628445e-02  6.97240508e-01]\n",
      " [-9.98197357e-01 -3.44927956e-02  4.91150131e-02  1.74385217e-04]\n",
      " [-4.76683010e-02 -4.15820150e-02 -9.97997329e-01  2.13021421e-02]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "[0.6972405082839007, 0.00017438521650547343, 0.12130214211670048] [-3.0999512819014168, 0.047686371957770035, -1.53428069231817]\n",
      "5\n",
      "[[-0.99071668 -0.0429346   0.12898482  0.69988768]\n",
      " [-0.04537424  0.99884137 -0.01603416 -0.09890263]\n",
      " [-0.12814695 -0.0217379  -0.99151693  0.02125301]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "[0.6998876828344169, -0.09890262537148707, 0.12125301012984324] [-3.119672285358245, 0.12850029899273374, -3.095825228188937]\n",
      "[array([[ 0.99178994,  0.0543546 ,  0.11575098,  0.70006641],\n",
      "       [ 0.06103386, -0.99662136, -0.0549612 , -0.39853327],\n",
      "       [ 0.1123725 ,  0.0615747 , -0.99175651,  0.02090872],\n",
      "       [ 0.        ,  0.        ,  0.        ,  1.        ]]), array([[-0.03912899, -0.99792591,  0.05111558,  0.69798912],\n",
      "       [-0.99492167,  0.03416163, -0.09467766, -0.20022832],\n",
      "       [ 0.0927351 , -0.05456064, -0.99419482,  0.02091544],\n",
      "       [ 0.        ,  0.        ,  0.        ,  1.        ]]), array([[ 0.99345503,  0.04550642,  0.10476769,  0.70006371],\n",
      "       [ 0.05131262, -0.99725358, -0.05340709, -0.49847667],\n",
      "       [ 0.10204959,  0.05843345, -0.99306164,  0.02085475],\n",
      "       [ 0.        ,  0.        ,  0.        ,  1.        ]]), array([[-0.0514598 , -0.99743961,  0.04965991,  0.69827517],\n",
      "       [-0.99300288,  0.04581196, -0.10884182, -0.3002676 ],\n",
      "       [ 0.10628812, -0.05491341, -0.99281788,  0.020863  ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  1.        ]]), array([[ 3.64660191e-02, -9.98539525e-01,  3.98628445e-02,\n",
      "         6.97240508e-01],\n",
      "       [-9.98197357e-01, -3.44927956e-02,  4.91150131e-02,\n",
      "         1.74385217e-04],\n",
      "       [-4.76683010e-02, -4.15820150e-02, -9.97997329e-01,\n",
      "         2.13021421e-02],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         1.00000000e+00]]), array([[-0.99071668, -0.0429346 ,  0.12898482,  0.69988768],\n",
      "       [-0.04537424,  0.99884137, -0.01603416, -0.09890263],\n",
      "       [-0.12814695, -0.0217379 , -0.99151693,  0.02125301],\n",
      "       [ 0.        ,  0.        ,  0.        ,  1.        ]])]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PointCloud2Image import enlarge_points_as_cubes, max_downsample_image, pointcloud_to_top_view_image_color, interpolate_sparse_image, pointcloud_to_colored_image_with_filling, triangle_mesh_to_image\n",
    "\n",
    "try:\n",
    "    broadcaster\n",
    "except NameError:\n",
    "    broadcaster = TransformBroadcaster()           \n",
    "    \n",
    "    \n",
    "def pointcloud_process(point_cloud, slice_tolerance=0.005):\n",
    "    '''\n",
    "        Identify the cubes\n",
    "        Returns:\n",
    "            [\n",
    "                json,\n",
    "                T\n",
    "            ]\n",
    "    '''\n",
    "    orignal_point_cloud = copy.deepcopy(point_cloud)\n",
    "    T = []\n",
    "    remaining_pointcloud_count = 10000\n",
    "    countdown = 50\n",
    "    # Use open3d to visualize the point cloud\n",
    "    # o3d.visualization.draw_geometries([layer_point_cloud], window_name=f\"Layer {layer} (Z range: {z_min:.4f} to {z_max:.4f})\")\n",
    "    movecount = 0\n",
    "    while remaining_pointcloud_count > 50 and countdown > 0:\n",
    "        cube_point_cloud = cube_mesh.sample_points_uniformly(number_of_points=50000) \n",
    "        cube_point_cloud = filter_point_cloud_by_depth(cube_point_cloud, depth_threshold=-0.01)\n",
    "        transform, remaining_pointcloud, deleted_pointcloud, fitness = register_and_filter(point_cloud, cube_point_cloud)\n",
    "        remaining_pointcloud_count = len(remaining_pointcloud.points)\n",
    "        # return transform, remaining_pointcloud, deleted_pointcloud\n",
    "        if fitness > 0.01:\n",
    "            countdown = countdown - 1\n",
    "        # print(fitness)\n",
    "        if check_transform_z_axis_alignment(transform) and fitness > 0.70 and np.array_equal(transform, align_transform_z_axis(transform)):\n",
    "        # if fitness > 0.70:\n",
    "            print(movecount)\n",
    "            \n",
    "\n",
    "            broadcaster.update(transform)\n",
    "            cube_coordinate_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(\n",
    "                size=0.1,  # Coordinate axis size, can be adjusted as needed\n",
    "                origin=[0, 0, 0]  # Origin of the coordinate axis\n",
    "            )\n",
    "            theta = np.radians(45)  # Convert angle to radians\n",
    "            transform_matrix_x_180 = np.array([\n",
    "                [1, 0, 0, 0],\n",
    "                [0, -1, 0, 0],\n",
    "                [0, 0, -1, 0],\n",
    "                [0, 0, 0, 1]\n",
    "            ])\n",
    "            transform_matrix_z_90 = np.array([\n",
    "                [0, -1, 0, 0],\n",
    "                [1, 0, 0, 0],\n",
    "                [0, 0, 1, 0],\n",
    "                [0, 0, 0, 1]\n",
    "            ])\n",
    "            graps_transform = transform @ transform_matrix_x_180\n",
    "            graps_transform_rotate = transform @ transform_matrix_x_180 @ transform_matrix_z_90\n",
    "                        # o3d.visualization.draw_geometries([coordinate_frame, remaining_pointcloud], window_name=\"remaining_pointcloud\")\n",
    "            cube_point_cloud_transormed = copy.deepcopy(deleted_pointcloud)\n",
    "            cube_point_cloud_transormed = cube_point_cloud_transormed.transform(np.linalg.inv(graps_transform))\n",
    "            cube_point_cloud_transormed_cubes = enlarge_points_as_cubes(cube_point_cloud_transormed)\n",
    "\n",
    "            cube_top_image = triangle_mesh_to_image(cube_point_cloud_transormed_cubes, image_size=(100, 100))\n",
    "            cube_top_image = (cube_top_image / cube_top_image.max() * 255).astype(np.uint8)\n",
    "            grasp_coordinate_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(\n",
    "                size=0.1,  # Coordinate axis size, can be adjusted as needed\n",
    "                origin=[0, 0, 0]  # Origin of the coordinate axis\n",
    "            )\n",
    "            grasp_mesh, gripper_meshes_rotate = generate_gripper_from_transform(graps_transform)\n",
    "            # Apply transformation matrix to the coordinate frame\n",
    "            grasp_coordinate_frame.transform(graps_transform)\n",
    "            grasp_final_matrix = None\n",
    "            # o3d.visualization.draw_geometries(grasp_mesh+gripper_meshes_rotate+[orignal_point_cloud, coordinate_frame], window_name=\"remaining_pointcloud\")\n",
    "            if check_grasp_collision(grasp_mesh, orignal_point_cloud):\n",
    "            # if check_grasp_collision(grasp_mesh, deleted_pointcloud):\n",
    "                # If collision\n",
    "                grasp_mesh = []\n",
    "            else:\n",
    "                grasp_final_matrix = graps_transform\n",
    "            if check_grasp_collision(gripper_meshes_rotate, orignal_point_cloud):\n",
    "            # if check_grasp_collision(gripper_meshes_rotate, deleted_pointcloud):\n",
    "                gripper_meshes_rotate = []\n",
    "            else:\n",
    "                grasp_final_matrix = graps_transform_rotate\n",
    "            # o3d.visualization.draw_geometries(grasp_mesh+gripper_meshes_rotate+[grasp_coordinate_frame, deleted_pointcloud, coordinate_frame], window_name=\"deleted_pointcloud\")\n",
    "            # o3d.visualization.draw_geometries(grasp_mesh+gripper_meshes_rotate+[grasp_coordinate_frame, remaining_pointcloud, coordinate_frame], window_name=\"deleted_pointcloud\")\n",
    "            if grasp_final_matrix is not None: # Can move\n",
    "                countdown = 50\n",
    "                print(grasp_final_matrix)\n",
    "                movecount += 1\n",
    "                pick_rpy, pick_pos = matrix_to_rpy_and_translation(grasp_final_matrix)\n",
    "                pick_pos_ = [a + b for a, b in zip(pick_pos, [0, 0, 0.10])]\n",
    "                pick_rpy = [a + b for a, b in zip(pick_rpy, [0, 0, 0])]\n",
    "                # pick_place.move(pick_pos_, pick_rpy)\n",
    "                print(pick_pos_, pick_rpy)\n",
    "                # cube_top_image = point_cloud_to_image(cube_point_cloud_transormed)\n",
    "                plt.imsave(f\"test_{movecount}.png\", cube_top_image)\n",
    "                plt.show()\n",
    "                point_cloud = remaining_pointcloud\n",
    "                # TODO: Identify the first face\n",
    "            \n",
    "                T.append(grasp_final_matrix)\n",
    "        else:\n",
    "            cube_point_cloud = cube_mesh.sample_points_uniformly(number_of_points=50000)\n",
    "            cube_point_cloud = filter_point_cloud_by_depth(cube_point_cloud, depth_threshold=-0.01)\n",
    "    print(T)\n",
    "    return T\n",
    "            \n",
    "Ts = pointcloud_process(filtered_point_cloud)\n",
    "broadcaster.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a266f54-a76e-4263-8be7-958b208f8df2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ts.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7be0151b-983a-4f9f-b066-f8220e1f2f6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import copy\n",
    "\n",
    "def generate_pascal_triangle_transforms(t, T):\n",
    "    \"\"\"\n",
    "    Generate a list of transformation matrices for a Pascal triangle arrangement.\n",
    "    Each cube's center position is used as the translation part of the transform matrix.\n",
    "    \"\"\"\n",
    "    level = 1\n",
    "    total = 1\n",
    "    while total < t:\n",
    "        level += 1\n",
    "        total += level\n",
    "\n",
    "    transforms = []\n",
    "    cube_size = 0.045  # 4.5 cm\n",
    "    spacing_xy = cube_size * 1.25  # Increase spacing to 1.25 times the cube size\n",
    "    spacing_z = cube_size * 1.05   # Use the same spacing in the vertical direction\n",
    "\n",
    "    current_pos = 0\n",
    "    for row in range(level-1, -1, -1):\n",
    "        for col in range(row + 1):\n",
    "            if current_pos >= t:\n",
    "                break\n",
    "\n",
    "            center_x = 0\n",
    "            center_y = (col - row/2) * spacing_xy\n",
    "            center_z = (level - 1 - row) * spacing_z\n",
    "\n",
    "            # Create local transformation matrix\n",
    "            local_transform = np.eye(4)\n",
    "            local_transform[:3, 3] = [center_x, center_y, center_z]\n",
    "\n",
    "            # Combine local transformation with T transformation\n",
    "            transform = np.dot(T, local_transform)\n",
    "            transforms.append(transform)\n",
    "            current_pos += 1\n",
    "\n",
    "        if current_pos >= t:\n",
    "            break\n",
    "\n",
    "    return transforms\n",
    "\n",
    "def create_coordinate_frame(size=0.1, transform=None):\n",
    "    frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=size)\n",
    "    if transform is not None:\n",
    "        frame.transform(transform)\n",
    "    return frame\n",
    "\n",
    "def visualize_pascal_triangle(transforms, T):\n",
    "    # Create a cube centered at the origin\n",
    "    cube = o3d.geometry.TriangleMesh.create_box(\n",
    "        width=0.045,\n",
    "        height=0.045, \n",
    "        depth=0.045\n",
    "    )\n",
    "    # Move the cube to be centered at the origin\n",
    "    cube.translate([-0.045/2, -0.045/2, -0.045/2])\n",
    "    cube.compute_vertex_normals()\n",
    "\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window()\n",
    "\n",
    "    # Add world coordinate frame\n",
    "    world_frame = create_coordinate_frame(size=0.2)\n",
    "    vis.add_geometry(world_frame)\n",
    "\n",
    "    # Add T coordinate frame\n",
    "    t_frame = create_coordinate_frame(size=0.2, transform=T)\n",
    "    vis.add_geometry(t_frame)\n",
    "\n",
    "    # Add all cubes and their local coordinate frames\n",
    "    for transform in transforms:\n",
    "        # Add cube\n",
    "        cube_copy = copy.deepcopy(cube)\n",
    "        cube_copy.transform(transform)\n",
    "        vis.add_geometry(cube_copy)\n",
    "        \n",
    "        # Add local coordinate frame\n",
    "        local_frame = create_coordinate_frame(size=0.05, transform=transform)\n",
    "        vis.add_geometry(local_frame)\n",
    "\n",
    "    opt = vis.get_render_option()\n",
    "    opt.background_color = np.asarray([0.5, 0.5, 0.5])\n",
    "\n",
    "    ctr = vis.get_view_control()\n",
    "    ctr.set_zoom(0.2)  # Adjust zoom to fit larger spacing\n",
    "    ctr.set_front([-0.8, -0.5, 0.5])\n",
    "    ctr.set_lookat([0, 0, 0])\n",
    "    ctr.set_up([0, 0, 1])\n",
    "\n",
    "    vis.run()\n",
    "    vis.destroy_window()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f396100-4bfb-4740-a839-6a5cef0fbb07",
   "metadata": {},
   "source": [
    "# 3. Motion Planning, movement, and grasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0ff91c9-6108-48f9-8132-1d623b646c89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import generate_pascal_triangle_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67d90518-6b07-4d34-9b10-13a9a97c7c26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = Ts.__len__()\n",
    "# 创建T矩阵（示例：绕Z轴旋转45度并平移）\n",
    "T = np.eye(4)\n",
    "theta = 0\n",
    "T[:3, :3] = np.array([\n",
    "    [np.cos(theta), -np.sin(theta), 0],\n",
    "    [np.sin(theta), np.cos(theta), 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "T[:3, 3] = [0.3, 0.00, 0]\n",
    "\n",
    "aim_transforms = generate_pascal_triangle_transforms(t, T)\n",
    "visualize_pascal_triangle(aim_transforms, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed7bf932-3c72-44a4-90b3-559ba899174c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.     ,  0.     ,  0.     ,  0.3    ],\n",
       "        [ 0.     ,  1.     ,  0.     , -0.05625],\n",
       "        [ 0.     ,  0.     ,  1.     ,  0.     ],\n",
       "        [ 0.     ,  0.     ,  0.     ,  1.     ]]),\n",
       " array([[1. , 0. , 0. , 0.3],\n",
       "        [0. , 1. , 0. , 0. ],\n",
       "        [0. , 0. , 1. , 0. ],\n",
       "        [0. , 0. , 0. , 1. ]]),\n",
       " array([[1.     , 0.     , 0.     , 0.3    ],\n",
       "        [0.     , 1.     , 0.     , 0.05625],\n",
       "        [0.     , 0.     , 1.     , 0.     ],\n",
       "        [0.     , 0.     , 0.     , 1.     ]]),\n",
       " array([[ 1.      ,  0.      ,  0.      ,  0.3     ],\n",
       "        [ 0.      ,  1.      ,  0.      , -0.028125],\n",
       "        [ 0.      ,  0.      ,  1.      ,  0.04725 ],\n",
       "        [ 0.      ,  0.      ,  0.      ,  1.      ]]),\n",
       " array([[1.      , 0.      , 0.      , 0.3     ],\n",
       "        [0.      , 1.      , 0.      , 0.028125],\n",
       "        [0.      , 0.      , 1.      , 0.04725 ],\n",
       "        [0.      , 0.      , 0.      , 1.      ]]),\n",
       " array([[1.    , 0.    , 0.    , 0.3   ],\n",
       "        [0.    , 1.    , 0.    , 0.    ],\n",
       "        [0.    , 0.    , 1.    , 0.0945],\n",
       "        [0.    , 0.    , 0.    , 1.    ]])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aim_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcf7d104-3c62-454c-8cdc-25427abc1061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.99178994,  0.0543546 ,  0.11575098,  0.70006641],\n",
       "        [ 0.06103386, -0.99662136, -0.0549612 , -0.39853327],\n",
       "        [ 0.1123725 ,  0.0615747 , -0.99175651,  0.02090872],\n",
       "        [ 0.        ,  0.        ,  0.        ,  1.        ]]),\n",
       " array([[-0.03912899, -0.99792591,  0.05111558,  0.69798912],\n",
       "        [-0.99492167,  0.03416163, -0.09467766, -0.20022832],\n",
       "        [ 0.0927351 , -0.05456064, -0.99419482,  0.02091544],\n",
       "        [ 0.        ,  0.        ,  0.        ,  1.        ]]),\n",
       " array([[ 0.99345503,  0.04550642,  0.10476769,  0.70006371],\n",
       "        [ 0.05131262, -0.99725358, -0.05340709, -0.49847667],\n",
       "        [ 0.10204959,  0.05843345, -0.99306164,  0.02085475],\n",
       "        [ 0.        ,  0.        ,  0.        ,  1.        ]]),\n",
       " array([[-0.0514598 , -0.99743961,  0.04965991,  0.69827517],\n",
       "        [-0.99300288,  0.04581196, -0.10884182, -0.3002676 ],\n",
       "        [ 0.10628812, -0.05491341, -0.99281788,  0.020863  ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  1.        ]]),\n",
       " array([[ 3.64660191e-02, -9.98539525e-01,  3.98628445e-02,\n",
       "          6.97240508e-01],\n",
       "        [-9.98197357e-01, -3.44927956e-02,  4.91150131e-02,\n",
       "          1.74385217e-04],\n",
       "        [-4.76683010e-02, -4.15820150e-02, -9.97997329e-01,\n",
       "          2.13021421e-02],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          1.00000000e+00]]),\n",
       " array([[-0.99071668, -0.0429346 ,  0.12898482,  0.69988768],\n",
       "        [-0.04537424,  0.99884137, -0.01603416, -0.09890263],\n",
       "        [-0.12814695, -0.0217379 , -0.99151693,  0.02125301],\n",
       "        [ 0.        ,  0.        ,  0.        ,  1.        ]])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8276ec04-7fa0-42e1-ae1c-32d03f30fbd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [1738697526.965434, 1050.539000]: Move successful to position: [0.7000664120432076, -0.39853326900983227, 0.12090872392137775] and RPY: [3.0795857377428915, -0.11261035731720659, 0.061461593483156246]\n",
      "[INFO] [1738697526.984128, 1050.539000]: Path constraints cleared.\n",
      "[INFO] [1738697535.359719, 1058.541000]: Move successful to position: [0.6979891217428389, -0.20022831669240093, 0.12091543729222774] and RPY: [-3.0867684291130955, -0.09286853287786756, -1.6101047823493848]\n",
      "[INFO] [1738697535.361636, 1058.677000]: Path constraints cleared.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[ WARN] [1738697540.387587835, 1063.491000000]: Fail: ABORTED: TIMED_OUT\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [1738697545.418143, 1068.308000]: Path constraints cleared.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m pick_pos_ \u001b[38;5;241m=\u001b[39m [a \u001b[38;5;241m+\u001b[39m b \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(pick_pos, [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.10\u001b[39m])]\n\u001b[1;32m      5\u001b[0m pick_rpy \u001b[38;5;241m=\u001b[39m [a \u001b[38;5;241m+\u001b[39m b \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(pick_rpy, [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m])]\n\u001b[0;32m----> 6\u001b[0m \u001b[43mpick_place\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpick_pos_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpick_rpy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/ros_ws/src/franka_zed_gazebo/scripts/mycode_clean/3_move/PickAndPlace.py:66\u001b[0m, in \u001b[0;36mPickAndPlace.move\u001b[0;34m(self, position, rpy)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmove\u001b[39m(\u001b[38;5;28mself\u001b[39m, position, rpy):\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Move the robot based on target position and orientation\"\"\"\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrobot_mover\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrpy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/ros_ws/src/franka_zed_gazebo/scripts/mycode_clean/3_move/mymove.py:145\u001b[0m, in \u001b[0;36mMoveRobot.move\u001b[0;34m(self, position, rpy, z_min)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Execute the planned path\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success:\n\u001b[1;32m    148\u001b[0m     rospy\u001b[38;5;241m.\u001b[39mlogerr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMove execution failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/ros/noetic/lib/python3/dist-packages/moveit_commander/move_group.py:615\u001b[0m, in \u001b[0;36mMoveGroupCommander.go\u001b[0;34m(self, joints, wait)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_joint_value_target(joints)\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m--> 615\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_g\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_g\u001b[38;5;241m.\u001b[39masync_move()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# demostrate the movement to the head of identified cubes\n",
    "for T in Ts:\n",
    "    pick_rpy, pick_pos = matrix_to_rpy_and_translation(T)\n",
    "    pick_pos_ = [a + b for a, b in zip(pick_pos, [0.0, 0, 0.10])]\n",
    "    pick_rpy = [a + b for a, b in zip(pick_rpy, [0, 0, 0])]\n",
    "    pick_place.move(pick_pos_, pick_rpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c1ff911-c41c-4330-be82-95130ec626fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [1738697573.229665, 1095.058000]: Move successful to position: [0.3, -0.056249999999999994, 0.13] and RPY: [3.141592653589793, 0.0, -1.5707963267948966]\n",
      "[INFO] [1738697573.231336, 1095.194000]: Path constraints cleared.\n",
      "[INFO] [1738697575.213663, 1097.128000]: Move successful to position: [0.3, 0.0, 0.13] and RPY: [3.141592653589793, 0.0, -1.5707963267948966]\n",
      "[INFO] [1738697575.215311, 1097.128000]: Path constraints cleared.\n",
      "[INFO] [1738697577.201177, 1099.041000]: Move successful to position: [0.3, 0.056249999999999994, 0.13] and RPY: [3.141592653589793, 0.0, -1.5707963267948966]\n",
      "[INFO] [1738697577.202808, 1099.041000]: Path constraints cleared.\n",
      "[INFO] [1738697579.903540, 1101.636000]: Move successful to position: [0.3, -0.028124999999999997, 0.17725000000000002] and RPY: [3.141592653589793, 0.0, -1.5707963267948966]\n",
      "[INFO] [1738697579.912526, 1101.636000]: Path constraints cleared.\n",
      "[INFO] [1738697581.898160, 1103.574000]: Move successful to position: [0.3, 0.028124999999999997, 0.17725000000000002] and RPY: [3.141592653589793, 0.0, -1.5707963267948966]\n",
      "[INFO] [1738697581.899874, 1103.574000]: Path constraints cleared.\n",
      "[INFO] [1738697584.356174, 1105.798000]: Move successful to position: [0.3, 0.0, 0.2245] and RPY: [3.141592653589793, 0.0, -1.5707963267948966]\n",
      "[INFO] [1738697584.357856, 1105.928000]: Path constraints cleared.\n"
     ]
    }
   ],
   "source": [
    "# move the minipulator to the aim position\n",
    "for T in aim_transforms:\n",
    "    transform_matrix_x_180 = np.array([\n",
    "        [1, 0, 0, 0],\n",
    "        [0, -1, 0, 0],\n",
    "        [0, 0, -1, 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "    transform_matrix_z_90 = np.array([\n",
    "        [0, -1, 0, 0],\n",
    "        [1, 0, 0, 0],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "    pick_rpy, pick_pos = matrix_to_rpy_and_translation(T@transform_matrix_x_180@transform_matrix_z_90)\n",
    "    pick_pos_ = [a + b for a, b in zip(pick_pos, [0.0, 0, 0.13])]\n",
    "    pick_rpy = [a + b for a, b in zip(pick_rpy, [0, 0, 0])]\n",
    "    pick_place.move(pick_pos_, pick_rpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da6b4027-9a96-484e-b6b0-c423c42cd075",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [1738697587.639619, 1108.931000]: Sending open goal: width: 0.08\n",
      "speed: 0.1\n",
      "[INFO] [1738697589.104376, 1110.283000]: Gripper opened successfully.\n",
      "[INFO] [1738697601.703166, 1122.613000]: Move successful to position: [0.7000664120432076, -0.39853326900983227, 0.23090872392137776] and RPY: [3.0795857377428915, -0.11261035731720659, 0.061461593483156246]\n",
      "[INFO] [1738697601.705020, 1122.613000]: Path constraints cleared.\n",
      "[INFO] [1738697605.097958, 1125.788000]: Move successful to position: [0.7000664120432076, -0.39853326900983227, 0.03090872392137775] and RPY: [3.0795857377428915, -0.11261035731720659, 0.061461593483156246]\n",
      "[INFO] [1738697605.099806, 1125.932000]: Path constraints cleared.\n",
      "[INFO] [1738697605.401285, 1126.186000]: Sending grasp goal: width: 0.05\n",
      "epsilon: \n",
      "  inner: 0.02\n",
      "  outer: 0.02\n",
      "speed: 0.1\n",
      "force: 50.0\n",
      "[INFO] [1738697606.600760, 1127.265000]: Grasp successful.\n",
      "[INFO] [1738697606.903311, 1127.665000]: Attempt 1 to plan Cartesian path...\n",
      "[INFO] [1738697606.906886, 1127.665000]: Path planning completed successfully!\n",
      "[INFO] [1738697606.908028, 1127.665000]: Executing Cartesian path...\n",
      "[INFO] [1738697608.399281, 1129.007000]: Grasp approach executed successfully.\n",
      "[INFO] [1738697619.836271, 1140.090000]: Move successful to position: [0.3, -0.056249999999999994, 0.23] and RPY: [3.141592653589793, 0.0, -1.5707963267948966]\n",
      "[INFO] [1738697619.838666, 1140.090000]: Path constraints cleared.\n",
      "[INFO] [1738697620.340293, 1140.498000]: Attempt 1 to plan Cartesian path...\n",
      "[INFO] [1738697620.344858, 1140.498000]: Path planning completed successfully!\n",
      "[INFO] [1738697620.346424, 1140.498000]: Executing Cartesian path...\n",
      "[INFO] [1738697623.208633, 1143.210000]: Grasp approach executed successfully.\n",
      "[INFO] [1738697623.711112, 1143.735000]: Sending open goal: width: 0.07\n",
      "speed: 0.1\n",
      "[INFO] [1738697624.629115, 1144.572000]: Gripper opened successfully.\n",
      "[INFO] [1738697625.131987, 1145.048000]: Attempt 1 to plan Cartesian path...\n",
      "[INFO] [1738697625.135939, 1145.048000]: Path planning completed successfully!\n",
      "[INFO] [1738697625.137005, 1145.048000]: Executing Cartesian path...\n",
      "[INFO] [1738697628.113824, 1147.881000]: Grasp approach executed successfully.\n"
     ]
    }
   ],
   "source": [
    "# pick and place the first cube \n",
    "transform_matrix_x_180 = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, -1, 0, 0],\n",
    "    [0, 0, -1, 0],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "transform_matrix_z_90 = np.array([\n",
    "    [0, -1, 0, 0],\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "pick_rpy, pick_pos = matrix_to_rpy_and_translation(Ts[0])\n",
    "pick_pos_ = [a + b for a, b in zip(pick_pos, [0.0, 0, 0.01])]\n",
    "pick_rpy = [a + b for a, b in zip(pick_rpy, [0, 0, 0])]\n",
    "    \n",
    "place_rpy, place_pos = matrix_to_rpy_and_translation(aim_transforms[0]@transform_matrix_x_180@transform_matrix_z_90)\n",
    "place_pos_ = [a + b for a, b in zip(place_pos, [0.0, 0, 0.03])]\n",
    "\n",
    "pick_place.pick_and_place(\n",
    "    pick_pos=pick_pos_,\n",
    "    pick_rpy=pick_rpy,\n",
    "    place_pos=place_pos_,\n",
    "    place_rpy=place_rpy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a662392-36bb-46d1-a2b3-ac8562db28ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [1738698046.626357, 1548.957000]: Sending open goal: width: 0.08\n",
      "speed: 0.1\n",
      "[INFO] [1738698047.245502, 1549.533000]: Gripper opened successfully.\n",
      "[INFO] [1738698056.627111, 1558.661000]: Move successful to position: [0.6972405082839007, 0.00017438521650547343, 0.2293021421167005] and RPY: [-3.0999512819014168, 0.047686371957770035, -1.53428069231817]\n",
      "[INFO] [1738698056.628973, 1558.661000]: Path constraints cleared.\n",
      "[INFO] [1738698060.125085, 1562.047000]: Move successful to position: [0.6972405082839007, 0.00017438521650547343, 0.029302142116700482] and RPY: [-3.0999512819014168, 0.047686371957770035, -1.53428069231817]\n",
      "[INFO] [1738698060.127339, 1562.053000]: Path constraints cleared.\n",
      "[INFO] [1738698060.429233, 1562.337000]: Sending grasp goal: width: 0.05\n",
      "epsilon: \n",
      "  inner: 0.02\n",
      "  outer: 0.02\n",
      "speed: 0.1\n",
      "force: 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARN] [1738698062.110759, 1563.845000]: Grasp failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [1738698062.413327, 1564.254000]: Attempt 1 to plan Cartesian path...\n",
      "[INFO] [1738698062.416792, 1564.254000]: Path planning completed successfully!\n",
      "[INFO] [1738698062.417940, 1564.254000]: Executing Cartesian path...\n",
      "[INFO] [1738698064.212362, 1565.880000]: Grasp approach executed successfully.\n",
      "[INFO] [1738698072.557969, 1574.082000]: Move successful to position: [0.3, 0.028124999999999997, 0.27725] and RPY: [3.141592653589793, 0.0, -1.5707963267948966]\n",
      "[INFO] [1738698072.560002, 1574.082000]: Path constraints cleared.\n",
      "[INFO] [1738698073.061565, 1574.510000]: Attempt 1 to plan Cartesian path...\n",
      "[INFO] [1738698073.064756, 1574.510000]: Path planning completed successfully!\n",
      "[INFO] [1738698073.065916, 1574.510000]: Executing Cartesian path...\n",
      "[INFO] [1738698075.265953, 1576.554000]: Grasp approach executed successfully.\n",
      "[INFO] [1738698075.768704, 1577.103000]: Sending open goal: width: 0.07\n",
      "speed: 0.1\n",
      "[INFO] [1738698077.171196, 1578.406000]: Gripper opened successfully.\n",
      "[INFO] [1738698077.675485, 1578.963000]: Attempt 1 to plan Cartesian path...\n",
      "[INFO] [1738698077.679095, 1578.963000]: Path planning completed successfully!\n",
      "[INFO] [1738698077.680236, 1578.963000]: Executing Cartesian path...\n",
      "[INFO] [1738698079.754385, 1580.882000]: Grasp approach executed successfully.\n"
     ]
    }
   ],
   "source": [
    "# pick and place following cubes \n",
    "for i in range(4, Ts.__len__()-1):\n",
    "    ptransform_matrix_x_180 = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, -1, 0, 0],\n",
    "    [0, 0, -1, 0],\n",
    "    [0, 0, 0, 1]\n",
    "    ])\n",
    "    transform_matrix_z_90 = np.array([\n",
    "        [0, -1, 0, 0],\n",
    "        [1, 0, 0, 0],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    pick_rpy, pick_pos = matrix_to_rpy_and_translation(Ts[i])\n",
    "    pick_pos_ = [a + b for a, b in zip(pick_pos, [0.0, 0, 0.008])]\n",
    "    pick_rpy = [a + b for a, b in zip(pick_rpy, [0, 0, 0])]\n",
    "\n",
    "    place_rpy, place_pos = matrix_to_rpy_and_translation(aim_transforms[i]@transform_matrix_x_180@transform_matrix_z_90)\n",
    "    place_pos_ = [a + b for a, b in zip(place_pos, [0.0, 0, 0.03])]\n",
    "\n",
    "    pick_place.pick_and_place(\n",
    "        pick_pos=pick_pos_,\n",
    "        pick_rpy=pick_rpy,\n",
    "        place_pos=place_pos_,\n",
    "        place_rpy=place_rpy\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae12895-f189-48d9-98a8-7f5b4ce8f8dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28a61a9-5a2d-46d9-85f8-6e0d2d2e66e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb28b7de-2a7b-447b-b92c-f9487c4a8977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344eea87-4f39-40cd-adcb-2d3490950865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b49a45-b76a-4109-a1b9-92473eb0cc5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90752b1-45fd-40c6-95e6-5c641111580b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e628f0-7d4c-4612-8453-cc769bb96ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0b93ec-fc14-4c5b-a51b-ddd187495ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cf24a9-2107-4c18-8b75-410152602d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97c4cfa-2497-47fb-abcc-b4d8102d5a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97a75a1-7a1b-4df9-87e5-f22fc098e059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916923c9-2c36-417b-bb2a-7a07e665e320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1aa9b1-890b-4a21-a656-5c54a75d5c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e1942-89b9-460e-8013-c3406e5ec660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f97b18-cf6e-4683-b2b0-b18f61f0c335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84bdaf3-eb89-4acf-aa35-34d10d9ff810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72f5be6-10ab-4a45-9860-f3cdf9737da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc74575-a184-436e-b10f-6e5cbd701094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
